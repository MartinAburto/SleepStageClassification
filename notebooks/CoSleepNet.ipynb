{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf9e79a",
   "metadata": {},
   "source": [
    "#  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "#  INSTALACI√ìN Y CONFIGURACI√ìN DE DEPENDENCIAS\n",
    "# ================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\" CONFIGURANDO ENTORNO DE DEPENDENCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================\n",
    "# PASO 1: Instalar librer√≠as principales\n",
    "# ================================================\n",
    "print(\"\\n[1/4] Instalando librer√≠as de procesamiento y datos...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'mne', 'pyedflib', 'numpy', 'pandas', 'scipy', 'tqdm', 'pydrive2'\n",
    "])\n",
    "print(\"    Librer√≠as principales instaladas\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 2: Instalar soporte visual (ipywidgets)\n",
    "# ================================================\n",
    "print(\"\\n[2/4] Instalando soporte visual para tqdm y widgets interactivos...\")\n",
    "try:\n",
    "    subprocess.check_call([\n",
    "        sys.executable, '-m', 'pip', 'install', '-q', '--upgrade', 'ipywidgets'\n",
    "    ])\n",
    "    print(\"    ipywidgets instalado/actualizado\")\n",
    "\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'jupyter', 'nbextension',\n",
    "            'enable', '--py', 'widgetsnbextension', '--sys-prefix'\n",
    "        ])\n",
    "        print(\"    Extensi√≥n widgetsnbextension habilitada\")\n",
    "    except:\n",
    "        print(\"    No se pudo habilitar widgetsnbextension (Colab normalmente no lo requiere)\")\n",
    "except Exception as e:\n",
    "    print(f\"    Advertencia durante instalaci√≥n: {e}\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 3: Instalar utilidades opcionales\n",
    "# ================================================\n",
    "print(\"\\n[3/4] Instalando utilidades adicionales (matplotlib, seaborn, openpyxl)...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'matplotlib', 'seaborn', 'openpyxl'\n",
    "])\n",
    "print(\"   Utilidades adicionales instaladas\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 4: Importar y verificar versiones\n",
    "# ================================================\n",
    "print(\"\\n[4/4] Importando m√≥dulos y verificando entorno...\")\n",
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import ipywidgets\n",
    "\n",
    "print(\"    Dependencias cargadas correctamente\")\n",
    "print(f\"\\n Versi√≥n de componentes:\")\n",
    "print(f\"   ‚Ä¢ Python:   {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ MNE:      {mne.__version__}\")\n",
    "print(f\"   ‚Ä¢ pandas:   {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ numpy:    {np.__version__}\")\n",
    "\n",
    "print(\"\\n Entorno completamente configurado. Puedes continuar con el procesamiento.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f33d0a",
   "metadata": {},
   "source": [
    "# Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  CONFIGURACI√ìN DE RUTAS, PAR√ÅMETROS Y DIRECTORIOS\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"  CONFIGURANDO RUTAS Y PAR√ÅMETROS DE PROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# PASO 1: DEFINIR DIRECTORIOS BASE\n",
    "# ============================================================\n",
    "print(\"\\n[1/4] Estableciendo rutas principales...\")\n",
    "\n",
    "# Carpeta local con los archivos EDF (PSG + Hypnograma)\n",
    "RAW_DIR = Path(\n",
    "    r\"C:\\Users\\shipa\\OneDrive\\Escritorio\\Inteligencia Computacional\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Carpeta ra√≠z donde se guardar√°n resultados\n",
    "OUTPUT_DIR = RAW_DIR / \"ventanas_out_mas_corto\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"    RAW_DIR:    {RAW_DIR}\")\n",
    "print(f\"    OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 2: CREAR SUBCARPETAS DE SALIDA\n",
    "# ============================================================\n",
    "print(\"\\n[2/4] Creando subdirectorios para ventanas y an√°lisis...\")\n",
    "\n",
    "WINDOWS_DIR = OUTPUT_DIR / \"ventanas_extraidas_mas_corto\"\n",
    "ANALYSIS_DIR = OUTPUT_DIR / \"analisis_canales_mas_corto\"\n",
    "\n",
    "WINDOWS_DIR.mkdir(exist_ok=True)\n",
    "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"    WINDOWS_DIR:  {WINDOWS_DIR}\")\n",
    "print(f\"    ANALYSIS_DIR: {ANALYSIS_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 3: DEFINIR PAR√ÅMETROS DE PROCESAMIENTO\n",
    "# ============================================================\n",
    "print(\"\\n[3/4] Estableciendo par√°metros de ventaneo...\")\n",
    "\n",
    "WINDOW_SIZE = 30.0   # en segundos\n",
    "OVERLAP = 15.0       # en segundos\n",
    "STRIDE = WINDOW_SIZE - OVERLAP\n",
    "\n",
    "print(f\"     Ventana:  {WINDOW_SIZE}s\")\n",
    "print(f\"     Overlap:  {OVERLAP}s\")\n",
    "print(f\"     Stride:   {STRIDE}s\")\n",
    "\n",
    "# ============================================================\n",
    "# PASO 4: CONFIGURAR ARCHIVO ZIP DE EXPORTACI√ìN\n",
    "# ============================================================\n",
    "print(\"\\n[4/4] Configurando ruta para archivo ZIP de exportaci√≥n...\")\n",
    "\n",
    "ZIP_PATH = OUTPUT_DIR / \"sleep_edf_ventanas.zip\"\n",
    "print(f\"    ZIP_PATH: {ZIP_PATH}\")\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN FINAL DE CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CONFIGURACI√ìN COMPLETADA\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9282d85",
   "metadata": {},
   "source": [
    "# Funciones auxiliares + recorte de Wakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24840a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  FUNCIONES AUXILIARES Y PREPROCESAMIENTO DE SUE√ëO \n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\" Cargando funciones auxiliares, extracci√≥n de ventanas y recorte de sue√±o\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================\n",
    "# PASO 1: FUNCIONES PARA DETECCI√ìN DE PARES PSG/HYPNOGRAM\n",
    "# ================================================================\n",
    "\n",
    "def key_from_psg(name: str) -> str:\n",
    "    \"\"\"SC4001E0-PSG.edf  ‚Üí  SC4001E\"\"\"\n",
    "    return Path(name).name.split('-')[0][:7]\n",
    "\n",
    "def key_from_hyp(name: str) -> str:\n",
    "    \"\"\"SC4001EC-Hypnogram.edf ‚Üí SC4001E\"\"\"\n",
    "    return Path(name).name.split('-')[0][:7]\n",
    "\n",
    "def encontrar_pares(raw_dir: Path):\n",
    "    \"\"\"\n",
    "    Busca pares PSG/Hypnogram en estilo Sleep-EDF Cassette (SCxxxxE).\n",
    "    Devuelve lista de tuplas (key, psg_path, hyp_path).\n",
    "    \"\"\"\n",
    "    psgs, hyps = {}, {}\n",
    "\n",
    "    for fp in raw_dir.glob(\"*.edf\"):\n",
    "        nm = fp.name\n",
    "        if not nm.startswith(\"SC\"):\n",
    "            continue\n",
    "\n",
    "        if nm.endswith(\"-PSG.edf\"):\n",
    "            psgs[key_from_psg(nm)] = fp\n",
    "        elif nm.endswith(\"-Hypnogram.edf\"):\n",
    "            hyps[key_from_hyp(nm)] = fp\n",
    "\n",
    "    keys = sorted(set(psgs) & set(hyps))\n",
    "    return [(k, psgs[k], hyps[k]) for k in keys]\n",
    "\n",
    "print(\"   Funciones para emparejar PSG/Hypnogram listas\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# PASO 2: LECTURA DEL HYPNOGRAMA\n",
    "# ================================================================\n",
    "\n",
    "def leer_hypnograma_mne(hyp_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee anotaciones y devuelve DataFrame con columnas:\n",
    "    [inicio, duracion, etapa]\n",
    "    Etapas v√°lidas (ya normalizadas): W, 1, 2, 3, R.\n",
    "    N3 = {3,4} colapsadas.\n",
    "    \"\"\"\n",
    "    ann = mne.read_annotations(str(hyp_path))\n",
    "    etapas = []\n",
    "\n",
    "    for desc, onset, dur in zip(ann.description, ann.onset, ann.duration):\n",
    "        if \"Sleep stage\" in desc:\n",
    "            st = desc.replace(\"Sleep stage\", \"\").strip()\n",
    "\n",
    "            # üîπ Normalizaci√≥n de etiquetas\n",
    "            if st in {\"W\", \"1\", \"2\", \"3\", \"4\", \"R\"}:\n",
    "                # Colapsar N3/N4 ‚Üí \"3\"\n",
    "                if st in {\"3\", \"4\"}:\n",
    "                    st_norm = \"3\"\n",
    "                else:\n",
    "                    st_norm = st\n",
    "\n",
    "                etapas.append({\n",
    "                    \"inicio\": float(onset),\n",
    "                    \"duracion\": float(dur),\n",
    "                    \"etapa\": st_norm\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(etapas)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"Hypnograma vac√≠o o sin etiquetas v√°lidas: {hyp_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"   Lector de hypnograma disponible\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# PASO 3: C√ìMPUTO DE OFFSET (ALINEACI√ìN PSG‚ÄìHYPNO)\n",
    "# ================================================================\n",
    "\n",
    "def _to_datetime(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, tuple) and len(x) == 2:\n",
    "        return datetime.fromtimestamp(x[0]) + timedelta(microseconds=x[1])\n",
    "    try:\n",
    "        return pd.to_datetime(x).to_pydatetime()\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def calcular_offset_segundos(psg_path: Path, hyp_path: Path) -> float:\n",
    "    \"\"\"\n",
    "    offset = (inicio_hyp - inicio_psg) en segundos.\n",
    "    Positivo si el hypnograma empieza despu√©s del PSG.\n",
    "    \"\"\"\n",
    "    raw_psg = mne.io.read_raw_edf(str(psg_path), preload=False, verbose=False)\n",
    "    raw_hyp = mne.io.read_raw_edf(str(hyp_path), preload=False, verbose=False)\n",
    "\n",
    "    t_psg = _to_datetime(raw_psg.info.get('meas_date'))\n",
    "    t_hyp = _to_datetime(raw_hyp.info.get('meas_date'))\n",
    "\n",
    "    if t_psg is None or t_hyp is None:\n",
    "        return 0.0\n",
    "\n",
    "    return (t_hyp - t_psg).total_seconds()\n",
    "\n",
    "print(\"   Funci√≥n de alineaci√≥n (offset) lista\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# PASO 4: AN√ÅLISIS DE CANALES DEL PSG\n",
    "# ================================================================\n",
    "\n",
    "def analizar_canales(psg_path: Path):\n",
    "    \"\"\"\n",
    "    Devuelve dict {nombre_canal: {'tipo','freq','n_samples','duracion'}}.\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_edf(str(psg_path), preload=False, verbose=False)\n",
    "    info = {}\n",
    "\n",
    "    for ch in raw.ch_names:\n",
    "        ch_up = ch.upper()\n",
    "\n",
    "        if 'EEG' in ch_up:\n",
    "            tipo = 'EEG'\n",
    "        elif 'EOG' in ch_up:\n",
    "            tipo = 'EOG'\n",
    "        elif 'EMG' in ch_up:\n",
    "            tipo = 'EMG'\n",
    "        elif 'ECG' in ch_up or 'EKG' in ch_up:\n",
    "            tipo = 'ECG'\n",
    "        elif 'EVENT' in ch_up or 'MARKER' in ch_up:\n",
    "            tipo = 'EVENTO'\n",
    "        else:\n",
    "            tipo = 'OTRO'\n",
    "\n",
    "        info[ch] = {\n",
    "            'tipo': tipo,\n",
    "            'freq': float(raw.info['sfreq']),\n",
    "            'n_samples': int(raw.n_times),\n",
    "            'duracion': float(raw.times[-1]) if raw.n_times > 0 else 0.0\n",
    "        }\n",
    "\n",
    "    return info\n",
    "\n",
    "print(\"   Clasificador de canales cargado\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# PASO 5: EXTRACCI√ìN DE VENTANAS POR CANAL (CORREGIDA)\n",
    "# ================================================================\n",
    "\n",
    "def extraer_ventanas_por_canal(\n",
    "    psg_path: Path,\n",
    "    hyp_df: pd.DataFrame,\n",
    "    canal_nombre: str,\n",
    "    window_size: float,\n",
    "    stride: float,\n",
    "    t_ini_psg: float,\n",
    "    t_fin_psg: float,\n",
    "    hyp_offset: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Extrae ventanas solapadas del canal elegido usando:\n",
    "        - l√≠mites de PSG (t_ini_psg, t_fin_psg)\n",
    "        - hipnograma recortado\n",
    "        - asignaci√≥n de etiqueta desde hyp_df\n",
    "        - punto medio de la ventana\n",
    "    \"\"\"\n",
    "\n",
    "    raw = mne.io.read_raw_edf(str(psg_path), preload=True, verbose=False)\n",
    "    data, _ = raw[canal_nombre, :]\n",
    "    x = data.flatten()\n",
    "    fs = float(raw.info['sfreq'])\n",
    "\n",
    "    win_samps   = int(round(window_size * fs))\n",
    "    stride_samp = int(round(stride * fs))\n",
    "\n",
    "    # Si la se√±al es demasiado corta, retornar vac√≠o\n",
    "    if win_samps <= 0 or stride_samp <= 0 or len(x) < win_samps:\n",
    "        return {\n",
    "            \"ventanas\": np.empty((0, win_samps)),\n",
    "            \"etiquetas\": [],\n",
    "            \"tiempos_inicio\": [],\n",
    "            \"freq_muestreo\": fs,\n",
    "            \"nombre_canal\": canal_nombre\n",
    "        }\n",
    "\n",
    "    # Intervalos del hypnograma EN TIEMPO PSG\n",
    "    starts = hyp_df[\"inicio\"].to_numpy(float) + hyp_offset\n",
    "    ends   = (hyp_df[\"inicio\"] + hyp_df[\"duracion\"]).to_numpy(float) + hyp_offset\n",
    "\n",
    "    intervals = pd.IntervalIndex.from_arrays(starts, ends, closed=\"left\")\n",
    "\n",
    "    # N√∫mero m√°ximo posible de ventanas\n",
    "    n_max = 1 + (len(x) - win_samps) // stride_samp\n",
    "\n",
    "    ventanas = []\n",
    "    etiquetas = []\n",
    "    tiempos_inicio = []\n",
    "\n",
    "    for i in range(n_max):\n",
    "        t_ini = i * stride           # tiempo en eje PSG\n",
    "        t_mid = t_ini + window_size/2\n",
    "\n",
    "        # 1) Verificar si cae dentro del intervalo permitido\n",
    "        if not (t_ini_psg <= t_mid <= t_fin_psg):\n",
    "            continue\n",
    "\n",
    "        # 2) Calcular muestras\n",
    "        s = int(round(t_ini * fs))\n",
    "        e = s + win_samps\n",
    "\n",
    "        if e > len(x):\n",
    "            break\n",
    "\n",
    "        # 3) Guardar ventana\n",
    "        ventanas.append(x[s:e])\n",
    "        tiempos_inicio.append(t_ini)\n",
    "\n",
    "        # 4) Etiqueta desde hypnograma\n",
    "        idx = intervals.get_indexer([t_mid])[0]\n",
    "        if idx == -1:\n",
    "            etiquetas.append(\"W\")      # fallback seguro\n",
    "        else:\n",
    "            etiquetas.append(hyp_df.iloc[idx][\"etapa\"])\n",
    "\n",
    "    # Convertir a arrays\n",
    "    ventanas = np.array(ventanas, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        \"ventanas\": ventanas,\n",
    "        \"etiquetas\": etiquetas,\n",
    "        \"tiempos_inicio\": tiempos_inicio,\n",
    "        \"freq_muestreo\": fs,\n",
    "        \"nombre_canal\": canal_nombre\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"extraer_ventanas_por_canal redefinida correctamente\")\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# PASO 6: RECORTE A INTERVALO DE SUE√ëO (onset ‚Üí offset)\n",
    "# ================================================================\n",
    "\n",
    "def recortar_hyp_a_sueno(hyp_df: pd.DataFrame,\n",
    "                          etiqueta_wake: str = \"W\",\n",
    "                          margen_pre: float = 0.0,\n",
    "                          margen_post: float = 0.0):\n",
    "    \"\"\"\n",
    "    Recorta el hipnograma al intervalo [primer_no_W, ultimo_no_W] con m√°rgenes,\n",
    "    usando el PUNTO MEDIO de cada epoch para decidir si se conserva.\n",
    "    Devuelve (hyp_recortado, t_ini, t_fin) en EJE DE TIEMPO DEL HYPNOGRAMA.\n",
    "    \"\"\"\n",
    "    # Epochs que NO son Wake\n",
    "    mask_sueno = hyp_df['etapa'] != etiqueta_wake\n",
    "\n",
    "    if mask_sueno.sum() == 0:\n",
    "        # No hay sue√±o: devolvemos todo tal cual\n",
    "        return hyp_df.copy(), None, None\n",
    "\n",
    "    # Onset y offset del sue√±o (en segundos, eje hypnograma)\n",
    "    t_on = hyp_df.loc[mask_sueno, 'inicio'].min()\n",
    "    t_off = (hyp_df.loc[mask_sueno, 'inicio'] + hyp_df.loc[mask_sueno, 'duracion']).max()\n",
    "\n",
    "    # Aplicar m√°rgenes\n",
    "    t_ini = max(0.0, t_on - margen_pre)\n",
    "    t_fin = t_off + margen_post\n",
    "\n",
    "    # Usar el punto medio de cada epoch para decidir si queda dentro\n",
    "    mid = hyp_df['inicio'] + 0.5 * hyp_df['duracion']\n",
    "    mask_keep = (mid >= t_ini) & (mid <= t_fin)\n",
    "\n",
    "    hyp_rec = hyp_df.loc[mask_keep].reset_index(drop=True)\n",
    "\n",
    "    return hyp_rec, t_ini, t_fin\n",
    "\n",
    "\n",
    "print(\"  Funci√≥n de recorte de sue√±o lista\")\n",
    "\n",
    "# ================================================================\n",
    "print(\"\\n Todas las funciones auxiliares cargadas\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f2094",
   "metadata": {},
   "source": [
    "# Procesamiento principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ae451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# DETECTAR N√öMEROS DE WORKERS DEL EQUIPO\n",
    "# ======================================================\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "TOTAL_CORES = mp.cpu_count()\n",
    "N_WORKERS =  max(1, int(TOTAL_CORES * 0.75))\n",
    "\n",
    "print(\" Informaci√≥n del sistema:\")\n",
    "print(f\"   ‚Ä¢ N√∫cleos disponibles : {TOTAL_CORES}\")\n",
    "print(f\"   ‚Ä¢ Workers a utilizar  : {N_WORKERS}  (~75%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST MULTICANAL ‚Äî 1 PACIENTE COMPLETO \n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"Buscando pares PSG-Hypnogram...\")\n",
    "pares = encontrar_pares(RAW_DIR)\n",
    "\n",
    "if len(pares) == 0:\n",
    "    print(\"No se encontraron pares en RAW_DIR\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Elegimos un paciente para probar.\n",
    "# Si se quiere espec√≠ficamente el √≠ndice , dejamos un \"fallback\" a 0\n",
    "# por si el dataset no tiene tantos.\n",
    "# ------------------------------------------------------------------\n",
    "idx_paciente = 40\n",
    "if len(pares) <= idx_paciente:\n",
    "    idx_paciente = 0\n",
    "\n",
    "key, psg_path, hyp_path = pares[idx_paciente]\n",
    "\n",
    "print(f\"\\nProbando todos los canales para paciente: {key}\")\n",
    "print(f\"   PSG: {psg_path.name}\")\n",
    "print(f\"   HYP: {hyp_path.name}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) Leer hypnograma\n",
    "# ============================================================\n",
    "hyp_df = leer_hypnograma_mne(hyp_path)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Recorte a sue√±o (mismos m√°rgenes que el procesamiento global)\n",
    "# ============================================================\n",
    "MARGEN_PRE  = 0* 60   # 10 min antes del onset\n",
    "MARGEN_POST = 0* 60   # 10 min despu√©s del offset\n",
    "\n",
    "hyp_df_rec, t_ini_hyp, t_fin_hyp = recortar_hyp_a_sueno(\n",
    "    hyp_df,\n",
    "    etiqueta_wake=\"W\",\n",
    "    margen_pre=MARGEN_PRE,\n",
    "    margen_post=MARGEN_POST\n",
    ")\n",
    "\n",
    "# Si no hubo sue√±o, usar todo el hypnograma\n",
    "if t_ini_hyp is None:\n",
    "    t_ini_hyp = float(hyp_df[\"inicio\"].min())\n",
    "    t_fin_hyp = float((hyp_df[\"inicio\"] + hyp_df[\"duracion\"]).max())\n",
    "    hyp_df_uso = hyp_df\n",
    "else:\n",
    "    hyp_df_uso = hyp_df_rec\n",
    "\n",
    "# Alinear hypnograma al PSG\n",
    "offset = calcular_offset_segundos(psg_path, hyp_path)\n",
    "t_ini_psg = t_ini_hyp + offset\n",
    "t_fin_psg = t_fin_hyp + offset\n",
    "\n",
    "print(f\"\\nIntervalo de sue√±o en eje PSG: [{t_ini_psg:.1f} s, {t_fin_psg:.1f} s]\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Analizar canales\n",
    "# ============================================================\n",
    "canales = analizar_canales(psg_path)\n",
    "canales_validos = [c for c, meta in canales.items() if meta[\"tipo\"] != \"EVENTO\"]\n",
    "\n",
    "print(f\"\\nCanales detectados ({len(canales_validos)}):\")\n",
    "for c in canales_validos:\n",
    "    print(\"   ‚Ä¢\", c)\n",
    "\n",
    "resultados = {}  # guardamos X, y, t por canal\n",
    "\n",
    "# ============================================================\n",
    "# 4) Procesar canal por canal\n",
    "# ============================================================\n",
    "for canal in canales_validos:\n",
    "    print(f\"\\nProcesando canal: {canal}\")\n",
    "\n",
    "    try:\n",
    "        res = extraer_ventanas_por_canal(\n",
    "            psg_path=psg_path,\n",
    "            hyp_df=hyp_df_uso,\n",
    "            canal_nombre=canal,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            stride=STRIDE,\n",
    "            t_ini_psg=t_ini_psg,\n",
    "            t_fin_psg=t_fin_psg,\n",
    "            hyp_offset=offset\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"   Error:\", e)\n",
    "        continue\n",
    "\n",
    "    X = res[\"ventanas\"]\n",
    "    y = np.array(res[\"etiquetas\"])\n",
    "    t0 = np.array(res[\"tiempos_inicio\"])\n",
    "\n",
    "    print(f\"   Ventanas: {X.shape}\")\n",
    "    if len(y) > 0:\n",
    "        vc_abs = pd.Series(y).value_counts().to_dict()\n",
    "        vc_rel = (pd.Series(y).value_counts(normalize=True)\n",
    "                  .round(3).to_dict())\n",
    "        print(f\"   Distribuci√≥n absoluta:   {vc_abs}\")\n",
    "        print(f\"   Distribuci√≥n proporcional: {vc_rel}\")\n",
    "    else:\n",
    "        print(\"   Sin ventanas extra√≠das para este canal\")\n",
    "\n",
    "    # guardar para revisi√≥n\n",
    "    resultados[canal] = {\"X\": X, \"y\": y, \"t\": t0}\n",
    "\n",
    "    # guardar archivo de prueba\n",
    "    test_file = WINDOWS_DIR / f\"{key}_{canal.replace(' ','_')}_TEST.npz\"\n",
    "    np.savez_compressed(\n",
    "        test_file,\n",
    "        ventanas=X,\n",
    "        etiquetas=y,\n",
    "        tiempos_inicio=t0,\n",
    "        freq_muestreo=res[\"freq_muestreo\"],\n",
    "        nombre_canal=res[\"nombre_canal\"]\n",
    "    )\n",
    "    print(f\"   Guardado test ‚Üí {test_file.name}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Comparar alineaci√≥n entre canales\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACI√ìN ENTRE CANALES (N¬∫ DE VENTANAS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not resultados:\n",
    "    print(\"No se obtuvieron resultados en ning√∫n canal.\")\n",
    "else:\n",
    "    # Canal base para comparar n¬∫ de ventanas\n",
    "    can_base = list(resultados.keys())[0]\n",
    "    t_base = resultados[can_base][\"t\"]\n",
    "    print(f\"Canal base para comparaci√≥n: {can_base}\")\n",
    "\n",
    "    todo_consistente = True\n",
    "    for canal, data in resultados.items():\n",
    "        dt = len(data[\"t\"]) - len(t_base)\n",
    "        if dt == 0:\n",
    "            print(f\"{canal}: mismo n√∫mero de ventanas que {can_base} ({len(data['t'])})\")\n",
    "        else:\n",
    "            todo_consistente = False\n",
    "            print(f\"{canal}: {len(data['t'])} vs {len(t_base)} ventanas (diferencia {dt})\")\n",
    "\n",
    "    if todo_consistente:\n",
    "        print(\"\\n TODOS los canales de este paciente tienen el MISMO n√∫mero de ventanas.\")\n",
    "    else:\n",
    "        print(\"\\n Hay canales con distinto n√∫mero de ventanas. Revisar arriba.\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) GRAFICAR TIMELINES PARA TODOS LOS CANALES\n",
    "# ============================================================\n",
    "if resultados:\n",
    "    colores = {\n",
    "        \"W\":\"#f4d03f\",\"1\":\"#e67e22\",\"2\":\"#3498db\",\n",
    "        \"N1\":\"#e67e22\",\"N2\":\"#3498db\",\n",
    "        \"3\":\"#2ecc71\",\"4\":\"#27ae60\",\"N3\":\"#2ecc71\",\n",
    "        \"R\":\"#9b59b6\",\"REM\":\"#9b59b6\"\n",
    "    }\n",
    "\n",
    "    n_channels = len(resultados)\n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(16, 2*n_channels), sharex=True)\n",
    "\n",
    "    if n_channels == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (canal, data) in zip(axes, resultados.items()):\n",
    "        y = data[\"y\"]\n",
    "        cols = [colores.get(e, \"gray\") for e in y]\n",
    "        ax.bar(range(len(y)), np.ones(len(y)), color=cols, width=1.0)\n",
    "        ax.set_title(canal)\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # leyenda compacta\n",
    "    labels_usadas = sorted({et for data in resultados.values() for et in data[\"y\"]})\n",
    "    patches = []\n",
    "    for l in labels_usadas:\n",
    "        c = colores.get(l, \"gray\")\n",
    "        patches.append(mpatches.Patch(color=c, label=l))\n",
    "    axes[0].legend(handles=patches, loc=\"upper right\", bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n No hay resultados para graficar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROCESAMIENTO PRINCIPAL \n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(\"Iniciando procesamiento de registros PSG + Hypnogram\")\n",
    "print(\"=\"*80)\n",
    "print(\"Nota: Este script SOBRESCRIBE los archivos .npz existentes y el resumen_global_mas_corto.csv\")\n",
    "\n",
    "# Par√°metros de recorte (en segundos)\n",
    "MARGEN_PRE  = 0 * 60   #  min antes del onset de sue√±o\n",
    "MARGEN_POST = 0 * 60   #  min despu√©s del offset de sue√±o\n",
    "\n",
    "print(f\"    Recorte de sue√±o activado:\")\n",
    "print(f\"      ‚Ä¢ Margen antes  : {MARGEN_PRE/60:.1f} min\")\n",
    "print(f\"      ‚Ä¢ Margen despu√©s: {MARGEN_POST/60:.1f} min\")\n",
    "\n",
    "pares = encontrar_pares(RAW_DIR)\n",
    "print(f\"\\n Pares PSG-Hypnogram encontrados: {len(pares)}\")\n",
    "\n",
    "resumen_rows = []\n",
    "errores = []\n",
    "\n",
    "# Para chequear consistencia por paciente (n¬∫ de ventanas por canal)\n",
    "patient_channel_windows = defaultdict(dict)  # {paciente: {canal: n_ventanas}}\n",
    "\n",
    "# Para distribuci√≥n global de etapas: contamos solo UN canal por paciente\n",
    "stage_counts = defaultdict(int)             # {etapa: conteo}\n",
    "pacientes_etapas_contadas = set()          # pacientes ya considerados en stage_counts\n",
    "\n",
    "# Bucle principal: por paciente (sin multiproceso)\n",
    "for key, psg_path, hyp_path in tqdm(\n",
    "    pares, desc=\"Procesando pacientes\", unit=\"pac\", leave=True\n",
    "):\n",
    "    try:\n",
    "        # 1) Leer hypnograma\n",
    "        hyp_df = leer_hypnograma_mne(hyp_path)\n",
    "\n",
    "        # 2) Recortar a sue√±o (en eje del HYPNOGRAMA)\n",
    "        hyp_df_rec, t_ini_hyp, t_fin_hyp = recortar_hyp_a_sueno(\n",
    "            hyp_df,\n",
    "            etiqueta_wake=\"W\",\n",
    "            margen_pre=MARGEN_PRE,\n",
    "            margen_post=MARGEN_POST\n",
    "        )\n",
    "\n",
    "        if t_ini_hyp is None:\n",
    "            # Caso raro: no hay sue√±o, usar todo el hypnograma\n",
    "            hyp_df_uso = hyp_df\n",
    "            t_ini_hyp = float(hyp_df['inicio'].min())\n",
    "            t_fin_hyp = float((hyp_df['inicio'] + hyp_df['duracion']).max())\n",
    "        else:\n",
    "            hyp_df_uso = hyp_df_rec\n",
    "\n",
    "        # 3) Alinear tiempos PSG‚ÄìHypnograma\n",
    "        offset = calcular_offset_segundos(psg_path, hyp_path)\n",
    "        # Pasar a eje de tiempo del PSG\n",
    "        t_ini_psg = t_ini_hyp + offset\n",
    "        t_fin_psg = t_fin_hyp + offset\n",
    "\n",
    "        # 4) Analizar canales\n",
    "        canales = analizar_canales(psg_path)\n",
    "\n",
    "        # 5) Procesar por canal\n",
    "        desc_inner = f\"[{key}] Procesando canales\"\n",
    "        for canal, meta in tqdm(\n",
    "            canales.items(), desc=desc_inner, unit=\"canal\", leave=False\n",
    "        ):\n",
    "            # Saltar canales de eventos/marcadores\n",
    "            if meta[\"tipo\"] == \"EVENTO\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                resultado = extraer_ventanas_por_canal(\n",
    "                    psg_path=psg_path,\n",
    "                    hyp_df=hyp_df_uso,\n",
    "                    canal_nombre=canal,\n",
    "                    window_size=WINDOW_SIZE,\n",
    "                    stride=STRIDE,\n",
    "                    t_ini_psg=t_ini_psg,\n",
    "                    t_fin_psg=t_fin_psg,\n",
    "                    hyp_offset=offset\n",
    "                )\n",
    "\n",
    "                ventanas = resultado[\"ventanas\"]\n",
    "                n_vent = len(ventanas)\n",
    "\n",
    "                # Registrar n¬∫ de ventanas por canal para el chequeo de consistencia\n",
    "                patient_channel_windows[key][canal] = n_vent\n",
    "\n",
    "                # Contar distribuci√≥n de etapas SOLO una vez por paciente\n",
    "                if key not in pacientes_etapas_contadas:\n",
    "                    for et in resultado[\"etiquetas\"]:\n",
    "                        stage_counts[str(et)] += 1\n",
    "                    pacientes_etapas_contadas.add(key)\n",
    "\n",
    "                # ================================\n",
    "                # GUARDAR EN NPZ (COMPRIMIDO)\n",
    "                # (sobrescribe si ya existe)\n",
    "                # ================================\n",
    "                out_file = WINDOWS_DIR / f\"{key}_{canal.replace(' ', '_')}.npz\"\n",
    "                np.savez_compressed(\n",
    "                    out_file,\n",
    "                    ventanas=ventanas,\n",
    "                    etiquetas=np.array(resultado[\"etiquetas\"], dtype=object),\n",
    "                    tiempos_inicio=np.array(resultado[\"tiempos_inicio\"]),\n",
    "                    freq_muestreo=resultado[\"freq_muestreo\"],\n",
    "                    nombre_canal=resultado[\"nombre_canal\"]\n",
    "                )\n",
    "\n",
    "                resumen_rows.append({\n",
    "                    \"Paciente\": key,\n",
    "                    \"Canal\": canal,\n",
    "                    \"Tipo\": meta[\"tipo\"],\n",
    "                    \"Fs\": meta[\"freq\"],\n",
    "                    \"N_Ventanas\": n_vent,\n",
    "                    \"Archivo\": str(out_file)\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                errores.append((key, canal, str(e)))\n",
    "\n",
    "    except Exception as e:\n",
    "        errores.append((key, \"__paciente__\", str(e)))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN GLOBAL\n",
    "# ============================================================\n",
    "df_resumen = pd.DataFrame(resumen_rows)\n",
    "out_csv = ANALYSIS_DIR / \"resumen_global_mas_corto.csv\"\n",
    "df_resumen.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Resumen guardado (SOBREESCRITO) en:\", out_csv)\n",
    "print(\"Totales:\")\n",
    "print(\"  ‚Ä¢ Pacientes procesados:\", len(set([r[\"Paciente\"] for r in resumen_rows])) if resumen_rows else 0)\n",
    "print(\"  ‚Ä¢ Canales procesados:  \", len(resumen_rows))\n",
    "print(\"  ‚Ä¢ Ventanas totales:    \", int(df_resumen[\"N_Ventanas\"].sum()) if not df_resumen.empty else 0)\n",
    "\n",
    "if not df_resumen.empty:\n",
    "    display(df_resumen.head(10))\n",
    "    try:\n",
    "        print(\"\\n Ventanas por tipo de canal:\")\n",
    "        display(df_resumen.groupby(\"Tipo\")[\"N_Ventanas\"].describe())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ============================================================\n",
    "# CONSISTENCIA POR PACIENTE (N¬∫ VENTANAS POR CANAL)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Consistencia interna por paciente (n¬∫ de ventanas por canal)\")\n",
    "consistent_keys = []\n",
    "inconsistent_info = []\n",
    "\n",
    "for pac, canales_dict in patient_channel_windows.items():\n",
    "    counts = list(canales_dict.values())\n",
    "    if len(counts) == 0:\n",
    "        continue\n",
    "    if len(set(counts)) == 1:\n",
    "        consistent_keys.append(pac)\n",
    "    else:\n",
    "        inconsistent_info.append((pac, canales_dict))\n",
    "\n",
    "print(f\"  Pacientes con todos los canales consistentes: {len(consistent_keys)}\")\n",
    "\n",
    "if inconsistent_info:\n",
    "    print(f\"  ‚Ä¢ Pacientes con inconsistencias: {len(inconsistent_info)}\")\n",
    "    print(\"    (mostrando hasta 5 ejemplos)\")\n",
    "    for pac, canales_dict in inconsistent_info[:5]:\n",
    "        print(f\"    - {pac}:\")\n",
    "        for canal, n_vent in canales_dict.items():\n",
    "            print(f\"        {canal}: {n_vent} ventanas\")\n",
    "else:\n",
    "    print(\"   Todos los pacientes tienen el MISMO n√∫mero de ventanas en todos los canales (ignorando EVENTO).\")\n",
    "\n",
    "# ============================================================\n",
    "# DISTRIBUCI√ìN GLOBAL DE ETAPAS (W, N1, N2, N3, REM)\n",
    "# ============================================================\n",
    "if stage_counts:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" Distribuci√≥n global de etapas de sue√±o (por ventana,\")\n",
    "    print(\"   contando SOLO un canal por paciente):\\n\")\n",
    "\n",
    "    etapa_order = [\"W\", \"1\", \"2\", \"3\", \"R\"]\n",
    "    etapa_name = {\n",
    "        \"W\": \"Wake\",\n",
    "        \"1\": \"N1\",\n",
    "        \"2\": \"N2\",\n",
    "        \"3\": \"N3\",   # N3 (ya unifica N3+N4 en el preprocesamiento)\n",
    "        \"R\": \"REM\"\n",
    "    }\n",
    "\n",
    "    total_windows_stages = sum(stage_counts.values())\n",
    "    for et in etapa_order:\n",
    "        if et in stage_counts:\n",
    "            cnt = stage_counts[et]\n",
    "            pct = 100.0 * cnt / total_windows_stages if total_windows_stages > 0 else 0.0\n",
    "            print(f\"   ‚Ä¢ {etapa_name[et]:>3} ({et}): {cnt:5d} ventanas  ‚Üí {pct:5.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n No se pudo calcular la distribuci√≥n de etapas (sin datos de ventanas).\")\n",
    "\n",
    "# ============================================================\n",
    "#  ERRORES (SI LOS HUBO)\n",
    "# ============================================================\n",
    "if errores:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\" Errores encontrados ({len(errores)}): (mostrando hasta 10)\")\n",
    "    for i, (k, c, msg) in enumerate(errores[:10], 1):\n",
    "        print(f\"  {i:02d}. {k} | {c} -> {msg}\")\n",
    "    if len(errores) > 10:\n",
    "        print(f\"  ... y {len(errores)-10} m√°s\")\n",
    "else:\n",
    "    print(\"\\n Sin errores reportados. Todo OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9755f40",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05019bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Cargar y visualizar ventanas (.npz / .pkl)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Mapeos de etiquetas\n",
    "ID2LABEL = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"REM\"}\n",
    "LABEL_MAP = {\n",
    "    \"W\": 0, \"0\": 0,\n",
    "    \"1\": 1, \"N1\": 1,\n",
    "    \"2\": 2, \"N2\": 2,\n",
    "    \"3\": 3, \"4\": 3, \"N3\": 3,\n",
    "    \"R\": 4, \"REM\": 4\n",
    "}\n",
    "\n",
    "def cargar_ventanas(\n",
    "    paciente: str,\n",
    "    canal: str,\n",
    "    return_ids: bool = False,\n",
    "    mmap_npz: bool = True,\n",
    "    windows_dir: Path | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga ventanas de un paciente y canal espec√≠fico desde:\n",
    "      - .npz nuevo o viejo\n",
    "      - .pkl antiguo (fallback)\n",
    "\n",
    "    windows_dir:\n",
    "      - Si es None -> usa la variable global WINDOWS_DIR.\n",
    "      - Si es Path -> usa esa carpeta (sirve para tener carpeta 'larga'\n",
    "        y carpeta 'corta' a la vez).\n",
    "\n",
    "    Devuelve un dict con:\n",
    "      - 'ventanas': array (N, L) o (N, L, C)\n",
    "      - 'etiquetas': array de IDs 0-4 o strings, seg√∫n return_ids\n",
    "      - 'tiempos_inicio': array (N,)\n",
    "      - 'freq_muestreo': float\n",
    "      - 'nombre_canal': str\n",
    "    \"\"\"\n",
    "    base_dir = Path(windows_dir) if windows_dir is not None else WINDOWS_DIR\n",
    "    base = base_dir / f\"{paciente}_{canal.replace(' ', '_')}\"\n",
    "    npz_path = base.with_suffix(\".npz\")\n",
    "    pkl_path = base.with_suffix(\".pkl\")\n",
    "\n",
    "    # --------------------\n",
    "    # Preferir .npz\n",
    "    # --------------------\n",
    "    if npz_path.exists():\n",
    "        d = np.load(\n",
    "            npz_path,\n",
    "            allow_pickle=True,   # <- IMPORTANTE para etiquetas dtype=object\n",
    "            mmap_mode='r' if mmap_npz else None\n",
    "        )\n",
    "        files = set(d.files)\n",
    "\n",
    "        # Formato viejo: X, y, t, fs, canal\n",
    "        if {\"X\", \"y\", \"t\", \"fs\", \"canal\"}.issubset(files):\n",
    "            X = d[\"X\"].astype(np.float32, copy=False)\n",
    "            labels_raw = d[\"y\"]\n",
    "            t = d[\"t\"].astype(np.float32, copy=False)\n",
    "            fs = float(d[\"fs\"])\n",
    "            canal_name = str(d[\"canal\"])\n",
    "\n",
    "        # Formato nuevo: ventanas, etiquetas, tiempos_inicio, freq_muestreo, nombre_canal\n",
    "        elif {\"ventanas\", \"etiquetas\", \"tiempos_inicio\",\n",
    "              \"freq_muestreo\", \"nombre_canal\"}.issubset(files):\n",
    "            X = d[\"ventanas\"].astype(np.float32, copy=False)\n",
    "            labels_raw = d[\"etiquetas\"]\n",
    "            t = d[\"tiempos_inicio\"].astype(np.float32, copy=False)\n",
    "            fs = float(d[\"freq_muestreo\"])\n",
    "            canal_name = str(d[\"nombre_canal\"])\n",
    "\n",
    "        else:\n",
    "            print(f\" Formato .npz desconocido: {npz_path.name}, claves={d.files}\")\n",
    "            return None\n",
    "\n",
    "        labels_raw = np.asarray(labels_raw)\n",
    "        if labels_raw.dtype.kind in {\"U\", \"S\", \"O\"}:\n",
    "            y = np.array([LABEL_MAP.get(str(s), 255) for s in labels_raw],\n",
    "                         dtype=np.uint8)\n",
    "        else:\n",
    "            y = labels_raw.astype(np.uint8, copy=False)\n",
    "        fmt = \".npz\"\n",
    "\n",
    "    # --------------------\n",
    "    # Fallback: .pkl antiguo\n",
    "    # --------------------\n",
    "    elif pkl_path.exists():\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            raw = pickle.load(f)\n",
    "\n",
    "        labels_raw = raw.get(\"etiquetas\", [])\n",
    "        if labels_raw and isinstance(labels_raw[0], str):\n",
    "            y = np.array([LABEL_MAP.get(s, 255) for s in labels_raw], dtype=np.uint8)\n",
    "        else:\n",
    "            y = np.asarray(labels_raw, dtype=np.uint8)\n",
    "\n",
    "        X = np.asarray(raw.get(\"ventanas\", []), dtype=np.float32)\n",
    "        t = np.asarray(raw.get(\"tiempos_inicio\", []), dtype=np.float32)\n",
    "        fs = float(raw.get(\"freq_muestreo\", 100.0))\n",
    "        canal_name = raw.get(\"nombre_canal\", \"CANAL\")\n",
    "        fmt = \".pkl\"\n",
    "\n",
    "    else:\n",
    "        print(f\" No se encontr√≥ ni {npz_path.name} ni {pkl_path.name}\")\n",
    "        return None\n",
    "\n",
    "    # --------------------\n",
    "    # Preparar etiquetas para salida\n",
    "    # --------------------\n",
    "    n = X.shape[0]\n",
    "    total = max(1, n)\n",
    "\n",
    "    if return_ids:\n",
    "        y_out = y\n",
    "        dist_keys = [ID2LABEL.get(int(v), f\"id{int(v)}\") for v in y]\n",
    "    else:\n",
    "        y_out = np.array([ID2LABEL.get(int(v), f\"id{int(v)}\") for v in y],\n",
    "                         dtype=object)\n",
    "        dist_keys = y_out\n",
    "\n",
    "    dist = Counter(dist_keys)\n",
    "    print(f\" Cargado ({fmt}): {paciente} - {canal_name}\")\n",
    "    print(f\"   ‚Ä¢ Ventanas: {X.shape} | Fs: {fs} Hz\")\n",
    "    print(f\"   ‚Ä¢ Distribuci√≥n:\")\n",
    "    for k, c in sorted(dist.items()):\n",
    "        print(f\"     {k}: {c} ({100.0*c/total:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        \"ventanas\": X,\n",
    "        \"etiquetas\": y_out,\n",
    "        \"tiempos_inicio\": t,\n",
    "        \"freq_muestreo\": fs,\n",
    "        \"nombre_canal\": canal_name\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Recuento global de etiquetas \n",
    "# ================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta base \n",
    "BASE_DIR = Path(\n",
    "    r\"C:\\Users\\shipa\\OneDrive\\Escritorio\\Inteligencia Computacional\"\n",
    "    r\"\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\"\n",
    "    r\"\\sleep-cassette\\ventanas_out_mas_corto\"\n",
    ")\n",
    "\n",
    "# Carpeta donde est√°n los .npz nuevos (las ventanas recortadas)\n",
    "WINDOWS_DIR  = BASE_DIR / \"ventanas_extraidas_mas_corto\"\n",
    "\n",
    "# Carpeta donde est√° el resumen \"mas corto\"\n",
    "ANALYSIS_DIR = BASE_DIR / \"analisis_canales_mas_corto\"\n",
    "\n",
    "RESUMEN_CSV = ANALYSIS_DIR / \"resumen_global_mas_corto.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(RESUMEN_CSV)\n",
    "\n",
    "# Obtener pares √∫nicos PACIENTE‚ÄìCANAL\n",
    "pares = (\n",
    "    df[[\"Paciente\", \"Canal\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "conteo_global = Counter()\n",
    "\n",
    "def extraer_etiquetas(data: dict):\n",
    "    \"\"\"Extrae etiquetas en str desde el dict de cargar_ventanas().\"\"\"\n",
    "    y = np.asarray(data.get(\"etiquetas\", []))\n",
    "    return [str(e) for e in y]\n",
    "\n",
    "\n",
    "for _, row in pares.iterrows():\n",
    "    paciente, canal = row[\"Paciente\"], row[\"Canal\"]\n",
    "\n",
    "    buf = io.StringIO()\n",
    "    try:\n",
    "        # Silenciar prints de cargar_ventanas\n",
    "        with redirect_stdout(buf):\n",
    "            data = cargar_ventanas(paciente, canal, return_ids=False)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    etiquetas = extraer_etiquetas(data)\n",
    "    conteo_global.update(etiquetas)\n",
    "\n",
    "\n",
    "# ---- Imprimir resultados ----\n",
    "total = sum(conteo_global.values())\n",
    "\n",
    "if total == 0:\n",
    "    print(\"No se encontraron etiquetas.\")\n",
    "else:\n",
    "    orden = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "    orden += [e for e in sorted(conteo_global) if e not in orden]\n",
    "\n",
    "    print(\" | \".join(\n",
    "        f\"{etapa}: {conteo_global.get(etapa,0) / total * 100:.2f}%\"\n",
    "        for etapa in orden\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbdb81",
   "metadata": {},
   "source": [
    "# Gr√°fico de cada se√±al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WINDOWS_DIR_CORTO = WINDOWS_DIR\n",
    "WINDOWS_DIR_LARGO = Path(\n",
    "    r\"C:\\Users\\shipa\\OneDrive\\Escritorio\\Inteligencia Computacional\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\\ventanas_out\\ventanas_extraidas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "def plot_examen_y_ventanas(\n",
    "    idx_paciente=0,\n",
    "    canal_preferido=\"EEG Fpz-Cz\",\n",
    "    win_sec=30.0,\n",
    "    decim_factor=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Panel 1: PSG completo (se√±al EEG decimada).\n",
    "    Panel 2: Hipnograma (etapas por epoch).\n",
    "    Panel 3: Ventanas:\n",
    "        - Capa superior: ventanas 'largas' (todo el examen).\n",
    "        - Capa inferior: ventanas 'cortas' (dataset recortado).\n",
    "    Todo alineado en tiempo (horas desde inicio PSG).\n",
    "    \"\"\"\n",
    "    # -----------------------\n",
    "    # 0) Buscar par PSG/HYP\n",
    "    # -----------------------\n",
    "    pares = encontrar_pares(RAW_DIR)\n",
    "    if not pares:\n",
    "        print(\"No hay pares PSG/Hyp en RAW_DIR\")\n",
    "        return\n",
    "    if idx_paciente >= len(pares):\n",
    "        print(f\"idx_paciente={idx_paciente} fuera de rango (hay {len(pares)} pares)\")\n",
    "        return\n",
    "\n",
    "    paciente, psg_path, hyp_path = pares[idx_paciente]\n",
    "    print(f\"Paciente: {paciente}\")\n",
    "    print(f\"  PSG:  {psg_path.name}\")\n",
    "    print(f\"  HYP:  {hyp_path.name}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) Leer PSG y canal EEG\n",
    "    # -----------------------\n",
    "    raw = mne.io.read_raw_edf(str(psg_path), preload=True, verbose=False)\n",
    "\n",
    "    if canal_preferido in raw.ch_names:\n",
    "        canal = canal_preferido\n",
    "    else:\n",
    "        eeg_cands = [ch for ch in raw.ch_names if \"EEG\" in ch.upper()]\n",
    "        if not eeg_cands:\n",
    "            print(\"No se encontraron canales EEG en este archivo.\")\n",
    "            return\n",
    "        canal = eeg_cands[0]\n",
    "\n",
    "    print(f\"Canal usado: {canal}\")\n",
    "\n",
    "    data, _ = raw[canal, :]\n",
    "    x_full = data.flatten()\n",
    "    t_full = raw.times  # segundos\n",
    "\n",
    "    if decim_factor > 1:\n",
    "        x_plot = x_full[::decim_factor]\n",
    "        t_plot = t_full[::decim_factor]\n",
    "    else:\n",
    "        x_plot = x_full\n",
    "        t_plot = t_full\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Hipnograma\n",
    "    # -----------------------\n",
    "    hyp_df = leer_hypnograma_mne(hyp_path)\n",
    "    offset = calcular_offset_segundos(psg_path, hyp_path)\n",
    "\n",
    "    starts = hyp_df[\"inicio\"].to_numpy(float) + offset\n",
    "    ends   = (hyp_df[\"inicio\"] + hyp_df[\"duracion\"]).to_numpy(float) + offset\n",
    "    etapas = hyp_df[\"etapa\"].tolist()\n",
    "\n",
    "    stage_order = [\"W\", \"1\", \"2\", \"3\", \"4\", \"R\"]\n",
    "    stage_to_y = {st: i for i, st in enumerate(stage_order)}\n",
    "    stage_to_label = {\n",
    "        \"W\": \"Wake (W)\",\n",
    "        \"1\": \"N1\",\n",
    "        \"2\": \"N2\",\n",
    "        \"3\": \"N3\",\n",
    "        \"4\": \"N3/N4\",\n",
    "        \"R\": \"REM\",\n",
    "    }\n",
    "    colors_stages = {\n",
    "        \"W\":  \"#f4d03f\",\n",
    "        \"1\":  \"#e67e22\",\n",
    "        \"2\":  \"#3498db\",\n",
    "        \"3\":  \"#2ecc71\",\n",
    "        \"4\":  \"#27ae60\",\n",
    "        \"R\":  \"#9b59b6\",\n",
    "    }\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Ventanas largas y cortas\n",
    "    # -----------------------\n",
    "    # Dataset \"corto\" (carpeta actual WINDOWS_DIR_CORTO)\n",
    "    data_corto = cargar_ventanas(\n",
    "        paciente, canal, return_ids=False, windows_dir=WINDOWS_DIR_CORTO\n",
    "    )\n",
    "    if data_corto is None:\n",
    "        print(\"No se pudieron cargar ventanas 'cortas'\")\n",
    "        return\n",
    "\n",
    "    etiquetas_corto = np.asarray(data_corto[\"etiquetas\"])\n",
    "    t_corto = np.asarray(data_corto[\"tiempos_inicio\"])  # segundos\n",
    "\n",
    "    # Dataset \"largo\" (carpeta WINDOWS_DIR_LARGO)\n",
    "    data_largo = cargar_ventanas(\n",
    "        paciente, canal, return_ids=False, windows_dir=WINDOWS_DIR_LARGO\n",
    "    )\n",
    "    if data_largo is None:\n",
    "        print(\"No se pudieron cargar ventanas 'largas' (examen completo)\")\n",
    "        return\n",
    "\n",
    "    etiquetas_largo = np.asarray(data_largo[\"etiquetas\"])\n",
    "    t_largo = np.asarray(data_largo[\"tiempos_inicio\"])\n",
    "\n",
    "    # Mismos colores pero con claves ya fusionadas (W, N1, N2, N3, REM)\n",
    "    colors_labels = {\n",
    "        \"W\":   \"#f4d03f\",\n",
    "        \"N1\":  \"#e67e22\",\n",
    "        \"N2\":  \"#3498db\",\n",
    "        \"N3\":  \"#2ecc71\",\n",
    "        \"REM\": \"#9b59b6\",\n",
    "    }\n",
    "\n",
    "    # -----------------------\n",
    "    # 4) Figuras\n",
    "    # -----------------------\n",
    "    fig, (ax_sig, ax_hyp, ax_win) = plt.subplots(\n",
    "        3, 1,\n",
    "        figsize=(16, 10),\n",
    "        sharex=False,\n",
    "        gridspec_kw={\"height_ratios\": [3, 1.5, 1.8]}\n",
    "    )\n",
    "\n",
    "    # Panel 1: se√±al completa\n",
    "    ax_sig.plot(t_plot / 3600.0, x_plot, linewidth=0.4, color=\"black\", alpha=0.7)\n",
    "    ax_sig.set_ylabel(\"Amplitud\")\n",
    "    ax_sig.set_title(f\"{paciente} ‚Äî {canal}\")\n",
    "    ax_sig.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 2: hipnograma\n",
    "    for s, e, st in zip(starts, ends, etapas):\n",
    "        if st not in stage_to_y:\n",
    "            continue\n",
    "        y = stage_to_y[st]\n",
    "        ax_hyp.hlines(\n",
    "            y,\n",
    "            s / 3600.0,\n",
    "            e / 3600.0,\n",
    "            colors=colors_stages.get(st, \"gray\"),\n",
    "            linewidth=8,\n",
    "            alpha=0.9,\n",
    "        )\n",
    "\n",
    "    ax_hyp.set_yticks([stage_to_y[st] for st in stage_order])\n",
    "    ax_hyp.set_yticklabels([stage_to_label.get(st, st) for st in stage_order])\n",
    "    ax_hyp.set_ylabel(\"Etapas\\n(hipnograma)\")\n",
    "    ax_hyp.grid(True, axis=\"x\", alpha=0.2)\n",
    "\n",
    "    # Leyenda hipnograma\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=colors_stages[st], label=stage_to_label.get(st, st))\n",
    "        for st in stage_order\n",
    "        if st in colors_stages\n",
    "    ]\n",
    "    ax_hyp.legend(handles=legend_handles, title=\"Etapas de Sue√±o\",\n",
    "                  loc=\"upper right\", bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # Panel 3: ventanas largas vs cortas\n",
    "    ax_win.set_title(\"Ventanas recortadas vs completas (alineadas en tiempo)\")\n",
    "    ax_win.set_xlabel(\"Tiempo (horas desde inicio PSG)\")\n",
    "    ax_win.set_yticks([0.25, 0.75])\n",
    "    ax_win.set_yticklabels([\"CORTO\", \"LARGO\"])\n",
    "\n",
    "    # Helper para dibujar barras de ventanas\n",
    "    def _plot_windows(ax, t_starts, labels, y_center, height, label):\n",
    "        for ti, lab in zip(t_starts, labels):\n",
    "            lab_str = str(lab)\n",
    "            # etiquetas vienen como 'W', 'N1', etc (ya mapeado)\n",
    "            color = colors_labels.get(lab_str, \"gray\")\n",
    "            ax.add_patch(\n",
    "                mpatches.Rectangle(\n",
    "                    (ti / 3600.0, y_center - height / 2),\n",
    "                    width=win_sec / 3600.0,\n",
    "                    height=height,\n",
    "                    color=color,\n",
    "                    alpha=0.9 if label == \"corto\" else 0.3,  # largas m√°s transparentes\n",
    "                    linewidth=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    _plot_windows(ax_win, t_largo, etiquetas_largo, y_center=0.75, height=0.35, label=\"largo\")\n",
    "    _plot_windows(ax_win, t_corto, etiquetas_corto, y_center=0.25, height=0.35, label=\"corto\")\n",
    "\n",
    "    # Leyenda para etapas (reutilizamos colors_labels)\n",
    "    leg_handles2 = [\n",
    "        mpatches.Patch(color=c, label=l) for l, c in colors_labels.items()\n",
    "    ]\n",
    "    ax_win.legend(handles=leg_handles2, title=\"Etapas (ventanas)\",\n",
    "                  loc=\"upper right\", bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # Limites X: todo el PSG\n",
    "    ax_win.set_xlim(t_plot[0] / 3600.0, t_plot[-1] / 3600.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "plot_examen_y_ventanas(idx_paciente=0, canal_preferido=\"EEG Fpz-Cz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RECONSTRUIR resumen_global_mas_corto.csv DESDE LOS .NPZ NUEVOS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "rows = []\n",
    "\n",
    "for path in sorted(WINDOWS_DIR.glob(\"*.npz\")):\n",
    "    stem = path.stem  # ej: \"SC4001E_EEG_Fpz-Cz\"\n",
    "    paciente, canal_stub = stem.split(\"_\", 1)\n",
    "    canal = canal_stub.replace(\"_\", \" \")          # \"EEG Fpz-Cz\"\n",
    "\n",
    "    data = cargar_ventanas(paciente, canal, return_ids=True, mmap_npz=True)\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    X = data[\"ventanas\"]\n",
    "    fs = float(data[\"freq_muestreo\"])\n",
    "    n_ventanas = X.shape[0]\n",
    "\n",
    "    # Tipo de canal r√°pido\n",
    "    cname_up = canal.upper()\n",
    "    if \"EEG\" in cname_up:\n",
    "        tipo = \"EEG\"\n",
    "    elif \"EOG\" in cname_up:\n",
    "        tipo = \"EOG\"\n",
    "    elif \"EMG\" in cname_up:\n",
    "        tipo = \"EMG\"\n",
    "    else:\n",
    "        tipo = \"OTRO\"\n",
    "\n",
    "    rows.append({\n",
    "        \"Paciente\": paciente,\n",
    "        \"Canal\": canal,\n",
    "        \"Tipo\": tipo,\n",
    "        \"Fs\": fs,\n",
    "        \"N_Ventanas\": n_ventanas,\n",
    "        \"Archivo\": str(path)\n",
    "    })\n",
    "\n",
    "df_new = pd.DataFrame(rows)\n",
    "out_csv = ANALYSIS_DIR / \"resumen_global_mas_corto.csv\"\n",
    "df_new.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"Nuevo resumen reconstruido desde los .npz cortos\")\n",
    "print(\"Guardado en:\", out_csv)\n",
    "display(df_new.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "data = cargar_ventanas(\"SC4001E\", \"EEG Fpz-Cz\", return_ids=False)\n",
    "y = np.asarray(data[\"etiquetas\"])\n",
    "\n",
    "dist = Counter(y)\n",
    "total = len(y)\n",
    "print(\"Distribuci√≥n sobre ESTE .npz:\")\n",
    "for k, v in sorted(dist.items()):\n",
    "    print(f\"  {k}: {v} ({100*v/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b26664",
   "metadata": {},
   "source": [
    "# Separaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASETS CoSleepNet: RAW + DCT \n",
    "# - Genera NPZ por paciente y un √≠ndice CSV por dataset\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from scipy.fft import dct   # DCT tipo-II\n",
    "\n",
    "# ---------- CONFIG GENERAL ----------\n",
    "ANALYSIS_FILE = ANALYSIS_DIR / \"resumen_global_mas_corto.csv\"\n",
    "\n",
    "# EDF: W, 1, 2, 3, 4, R  -->  W, N1, N2, N3 (3+4), REM\n",
    "LABEL2ID = {\"W\":0, \"N1\":1, \"N2\":2, \"N3\":3, \"REM\":4}\n",
    "ID2LABEL = {v:k for k, v in LABEL2ID.items()}\n",
    "\n",
    "def _etiquetas_to_ids(etiquetas_raw):\n",
    "    \"\"\"['W','1','2','3','4','R'] -> [0..4] con N3 = {3,4}.\"\"\"\n",
    "    etiquetas_raw = np.array(etiquetas_raw, dtype=str)\n",
    "    y = np.empty(len(etiquetas_raw), dtype=np.uint8)\n",
    "    for i, s in enumerate(etiquetas_raw):\n",
    "        if s == \"W\":\n",
    "            y[i] = LABEL2ID[\"W\"]\n",
    "        elif s == \"1\":\n",
    "            y[i] = LABEL2ID[\"N1\"]\n",
    "        elif s == \"2\":\n",
    "            y[i] = LABEL2ID[\"N2\"]\n",
    "        elif s in (\"3\", \"4\"):\n",
    "            y[i] = LABEL2ID[\"N3\"]\n",
    "        elif s == \"R\":\n",
    "            y[i] = LABEL2ID[\"REM\"]\n",
    "        else:\n",
    "            y[i] = 255   # etiqueta rara\n",
    "    return y\n",
    "\n",
    "# Aliases de canales\n",
    "CHANNEL_PATTERNS = {\n",
    "    \"EEG1\": [\"EEG Fpz-Cz\", \"Fpz-Cz\"],\n",
    "    \"EEG2\": [\"EEG Pz-Oz\", \"Pz-Oz\"],\n",
    "    \"EOG\" : [\"EOG horizontal\", \"EOG\", \"EOG horizontal derivation\"],\n",
    "}\n",
    "\n",
    "# Datasets CoSleepNet\n",
    "# 1: EEG1 + EEG2   (paper)\n",
    "# 2: EEG1 + EOG\n",
    "# 3: EEG1 + EEG2 + EOG  \n",
    "RECIPES = {\n",
    "    1: ([\"EEG1\", \"EEG2\"],              \"cosleep_ds1_eeg1_eeg2\"),\n",
    "    2: ([\"EEG1\", \"EOG\"],               \"cosleep_ds2_eeg1_eog\"),\n",
    "    3: ([\"EEG1\", \"EEG2\", \"EOG\"],       \"cosleep_ds3_eeg1_eeg2_eog\"),\n",
    "}\n",
    "\n",
    "LIMIT_PATIENTS   = None     # None o int para debug (limitar n¬∫ pacientes)\n",
    "DATASET_DIR      = Path(\"datasets_cosleepnet_on_disk\")\n",
    "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Utils ----------\n",
    "\n",
    "def pick_channel_name(dfp: pd.DataFrame, aliases: list[str]) -> str | None:\n",
    "    \"\"\"Elige el nombre real de canal que matchee alg√∫n alias.\"\"\"\n",
    "    names = list(dfp[\"Canal\"].unique())\n",
    "    u_names = [n.upper() for n in names]\n",
    "    for alias in aliases:\n",
    "        alias_u = alias.upper()\n",
    "        # match exacto\n",
    "        for n, u in zip(names, u_names):\n",
    "            if u == alias_u:\n",
    "                return n\n",
    "        # substring\n",
    "        for n, u in zip(names, u_names):\n",
    "            if alias_u in u:\n",
    "                return n\n",
    "    return None\n",
    "\n",
    "def load_windows_npz(path: Path):\n",
    "    \"\"\"Carga el .npz creado en el preprocesamiento principal.\"\"\"\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    X = d[\"ventanas\"].astype(np.float32)          # (N, L)\n",
    "    etiquetas = d[\"etiquetas\"].astype(str)        # (N,)\n",
    "    t = d[\"tiempos_inicio\"].astype(np.float32)    # (N,)\n",
    "    fs = float(d[\"freq_muestreo\"])\n",
    "    canal = str(d[\"nombre_canal\"])\n",
    "    return X, etiquetas, t, fs, canal\n",
    "\n",
    "def compute_dct_per_channel(X_raw):\n",
    "    \"\"\"\n",
    "    X_raw: (N, L, C)  -> DCT por canal en eje temporal (L).\n",
    "    Devuelve X_dct con misma shape.\n",
    "    \"\"\"\n",
    "    X_ncL = np.transpose(X_raw, (0, 2, 1))          # (N, C, L)\n",
    "    X_dct_ncL = dct(X_ncL, type=2, axis=2, norm=\"ortho\")\n",
    "    X_dct = np.transpose(X_dct_ncL, (0, 2, 1)).astype(np.float32)\n",
    "    return X_dct\n",
    "\n",
    "# ---------- Cargar resumen y lista de pacientes ----------\n",
    "df = pd.read_csv(ANALYSIS_FILE)\n",
    "patients_all = sorted(df[\"Paciente\"].unique())\n",
    "if LIMIT_PATIENTS is not None:\n",
    "    patients_all = patients_all[:LIMIT_PATIENTS]\n",
    "\n",
    "print(f\" Pacientes disponibles en resumen_global_mas_corto: {len(patients_all)}\")\n",
    "\n",
    "# ============================================================\n",
    "# CONSTRUCCI√ìN *ON DISK* (NPZ POR PACIENTE + √çNDICE CSV)\n",
    "# ============================================================\n",
    "for ds_id, (ch_keys, tag) in RECIPES.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\" Construyendo dataset {ds_id}: canales {ch_keys}  [{tag}]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    index_rows = []   # para index CSV\n",
    "    total_by_class = np.zeros(5, dtype=np.int64)\n",
    "\n",
    "    for p in patients_all:\n",
    "        dfp = df[df[\"Paciente\"] == p]\n",
    "\n",
    "        # 1) nombres reales de canales\n",
    "        chosen = {}\n",
    "        for ck in ch_keys:\n",
    "            ch_name = pick_channel_name(dfp, CHANNEL_PATTERNS[ck])\n",
    "            if ch_name is None:\n",
    "                chosen = None\n",
    "                break\n",
    "            chosen[ck] = ch_name\n",
    "\n",
    "        if chosen is None:\n",
    "            continue  # paciente no tiene todos los canales requeridos\n",
    "\n",
    "        # 2) cargar ventanas por canal y alinear por tiempo\n",
    "        times_rounded = {}\n",
    "        X_by_ck = {}\n",
    "        y_raw_by_ck = {}\n",
    "\n",
    "        for ck, ch_name in chosen.items():\n",
    "            row = dfp[dfp[\"Canal\"] == ch_name].iloc[0]\n",
    "            npz_path = Path(row[\"Archivo\"])\n",
    "            X, etiquetas_raw, t, fs, canal_real = load_windows_npz(npz_path)\n",
    "\n",
    "            X_by_ck[ck] = X\n",
    "            y_raw_by_ck[ck] = np.array(etiquetas_raw, dtype=str)\n",
    "            times_rounded[ck] = np.round(t.astype(np.float32), 4)\n",
    "\n",
    "        # intersecci√≥n de tiempos\n",
    "        common = set(times_rounded[ch_keys[0]])\n",
    "        for ck in ch_keys[1:]:\n",
    "            common &= set(times_rounded[ck])\n",
    "        if not common:\n",
    "            continue\n",
    "\n",
    "        common_sorted = np.array(sorted(list(common)), dtype=np.float32)\n",
    "\n",
    "        idx_maps = {}\n",
    "        for ck in ch_keys:\n",
    "            t = times_rounded[ck]\n",
    "            t2idx = {float(tt): i for i, tt in enumerate(t)}\n",
    "            idx_maps[ck] = [t2idx[float(tt)] for tt in common_sorted]\n",
    "\n",
    "        # 3) recortar y chequear longitudes\n",
    "        X_aligned_list = []\n",
    "        y_ref = None\n",
    "        for ck in ch_keys:\n",
    "            X_ch = X_by_ck[ck][idx_maps[ck]]  # (n, L_ck)\n",
    "            if y_ref is None:\n",
    "                y_ref = y_raw_by_ck[ck][idx_maps[ck]]\n",
    "            X_aligned_list.append(X_ch)\n",
    "\n",
    "        L_min = min(X.shape[1] for X in X_aligned_list)\n",
    "        X_aligned_list = [X[:, :L_min] for X in X_aligned_list]\n",
    "\n",
    "        # 4) tensor RAW (n, L_min, C)\n",
    "        X_raw_p = np.stack(X_aligned_list, axis=-1).astype(np.float32)\n",
    "\n",
    "        #  Normalizaci√≥n por paciente y canal (z-score)\n",
    "        mu = X_raw_p.mean(axis=(0, 1), keepdims=True)          # (1,1,C)\n",
    "        sigma = X_raw_p.std(axis=(0, 1), keepdims=True) + 1e-6\n",
    "        X_raw_p = (X_raw_p - mu) / sigma\n",
    "\n",
    "        # 5) DCT por canal\n",
    "        X_dct_p = compute_dct_per_channel(X_raw_p)\n",
    "\n",
    "        # 6) etiquetas -> ids + filtro 255\n",
    "        y_p = _etiquetas_to_ids(y_ref)\n",
    "        valid_mask = y_p != 255\n",
    "        X_raw_p = X_raw_p[valid_mask]\n",
    "        X_dct_p = X_dct_p[valid_mask]\n",
    "        y_p     = y_p[valid_mask]\n",
    "\n",
    "        n_p = y_p.shape[0]\n",
    "        if n_p == 0:\n",
    "            continue\n",
    "\n",
    "        # 7) Guardar ESTE paciente en disco\n",
    "        out_npz = DATASET_DIR / f\"{tag}_{p}.npz\"\n",
    "        np.savez_compressed(\n",
    "            out_npz,\n",
    "            X_raw=X_raw_p.astype(np.float32),\n",
    "            X_dct=X_dct_p.astype(np.float32),\n",
    "            y=y_p.astype(np.uint8),\n",
    "        )\n",
    "\n",
    "        # 8) Actualizar √≠ndices y conteos\n",
    "        counts_by_class = np.bincount(y_p, minlength=5)\n",
    "        total_by_class += counts_by_class\n",
    "\n",
    "        index_rows.append({\n",
    "            \"dataset_id\": ds_id,\n",
    "            \"tag\": tag,\n",
    "            \"paciente\": p,\n",
    "            \"n_ventanas\": int(n_p),\n",
    "            \"file\": str(out_npz),\n",
    "        })\n",
    "\n",
    "        # liberar RAM de este paciente\n",
    "        del X_raw_p, X_dct_p, y_p, X_by_ck, y_raw_by_ck\n",
    "        gc.collect()\n",
    "\n",
    "    # ---- Guardar √≠ndice CSV de este dataset ----\n",
    "    index_df = pd.DataFrame(index_rows)\n",
    "    index_csv = DATASET_DIR / f\"{tag}_index.csv\"\n",
    "    index_df.to_csv(index_csv, index=False)\n",
    "\n",
    "    print(f\"\\n Dataset {ds_id} listo (on-disk):\")\n",
    "    print(f\"   ‚Ä¢ Pacientes con datos: {len(index_rows)}\")\n",
    "    print(f\"   ‚Ä¢ Index CSV: {index_csv}\")\n",
    "    print(f\"   ‚Ä¢ Ventanas por clase:\")\n",
    "    for cls_id, count in enumerate(total_by_class):\n",
    "        print(f\"      - {ID2LABEL[cls_id]}: {int(count)}\")\n",
    "\n",
    "print(\"\\n Estructura generada en disco (por dataset):\")\n",
    "print(\"   ‚Ä¢ NPZ por paciente:  cosleep_dsX_..._SCxxxxE.npz\")\n",
    "print(\"   ‚Ä¢ √çndice:            cosleep_dsX_..._index.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db20262",
   "metadata": {},
   "source": [
    "# Creaci√≥n de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPLIT POR PACIENTE para datasets \n",
    "#  - Reporta n¬∫ de ventanas y distribuci√≥n de etapas por split\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_DIR = Path(\"datasets_cosleepnet_on_disk\")\n",
    "\n",
    "RECIPES = {\n",
    "    1: ([\"EEG1\", \"EEG2\"],              \"cosleep_ds1_eeg1_eeg2\"),\n",
    "    2: ([\"EEG1\", \"EOG\"],               \"cosleep_ds2_eeg1_eog\"),\n",
    "    3: ([\"EEG1\", \"EEG2\", \"EOG\"],       \"cosleep_ds3_eeg1_eeg2_eog\"), \n",
    "}\n",
    "\n",
    "# Si no est√°n en el entorno, definimos el mapeo de etiquetas\n",
    "try:\n",
    "    ID2LABEL\n",
    "except NameError:\n",
    "    ID2LABEL = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"REM\"}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Cargar √≠ndice de un dataset (usa columnas reales: paciente, file)\n",
    "# ------------------------------------------------------------------\n",
    "def load_index(tag):\n",
    "    \"\"\"Carga el CSV index de un dataset y normaliza rutas a NPZ.\"\"\"\n",
    "    index_csv = DATASET_DIR / f\"{tag}_index.csv\"\n",
    "    if not index_csv.exists():\n",
    "        raise FileNotFoundError(f\"No existe: {index_csv}\")\n",
    "\n",
    "    df = pd.read_csv(index_csv)\n",
    "\n",
    "    if \"paciente\" not in df.columns or \"file\" not in df.columns:\n",
    "        raise ValueError(\"El √≠ndice debe contener columnas 'paciente' y 'file'.\")\n",
    "\n",
    "    # normalizar rutas\n",
    "    corrected_paths = []\n",
    "    for fp in df[\"file\"]:\n",
    "        fp = Path(fp)\n",
    "        if fp.is_absolute():\n",
    "            corrected_paths.append(fp)\n",
    "        else:\n",
    "            if str(fp).startswith(str(DATASET_DIR.name)):\n",
    "                corrected_paths.append(DATASET_DIR / fp.name)\n",
    "            else:\n",
    "                corrected_paths.append(DATASET_DIR / fp)\n",
    "\n",
    "    df[\"npz_path\"] = corrected_paths\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Split por paciente \n",
    "# ------------------------------------------------------------------\n",
    "def split_patients(patient_list, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    patients = np.array(sorted(set(patient_list)))  # √∫nicos\n",
    "\n",
    "    idx = np.arange(len(patients))\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        idx, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    rel_val = val_size / (1.0 - test_size)\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_val_idx, test_size=rel_val, random_state=random_state + 1, shuffle=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_patients\": patients[train_idx].tolist(),\n",
    "        \"val_patients\":   patients[val_idx].tolist(),\n",
    "        \"test_patients\":  patients[test_idx].tolist(),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Resumen de pacientes + ventanas + etapas\n",
    "# ------------------------------------------------------------------\n",
    "def print_patient_split_with_stages(df_index: pd.DataFrame, splits: dict, name: str):\n",
    "    train_p = set(splits[\"train_patients\"])\n",
    "    val_p   = set(splits[\"val_patients\"])\n",
    "    test_p  = set(splits[\"test_patients\"])\n",
    "\n",
    "    inter_tv = train_p & val_p\n",
    "    inter_tt = train_p & test_p\n",
    "    inter_vt = val_p   & test_p\n",
    "\n",
    "    print(f\"\\n===== SPLIT {name} =====\")\n",
    "    print(f\"Pacientes -> Train: {len(train_p)} | Val: {len(val_p)} | Test: {len(test_p)}\")\n",
    "    print(\"Intersecciones pacientes (deben ser 0): \"\n",
    "          f\"Train‚à©Val={len(inter_tv)}, Train‚à©Test={len(inter_tt)}, Val‚à©Test={len(inter_vt)}\")\n",
    "\n",
    "    for split_name, pats in [(\"train\", train_p), (\"val\", val_p), (\"test\", test_p)]:\n",
    "        sub = df_index[df_index[\"paciente\"].isin(pats)]\n",
    "\n",
    "        total_ventanas = int(sub[\"n_ventanas\"].sum())\n",
    "        counts = np.zeros(len(ID2LABEL), dtype=np.int64)\n",
    "\n",
    "        for _, row in sub.iterrows():\n",
    "            npz_path = Path(row[\"npz_path\"])\n",
    "            try:\n",
    "                with np.load(npz_path, allow_pickle=True) as d:\n",
    "                    if \"y\" in d.files:\n",
    "                        y = d[\"y\"].astype(int)\n",
    "                    elif \"labels\" in d.files:\n",
    "                        y = d[\"labels\"].astype(int)\n",
    "                    else:\n",
    "                        continue\n",
    "                uniq, c = np.unique(y, return_counts=True)\n",
    "                for k, v in zip(uniq, c):\n",
    "                    if 0 <= int(k) < len(counts):\n",
    "                        counts[int(k)] += int(v)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error leyendo {npz_path.name}: {e}\")\n",
    "\n",
    "        total_etapas = int(counts.sum())\n",
    "\n",
    "        print(f\"\\n‚Üí {split_name.upper()}:\")\n",
    "        print(f\"   Pacientes: {len(pats)}\")\n",
    "        print(f\"   Ventanas (seg√∫n √≠ndice): {total_ventanas}\")\n",
    "        if total_etapas > 0:\n",
    "            print(f\"   Ventanas (sumando y):  {total_etapas}\")\n",
    "            for cls_id, c in enumerate(counts):\n",
    "                etiqueta = ID2LABEL.get(cls_id, str(cls_id))\n",
    "                perc = c / total_etapas * 100.0\n",
    "                print(f\"   - {etiqueta}: {c} ({perc:.1f}%)\")\n",
    "        else:\n",
    "            print(\"   (No se pudieron leer etiquetas 'y' en los NPZ)\")\n",
    "\n",
    "# ============================================================\n",
    "#             SPLITS DS1, DS2 y DS3\n",
    "# ============================================================\n",
    "\n",
    "splits = {}\n",
    "\n",
    "for ds_id, (ch_keys, tag) in RECIPES.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Creando SPLIT para dataset {ds_id}: {tag}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    df_index = load_index(tag)\n",
    "    patients = df_index[\"paciente\"].unique().tolist()\n",
    "\n",
    "    s = split_patients(patients, test_size=0.20, val_size=0.20, random_state=42)\n",
    "    splits[ds_id] = s\n",
    "\n",
    "    print_patient_split_with_stages(df_index, s, name=tag)\n",
    "\n",
    "print(\"\\n Splits listos. Puedes usarlos para cargar NPZ por paciente durante entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb15b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((DATASET_DIR / \"cosleep_ds1_eeg1_eeg2_index.csv\").resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0afbf",
   "metadata": {},
   "source": [
    "# DataLoader de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset PyTorch para CoSleepNet \n",
    "#   - Devuelve (C, T, 1) por muestra\n",
    "#   - Batch queda (N, C, T, 1)\n",
    "#   - Compatible con Conv2d kernel=(3,1)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "DATASET_DIR = Path(\"datasets_cosleepnet_on_disk\")\n",
    "ID2LABEL = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"REM\"}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cargar index y normalizar rutas\n",
    "# ------------------------------------------------------------\n",
    "def load_index_pytorch(tag: str) -> pd.DataFrame:\n",
    "    index_csv = DATASET_DIR / f\"{tag}_index.csv\"\n",
    "    if not index_csv.exists():\n",
    "        raise FileNotFoundError(f\"No existe: {index_csv}\")\n",
    "\n",
    "    df = pd.read_csv(index_csv)\n",
    "    corrected_paths = []\n",
    "    for fp in df[\"file\"]:\n",
    "        fp = Path(fp)\n",
    "        if fp.is_absolute():\n",
    "            corrected_paths.append(fp)\n",
    "        else:\n",
    "            if str(fp).startswith(str(DATASET_DIR.name)):\n",
    "                corrected_paths.append(DATASET_DIR / fp.name)\n",
    "            else:\n",
    "                corrected_paths.append(DATASET_DIR / fp)\n",
    "    df[\"npz_path\"] = corrected_paths\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset RAM\n",
    "# ------------------------------------------------------------\n",
    "class CoSleepNPZDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset que PRECARGA TODO en RAM al inicializarse.\n",
    "\n",
    "    Devuelve:\n",
    "      xr: (C, T, 1)\n",
    "      xd: (C, T, 1)\n",
    "      y : escalar long\n",
    "    \"\"\"\n",
    "    def __init__(self, tag: str, split_info: dict, split_name: str):\n",
    "        super().__init__()\n",
    "        self.tag = tag\n",
    "        self.split_name = split_name\n",
    "\n",
    "        df_index = load_index_pytorch(tag)\n",
    "        key_patients = f\"{split_name}_patients\"\n",
    "        patients_split = set(split_info[key_patients])\n",
    "        sub = df_index[df_index[\"paciente\"].isin(patients_split)].copy()\n",
    "\n",
    "        if sub.empty:\n",
    "            raise ValueError(f\"No hay pacientes para split '{split_name}'\")\n",
    "\n",
    "        file_paths = sub[\"npz_path\"].tolist()\n",
    "        n_files = len(file_paths)\n",
    "\n",
    "        print(f\"Cargando {split_name} en RAM ({n_files} archivos)...\")\n",
    "\n",
    "        all_X_raw, all_X_dct, all_y = [], [], []\n",
    "\n",
    "        for i, path in enumerate(file_paths):\n",
    "            if (i + 1) % 20 == 0 or i == 0 or (i + 1) == n_files:\n",
    "                print(f\"   Archivo {i+1}/{n_files}...\", end=\"\\r\")\n",
    "\n",
    "            d = np.load(path, allow_pickle=False)\n",
    "            all_X_raw.append(d[\"X_raw\"].astype(np.float32))  # (n_win, T, C)\n",
    "            all_X_dct.append(d[\"X_dct\"].astype(np.float32))  # (n_win, T, C)\n",
    "            all_y.append(d[\"y\"].astype(np.int64))            # (n_win,)\n",
    "\n",
    "        print()\n",
    "\n",
    "        self.X_raw = np.concatenate(all_X_raw, axis=0)  # (N_total, T, C)\n",
    "        self.X_dct = np.concatenate(all_X_dct, axis=0)  # (N_total, T, C)\n",
    "        self.y = np.concatenate(all_y, axis=0)          # (N_total,)\n",
    "\n",
    "        del all_X_raw, all_X_dct, all_y\n",
    "\n",
    "        self.total_windows = len(self.y)\n",
    "\n",
    "        mem_gb = (self.X_raw.nbytes + self.X_dct.nbytes + self.y.nbytes) / 1e9\n",
    "        print(f\" {split_name}: {self.total_windows} ventanas cargadas ({mem_gb:.2f} GB RAM)\")\n",
    "\n",
    "        # ======================================================\n",
    "        # DEBUG: distribuci√≥n de clases en este split\n",
    "        # ======================================================\n",
    "        uniq, cnts = np.unique(self.y, return_counts=True)\n",
    "        print(f\"   Distribuci√≥n de clases en split '{self.split_name}':\")\n",
    "        for k, c in zip(uniq, cnts):\n",
    "            nombre = ID2LABEL.get(int(k), f\"id{int(k)}\")\n",
    "            pct = 100.0 * c / max(1, self.total_windows)\n",
    "            print(f\"      - {nombre} (id={int(k)}): {c} ventanas ({pct:.1f}%)\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xr = self.X_raw[idx]   # (T, C)\n",
    "        xd = self.X_dct[idx]   # (T, C)\n",
    "        y  = self.y[idx]\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # (T, C) -> (C, T, 1)\n",
    "        # Batch final: (N, C, T, 1)\n",
    "        # --------------------------------------------------\n",
    "        xr = np.transpose(xr, (1, 0))[:, :, np.newaxis]   # (C, T, 1)\n",
    "        xd = np.transpose(xd, (1, 0))[:, :, np.newaxis]   # (C, T, 1)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(xr),\n",
    "            torch.from_numpy(xd),\n",
    "            torch.tensor(int(y), dtype=torch.long)\n",
    "        )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Loader factory\n",
    "# ------------------------------------------------------------\n",
    "def make_pytorch_loaders(\n",
    "    dataset_id: int,\n",
    "    splits: dict,\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "):\n",
    "    if dataset_id == 1:\n",
    "        tag = \"cosleep_ds1_eeg1_eeg2\"\n",
    "        DATASET_NAME = \"EEG1+EEG2\"\n",
    "\n",
    "    elif dataset_id == 2:\n",
    "        tag = \"cosleep_ds2_eeg1_eog\"\n",
    "        DATASET_NAME = \"EEG1+EOG\"\n",
    "\n",
    "    elif dataset_id == 3:\n",
    "        tag = \"cosleep_ds3_eeg1_eeg2_eog\"\n",
    "        DATASET_NAME = \"EEG1+EEG2+EOG\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"dataset_id debe ser 1, 2 o 3\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # cargar split correcto\n",
    "    # ------------------------------------\n",
    "    split_info = splits[dataset_id]\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Precargando dataset {DATASET_NAME} en RAM...\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    train_set = CoSleepNPZDataset(tag, split_info, \"train\")\n",
    "    val_set   = CoSleepNPZDataset(tag, split_info, \"val\")\n",
    "    test_set  = CoSleepNPZDataset(tag, split_info, \"test\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    xr0, xd0, y0 = train_set[0]\n",
    "    input_shape = tuple(xr0.shape)  # (C, T, 1)\n",
    "\n",
    "    print(f\"\\n Loaders listos para {DATASET_NAME}:\")\n",
    "    print(f\"   Input shape por muestra (C,T,1): {input_shape}\")\n",
    "    print(f\"   Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, input_shape, DATASET_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0672c",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3414ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Modelo CoSleepNet en PyTorch\n",
    "#  - Entrada: (N, C, T, 1)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, cohen_kappa_score, classification_report\n",
    ")\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=(3,1), padding=(1,0))\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=(3,1), padding=(1,0))\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=(3,1), stride=(3,1))\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C, T, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)       # reduce T\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoSleepBranch(nn.Module):\n",
    "    def __init__(self, in_ch, lstm_units=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block1 = CNNBlock(in_ch, 32, dropout)\n",
    "        self.block2 = CNNBlock(32, 64, dropout)\n",
    "        self.block3 = CNNBlock(64, 64, dropout)\n",
    "        self.block4 = CNNBlock(64, 128, dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=lstm_units,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C, T, 1)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        # ‚Üí (N, 128, T', 1)\n",
    "        N, C_feat, T_, _ = x.shape\n",
    "\n",
    "        # ‚Üí (N, T', 128)\n",
    "        x = x.squeeze(3).permute(0,2,1)\n",
    "\n",
    "        out,_ = self.lstm(x)\n",
    "        return out[:,-1,:]\n",
    "\n",
    "\n",
    "class CoSleepNetTorch(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes=5, lstm_units=64, dropout=0.3, use_dct=True):\n",
    "        super().__init__()\n",
    "        self.use_dct = use_dct\n",
    "        self.raw_branch = CoSleepBranch(in_ch, lstm_units, dropout)\n",
    "        if use_dct:\n",
    "            self.dct_branch = CoSleepBranch(in_ch, lstm_units, dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(lstm_units*(2 if use_dct else 1), 128)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, xr, xd=None):\n",
    "        h_raw = self.raw_branch(xr)\n",
    "        if self.use_dct:\n",
    "            h_dct = self.dct_branch(xd)\n",
    "            h = torch.cat([h_raw, h_dct], dim=1)\n",
    "        else:\n",
    "            h = h_raw\n",
    "\n",
    "        x = F.relu(self.fc1(h))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208e989",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# =====================================================\n",
    "#  Focal Loss \n",
    "# =====================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none', weight=self.alpha)\n",
    "        p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        loss = focal_weight * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# =====================================================\n",
    "#  Train con progreso + Macro F1\n",
    "# =====================================================\n",
    "def train_one_epoch_pytorch(model, loader, optimizer, criterion, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    n_batches = len(loader)\n",
    "    \n",
    "    for i, (xr, xd, y) in enumerate(loader):\n",
    "        xr = xr.to(device, non_blocking=True)\n",
    "        xd = xd.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xr, xd)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_samples += bs\n",
    "        total_loss += loss.item() * bs\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(y.cpu().numpy())\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_batches:\n",
    "            print(f\"  [Train] {i+1}/{n_batches} ({100*(i+1)/n_batches:.0f}%)\", end=\"\\r\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    acc = (all_preds == all_labels).mean()\n",
    "    \n",
    "    return avg_loss, acc, macro_f1\n",
    "\n",
    "# =====================================================\n",
    "#  Eval con progreso + Macro F1\n",
    "# =====================================================\n",
    "@torch.no_grad()\n",
    "def eval_pytorch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    n_batches = len(loader)\n",
    "    \n",
    "    for i, (xr, xd, y) in enumerate(loader):\n",
    "        xr = xr.to(device, non_blocking=True)\n",
    "        xd = xd.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(xr, xd)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_samples += bs\n",
    "        total_loss += loss.item() * bs\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(y.cpu().numpy())\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == n_batches:\n",
    "            print(f\"  [Eval]  {i+1}/{n_batches} ({100*(i+1)/n_batches:.0f}%)\", end=\"\\r\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    acc = (all_preds == all_labels).mean()\n",
    "    \n",
    "    return avg_loss, acc, macro_f1, all_labels, all_preds\n",
    "\n",
    "# =====================================================\n",
    "#  Class weights\n",
    "# =====================================================\n",
    "def get_class_weights(loader, n_classes=5, method='sqrt_inverse', device='cpu'):\n",
    "    print(\"  Calculando class weights...\")\n",
    "    all_labels = []\n",
    "    for batch in loader:\n",
    "        all_labels.append(batch[-1].numpy())\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    _, counts = np.unique(all_labels, return_counts=True)\n",
    "    \n",
    "    if method == 'sqrt_inverse':\n",
    "        weights = 1.0 / np.sqrt(counts)\n",
    "    elif method == 'inverse':\n",
    "        weights = 1.0 / counts\n",
    "    else:\n",
    "        weights = np.ones(n_classes)\n",
    "    \n",
    "    weights = weights / weights.sum() * n_classes\n",
    "    print(f\"  Distribuci√≥n: {counts}\")\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\" Celda 1 cargada: Modelo y funciones de entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d773c0",
   "metadata": {},
   "source": [
    "# Ejecuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45361632",
   "metadata": {},
   "source": [
    "# DCT ON   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc6f5a",
   "metadata": {},
   "source": [
    "## Set 1: EEG1 + EEG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64810f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 1        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f37992",
   "metadata": {},
   "source": [
    "## Set 2: EEG1 + EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf37be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 2        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3da36",
   "metadata": {},
   "source": [
    "## Set 3: EEG1 + EEG2 + EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 3        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08f9e0",
   "metadata": {},
   "source": [
    "# DCT OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabdce9",
   "metadata": {},
   "source": [
    "## Set 1: EEG1 + EEG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895572c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 1        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04adeee",
   "metadata": {},
   "source": [
    "## Set 2: EEG1 + EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 2        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c2216",
   "metadata": {},
   "source": [
    "## Set 3: EEG1 + EEG2 + EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ede5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELDA: MULTI-RUN (3‚Äì5 corridas) + MEDIA ¬± DESV. EST√ÅNDAR\n",
    "# + VISUALIZACI√ìN DE LA √öLTIMA CORRIDA\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_ID = 3        # 1, 2 o 3 \n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "USE_FOCAL = True\n",
    "FOCAL_GAMMA = 1.5\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "N_RUNS = 5            \n",
    "BASE_SEED = 42\n",
    "labels = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def kappa_per_class(y_true, y_pred, n_classes=5):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    N = cm.sum()\n",
    "    kappas = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = N - (TP + FN + FP)\n",
    "\n",
    "        obs = (TP + TN) / max(1, N)\n",
    "\n",
    "        p_yes_true = (TP + FN) / max(1, N)\n",
    "        p_yes_pred = (TP + FP) / max(1, N)\n",
    "        p_no_true  = (FP + TN) / max(1, N)\n",
    "        p_no_pred  = (FN + TN) / max(1, N)\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "\n",
    "        kappas[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    return cm, kappas\n",
    "\n",
    "# ---------- Loaders fijos (siempre los mismos splits) ----------\n",
    "train_loader, val_loader, test_loader, input_shape, DATASET_NAME = make_pytorch_loaders(\n",
    "    dataset_id=DATASET_ID,\n",
    "    splits=splits,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "C_in, H, L = input_shape\n",
    "print(f\"\\n Dataset: {DATASET_NAME} | Shape: C={C_in}, H={H}, L={L}\")\n",
    "print(f\" Ejecutando {N_RUNS} corridas independientes...\\n\")\n",
    "\n",
    "# ---------- Loop de runs ----------\n",
    "run_summaries = []\n",
    "per_class_tables = []\n",
    "\n",
    "# Variables para guardar historia de la √∫ltima corrida\n",
    "last_run_history = None\n",
    "last_run_cm = None\n",
    "last_run_test_f1 = None\n",
    "last_run_kappa = None\n",
    "last_run_best_val_f1 = None\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    seed = BASE_SEED + run\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n================ RUN {run+1}/{N_RUNS} ‚Äî seed={seed} ================\")\n",
    "\n",
    "    # Modelo nuevo\n",
    "    model = CoSleepNetTorch(\n",
    "        in_ch=C_in,\n",
    "        num_classes=5,\n",
    "        lstm_units=64,\n",
    "        dropout=0.3,\n",
    "        use_dct=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # Criterion\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = get_class_weights(train_loader, device=device)\n",
    "        print(f\"  Class weights: {class_weights.cpu().numpy().round(3)}\")\n",
    "\n",
    "    if USE_FOCAL:\n",
    "        criterion = FocalLoss(gamma=FOCAL_GAMMA, alpha=class_weights)\n",
    "        print(f\" Loss: Focal (Œ≥={FOCAL_GAMMA})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\" Loss: CrossEntropy\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Historia de entrenamiento\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_f1 = 0.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ Epoch {epoch}/{EPOCHS} ‚îÅ‚îÅ‚îÅ\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch_pytorch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = eval_pytorch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Guardar historia\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(tr_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        improved = val_f1 > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"  Train | Loss: {tr_loss:.4f} | Acc: {tr_acc:.4f} | F1: {tr_f1:.4f}\")\n",
    "        print(f\"  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}{star}\\n\")\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping en epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Mejor estado\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Test\n",
    "    print(\"‚îÅ‚îÅ‚îÅ Evaluando en TEST ‚îÅ‚îÅ‚îÅ\")\n",
    "    test_loss, test_acc, test_f1, y_true, y_pred = eval_pytorch(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "    cm, kappas = kappa_per_class(y_true, y_pred, n_classes=5)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(5), zero_division=0\n",
    "    )\n",
    "\n",
    "    df_metrics = pd.DataFrame({\n",
    "        \"Etapa\": labels,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Kappa\": kappas,\n",
    "        \"Soporte\": support\n",
    "    })\n",
    "\n",
    "    print(f\"\\n RUN {run+1} ‚Äî Resultados globales:\")\n",
    "    print(f\"   Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"   Test Acc:      {test_acc:.4f}\")\n",
    "    print(f\"   Test Macro F1: {test_f1:.4f}\")\n",
    "    print(f\"   Kappa global:  {kappa_global:.4f}\")\n",
    "    print(f\"   Best Val F1:   {best_val_f1:.4f}\")\n",
    "\n",
    "    run_summaries.append({\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": test_acc,\n",
    "        \"macro_f1\": test_f1,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"best_val_f1\": best_val_f1\n",
    "    })\n",
    "    per_class_tables.append(df_metrics)\n",
    "    \n",
    "    # Guardar datos de la √∫ltima corrida\n",
    "    if run == N_RUNS - 1:\n",
    "        last_run_history = history\n",
    "        last_run_cm = cm\n",
    "        last_run_test_f1 = test_f1\n",
    "        last_run_kappa = kappa_global\n",
    "        last_run_best_val_f1 = best_val_f1\n",
    "\n",
    "    #  Liberar GPU entre runs (por si acaso)\n",
    "    del model, optimizer, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Agregados: media ¬± std ----------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "losses  = np.array([r[\"loss\"] for r in run_summaries])\n",
    "accs    = np.array([r[\"acc\"] for r in run_summaries])\n",
    "f1s     = np.array([r[\"macro_f1\"] for r in run_summaries])\n",
    "kappasg = np.array([r[\"kappa\"] for r in run_summaries])\n",
    "\n",
    "def ms(x):  # mean ¬± std string\n",
    "    return f\"{x.mean():.4f} ¬± {x.std():.4f}\"\n",
    "\n",
    "print(f\"Test Loss:      {ms(losses)}\")\n",
    "print(f\"Test Accuracy:  {ms(accs)}\")\n",
    "print(f\"Test Macro F1:  {ms(f1s)}\")\n",
    "print(f\"Kappa global:   {ms(kappasg)}\")\n",
    "\n",
    "# ---------- Agregado por etapa ----------\n",
    "prec_arr  = np.stack([df[\"Precision\"].values for df in per_class_tables], axis=0)\n",
    "rec_arr   = np.stack([df[\"Recall\"].values    for df in per_class_tables], axis=0)\n",
    "f1_arr    = np.stack([df[\"F1-Score\"].values  for df in per_class_tables], axis=0)\n",
    "kappa_arr = np.stack([df[\"Kappa\"].values     for df in per_class_tables], axis=0)\n",
    "\n",
    "df_agg = pd.DataFrame({\"Etapa\": labels})\n",
    "for name, arr in [(\"Precision\", prec_arr),\n",
    "                  (\"Recall\", rec_arr),\n",
    "                  (\"F1-Score\", f1_arr),\n",
    "                  (\"Kappa\", kappa_arr)]:\n",
    "    means = arr.mean(axis=0)\n",
    "    stds  = arr.std(axis=0)\n",
    "    df_agg[name] = [f\"{m:.3f} ¬± {s:.3f}\" for m, s in zip(means, stds)]\n",
    "\n",
    "print(\"\\n M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "display(df_agg)\n",
    "\n",
    "# ---------- VISUALIZACI√ìN DE LA √öLTIMA CORRIDA ----------\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Loss\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "epochs_range = range(1, len(last_run_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_range, last_run_history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs_range, last_run_history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Macro F1 Score\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs_range, last_run_history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
    "ax2.plot(epochs_range, last_run_history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
    "ax2.axhline(y=last_run_best_val_f1, color='g', linestyle='--', \n",
    "            label=f'Best Val F1: {last_run_best_val_f1:.3f}', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Macro F1 Score', fontsize=11)\n",
    "ax2.set_title('Macro F1 Score (m√©trica principal)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Accuracy\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(epochs_range, last_run_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "ax3.plot(epochs_range, last_run_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11)\n",
    "ax3.set_title('Accuracy (secundaria)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# 4. Matriz de Confusi√≥n Normalizada\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "cm_normalized = last_run_cm.astype('float') / last_run_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, \n",
    "            cbar_kws={'label': 'Proporci√≥n'}, ax=ax4, vmin=0, vmax=1)\n",
    "ax4.set_xlabel('Predicho', fontsize=11)\n",
    "ax4.set_ylabel('Real', fontsize=11)\n",
    "ax4.set_title('Matriz de Confusi√≥n (normalizada)', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle(\n",
    "    f'CoSleepNet ‚Äî {DATASET_NAME}\\n'\n",
    "    f'Test F1: {last_run_test_f1:.3f} | Kappa: {last_run_kappa:.3f}',\n",
    "    fontsize=14, fontweight='bold', y=0.98\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
