{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pARKuhCtfguy"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR43INvgXOr6"
      },
      "outputs": [],
      "source": [
        "import os, re, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "import pywt\n",
        "from collections import defaultdict\n",
        "import mne\n",
        "import pyedflib\n",
        "from scipy.signal import welch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sys, time, shutil, subprocess, requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "from multiprocessing import Pool\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal, stats\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5SOXrrYY6YC"
      },
      "source": [
        "# 1. DWT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PSG_DWT_BatchProcessor:\n",
        "    \"\"\"\n",
        "    Procesador batch de se√±ales PSG usando DWT\n",
        "    Procesa TODOS los archivos de la base de datos Sleep-EDF\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, output_path=None):\n",
        "        \"\"\"\n",
        "        Inicializa el procesador batch\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        data_path : str\n",
        "            Ruta a la carpeta con archivos PSG\n",
        "        output_path : str\n",
        "            Ruta donde guardar los resultados (opcional)\n",
        "        \"\"\"\n",
        "        self.data_path = Path(data_path)\n",
        "        self.wavelet = 'db4'  # Daubechies-4\n",
        "        self.level = 4  # 4 niveles\n",
        "        self.epoch_duration = 30  # segundos\n",
        "        \n",
        "        # Configurar carpeta de salida\n",
        "        if output_path is None:\n",
        "            self.output_path = self.data_path / \"dwt_results\"\n",
        "        else:\n",
        "            self.output_path = Path(output_path)\n",
        "        \n",
        "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Para almacenar todos los resultados\n",
        "        self.all_subjects_data = {}\n",
        "        \n",
        "    def load_psg_file(self, filepath):\n",
        "        \"\"\"Carga un archivo PSG\"\"\"\n",
        "        try:\n",
        "            raw = mne.io.read_raw_edf(filepath, preload=True, verbose=False)\n",
        "            return raw\n",
        "        except Exception as e:\n",
        "            print(f\" Error cargando {filepath.name}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_channels(self, raw):\n",
        "        \"\"\"Extrae canales EOG y EEG\"\"\"\n",
        "        channels_data = {}\n",
        "        ch_names = raw.ch_names\n",
        "        sfreq = raw.info['sfreq']\n",
        "        \n",
        "        # Buscar EOG\n",
        "        eog_channels = [ch for ch in ch_names if 'EOG' in ch.upper()]\n",
        "        \n",
        "        # Buscar EEG (t√≠picamente Fpz-Cz y Pz-Oz en Sleep-EDF)\n",
        "        eeg_channels = [ch for ch in ch_names if 'EEG' in ch.upper() or \n",
        "                       'Fpz' in ch or 'Pz' in ch]\n",
        "        \n",
        "        # Extraer EOG\n",
        "        for ch in eog_channels:\n",
        "            data, times = raw[ch, :]\n",
        "            channels_data[ch] = {\n",
        "                'data': data[0],\n",
        "                'sfreq': sfreq,\n",
        "                'type': 'EOG'\n",
        "            }\n",
        "        \n",
        "        # Extraer EEG\n",
        "        for ch in eeg_channels:\n",
        "            data, times = raw[ch, :]\n",
        "            channels_data[ch] = {\n",
        "                'data': data[0],\n",
        "                'sfreq': sfreq,\n",
        "                'type': 'EEG'\n",
        "            }\n",
        "        \n",
        "        return channels_data\n",
        "    \n",
        "    def apply_dwt(self, signal):\n",
        "        \"\"\"\n",
        "        Aplica DWT y retorna los 8 coeficientes\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        dict con A1, A2, A3, A4, D1, D2, D3, D4\n",
        "        \"\"\"\n",
        "        # Descomposici√≥n completa nivel 4\n",
        "        coeffs_4 = pywt.wavedec(signal, self.wavelet, level=4)\n",
        "        # coeffs_4 = [cA4, cD4, cD3, cD2, cD1]\n",
        "        \n",
        "        # Descomposiciones parciales para obtener A1, A2, A3\n",
        "        coeffs_3 = pywt.wavedec(signal, self.wavelet, level=3)\n",
        "        coeffs_2 = pywt.wavedec(signal, self.wavelet, level=2)\n",
        "        coeffs_1 = pywt.wavedec(signal, self.wavelet, level=1)\n",
        "        \n",
        "        dwt_coeffs = {\n",
        "            'A1': coeffs_1[0],\n",
        "            'A2': coeffs_2[0],\n",
        "            'A3': coeffs_3[0],\n",
        "            'A4': coeffs_4[0],\n",
        "            'D1': coeffs_4[4],\n",
        "            'D2': coeffs_4[3],\n",
        "            'D3': coeffs_4[2],\n",
        "            'D4': coeffs_4[1],\n",
        "        }\n",
        "        \n",
        "        return dwt_coeffs\n",
        "    \n",
        "    def segment_into_epochs(self, signal, sfreq):\n",
        "        \"\"\"Segmenta en √©pocas de 30s\"\"\"\n",
        "        samples_per_epoch = int(self.epoch_duration * sfreq)\n",
        "        n_epochs = len(signal) // samples_per_epoch\n",
        "        \n",
        "        epochs = []\n",
        "        for i in range(n_epochs):\n",
        "            start = i * samples_per_epoch\n",
        "            end = start + samples_per_epoch\n",
        "            epochs.append(signal[start:end])\n",
        "        \n",
        "        return epochs\n",
        "    \n",
        "    def process_single_file(self, filepath, subject_id):\n",
        "        \"\"\"\n",
        "        Procesa UN archivo PSG completo\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        dict con los resultados DWT de todos los canales\n",
        "        \"\"\"\n",
        "        print(f\"\\n Procesando: {filepath.name}\")\n",
        "        \n",
        "        # Cargar archivo\n",
        "        raw = self.load_psg_file(filepath)\n",
        "        if raw is None:\n",
        "            return None\n",
        "        \n",
        "        # Extraer canales\n",
        "        channels_data = self.extract_channels(raw)\n",
        "        \n",
        "        if not channels_data:\n",
        "            print(f\"   ‚ö†Ô∏è  No se encontraron canales EOG/EEG\")\n",
        "            return None\n",
        "        \n",
        "        # Procesar cada canal\n",
        "        subject_results = {\n",
        "            'subject_id': subject_id,\n",
        "            'filename': filepath.name,\n",
        "            'channels': {}\n",
        "        }\n",
        "        \n",
        "        for ch_name, ch_data in channels_data.items():\n",
        "            signal = ch_data['data']\n",
        "            sfreq = ch_data['sfreq']\n",
        "            ch_type = ch_data['type']\n",
        "            \n",
        "            # Segmentar en √©pocas\n",
        "            epochs = self.segment_into_epochs(signal, sfreq)\n",
        "            # Aplicar DWT a cada √©poca\n",
        "            epochs_dwt = []\n",
        "            for epoch in epochs:\n",
        "                dwt_coeffs = self.apply_dwt(epoch)\n",
        "                epochs_dwt.append(dwt_coeffs)\n",
        "            \n",
        "            # Guardar resultados del canal\n",
        "            subject_results['channels'][ch_name] = {\n",
        "                'type': ch_type,\n",
        "                'sfreq': sfreq,\n",
        "                'n_epochs': len(epochs),\n",
        "                'dwt_coeffs': epochs_dwt\n",
        "            }\n",
        "        \n",
        "        return subject_results\n",
        "    \n",
        "    def process_all_files(self):\n",
        "        \"\"\"\n",
        "        Procesa TODOS los archivos PSG en la carpeta\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" PROCESAMIENTO BATCH DE TODOS LOS ARCHIVOS SLEEP-EDF\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Buscar todos los archivos EDF\n",
        "        edf_files = sorted(list(self.data_path.glob(\"*.edf\")))\n",
        "        \n",
        "        # Filtrar solo archivos PSG (no hypnogram)\n",
        "        psg_files = [f for f in edf_files if 'PSG' in f.name]\n",
        "        \n",
        "        print(f\"\\n Archivos PSG encontrados: {len(psg_files)}\")\n",
        "        print(f\" Archivos totales EDF: {len(edf_files)}\")\n",
        "        \n",
        "        if not psg_files:\n",
        "            print(\" No se encontraron archivos PSG\")\n",
        "            return\n",
        "        \n",
        "        # Procesar cada archivo\n",
        "        print(f\"\\n  Iniciando procesamiento...\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        successful = 0\n",
        "        failed = 0\n",
        "        \n",
        "        for i, filepath in enumerate(psg_files, 1):\n",
        "            # Extraer ID del sujeto del nombre del archivo\n",
        "            subject_id = filepath.stem.split('-')[0]  # Ej: SC4001E0\n",
        "            \n",
        "            print(f\"\\n[{i}/{len(psg_files)}] Sujeto: {subject_id}\")\n",
        "            \n",
        "            results = self.process_single_file(filepath, subject_id)\n",
        "            \n",
        "            if results:\n",
        "                self.all_subjects_data[subject_id] = results\n",
        "                successful += 1\n",
        "            else:\n",
        "                failed += 1       \n",
        "        return self.all_subjects_data\n",
        "    \n",
        "    def save_results(self, format='pickle'):\n",
        "        \"\"\"\n",
        "        Guarda los resultados en disco\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        format : str\n",
        "            'pickle', 'numpy', o 'both'\n",
        "        \"\"\"\n",
        "        print(f\"\\n Guardando resultados en: {self.output_path}\")\n",
        "        \n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        \n",
        "        if format in ['pickle', 'both']:\n",
        "            # Guardar como pickle (conserva toda la estructura)\n",
        "            pickle_file = self.output_path / f\"dwt_all_subjects_{timestamp}.pkl\"\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(self.all_subjects_data, f)\n",
        "            print(f\"   ‚úì Pickle guardado: {pickle_file.name}\")\n",
        "        \n",
        "        if format in ['numpy', 'both']:\n",
        "            # Guardar como arrays numpy (m√°s ligero pero menos estructura)\n",
        "            numpy_dir = self.output_path / f\"dwt_numpy_{timestamp}\"\n",
        "            numpy_dir.mkdir(exist_ok=True)\n",
        "            \n",
        "            for subject_id, subject_data in self.all_subjects_data.items():\n",
        "                subject_dir = numpy_dir / subject_id\n",
        "                subject_dir.mkdir(exist_ok=True)\n",
        "                \n",
        "                for ch_name, ch_data in subject_data['channels'].items():\n",
        "                    # Convertir lista de diccionarios a arrays numpy\n",
        "                    n_epochs = len(ch_data['dwt_coeffs'])\n",
        "                    \n",
        "                    # Crear arrays para cada coeficiente\n",
        "                    coeff_arrays = {}\n",
        "                    for coeff_name in ['A1', 'A2', 'A3', 'A4', 'D1', 'D2', 'D3', 'D4']:\n",
        "                        coeff_list = [epoch[coeff_name] for epoch in ch_data['dwt_coeffs']]\n",
        "                        coeff_arrays[coeff_name] = np.array(coeff_list)\n",
        "                    \n",
        "                    # Guardar cada coeficiente\n",
        "                    np.savez(\n",
        "                        subject_dir / f\"{ch_name}_dwt.npz\",\n",
        "                        **coeff_arrays,\n",
        "                        sfreq=ch_data['sfreq'],\n",
        "                        n_epochs=n_epochs\n",
        "                    )\n",
        "            \n",
        "            print(f\"   ‚úì Arrays numpy guardados en: {numpy_dir.name}/\")\n",
        "        \n",
        "        # Guardar resumen en CSV\n",
        "        self.save_summary_csv(timestamp)\n",
        "    \n",
        "    def save_summary_csv(self, timestamp):\n",
        "        \"\"\"Guarda un resumen en CSV\"\"\"\n",
        "        summary_data = []\n",
        "        \n",
        "        for subject_id, subject_data in self.all_subjects_data.items():\n",
        "            for ch_name, ch_data in subject_data['channels'].items():\n",
        "                summary_data.append({\n",
        "                    'subject_id': subject_id,\n",
        "                    'filename': subject_data['filename'],\n",
        "                    'channel': ch_name,\n",
        "                    'type': ch_data['type'],\n",
        "                    'sfreq': ch_data['sfreq'],\n",
        "                    'n_epochs': ch_data['n_epochs']\n",
        "                })\n",
        "        \n",
        "        df = pd.DataFrame(summary_data)\n",
        "        csv_file = self.output_path / f\"dwt_summary_{timestamp}.csv\"\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"   ‚úì Resumen CSV guardado: {csv_file.name}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def get_statistics(self):\n",
        "        \"\"\"Obtiene estad√≠sticas del procesamiento\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" ESTAD√çSTICAS DETALLADAS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        total_epochs = 0\n",
        "        channels_count = {'EOG': 0, 'EEG': 0}\n",
        "        \n",
        "        for subject_id, subject_data in self.all_subjects_data.items():\n",
        "            subject_epochs = 0\n",
        "            print(f\"\\nüîπ Sujeto: {subject_id}\")\n",
        "            \n",
        "            for ch_name, ch_data in subject_data['channels'].items():\n",
        "                n_epochs = ch_data['n_epochs']\n",
        "                ch_type = ch_data['type']\n",
        "                \n",
        "                print(f\"   {ch_name} ({ch_type}): {n_epochs} √©pocas\")\n",
        "                \n",
        "                subject_epochs += n_epochs\n",
        "                channels_count[ch_type] += 1\n",
        "            \n",
        "            total_epochs += subject_epochs\n",
        "            print(f\"   Total √©pocas: {subject_epochs}\")\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\" TOTALES:\")\n",
        "        print(f\"   ‚Ä¢ Total sujetos: {len(self.all_subjects_data)}\")\n",
        "        print(f\"   ‚Ä¢ Total canales EOG: {channels_count['EOG']}\")\n",
        "        print(f\"   ‚Ä¢ Total canales EEG: {channels_count['EEG']}\")\n",
        "        print(f\"   ‚Ä¢ Total √©pocas procesadas: {total_epochs}\")\n",
        "        print(f\"   ‚Ä¢ Total coeficientes DWT: {total_epochs * 8}\")\n",
        "    \n",
        "    def visualize_sample(self, subject_id=None, channel=None, epoch_idx=0):\n",
        "        \"\"\"Visualiza una muestra de los resultados\"\"\"\n",
        "        if subject_id is None:\n",
        "            subject_id = list(self.all_subjects_data.keys())[0]\n",
        "        \n",
        "        if subject_id not in self.all_subjects_data:\n",
        "            print(f\" Sujeto {subject_id} no encontrado\")\n",
        "            return\n",
        "        \n",
        "        subject_data = self.all_subjects_data[subject_id]\n",
        "        \n",
        "        if channel is None:\n",
        "            channel = list(subject_data['channels'].keys())[0]\n",
        "        \n",
        "        if channel not in subject_data['channels']:\n",
        "            print(f\" Canal {channel} no encontrado\")\n",
        "            return\n",
        "        \n",
        "        epoch_data = subject_data['channels'][channel]['dwt_coeffs'][epoch_idx]\n",
        "        \n",
        "        # Visualizar\n",
        "        fig, axes = plt.subplots(8, 1, figsize=(15, 12))\n",
        "        fig.suptitle(f'DWT - Sujeto: {subject_id} - Canal: {channel} - √âpoca: {epoch_idx}', \n",
        "                     fontsize=16, fontweight='bold')\n",
        "        \n",
        "        coeff_names = ['A1', 'A2', 'A3', 'A4', 'D1', 'D2', 'D3', 'D4']\n",
        "        colors = ['steelblue']*4 + ['darkorange']*4\n",
        "        \n",
        "        for i, (coeff_name, color) in enumerate(zip(coeff_names, colors)):\n",
        "            axes[i].plot(epoch_data[coeff_name], linewidth=0.8, color=color)\n",
        "            axes[i].set_ylabel(coeff_name, fontweight='bold')\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "            axes[i].set_title(f'{coeff_name} ({len(epoch_data[coeff_name])} muestras)')\n",
        "        \n",
        "        axes[-1].set_xlabel('Muestras', fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCI√ìN PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal para procesar todos los archivos\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PROCESAMIENTO BATCH DWT - SLEEP-EDF DATABASE\")\n",
        "    print(\"   Extracci√≥n de 8 coeficientes (A1-A4, D1-D4)\")\n",
        "    print(\"   Canales: EOG + EEG\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Configurar rutas\n",
        "    data_path = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    \n",
        "    # Verificar que existe\n",
        "    if not Path(data_path).exists():\n",
        "        print(f\" ERROR: La ruta no existe: {data_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Crear procesador\n",
        "    processor = PSG_DWT_BatchProcessor(data_path)\n",
        "    \n",
        "    # Procesar todos los archivos\n",
        "    results = processor.process_all_files()\n",
        "    \n",
        "    if not results:\n",
        "        print(\" No se procesaron archivos\")\n",
        "        return None\n",
        "    \n",
        "    # Mostrar estad√≠sticas\n",
        "    processor.get_statistics()\n",
        "    \n",
        "    # Guardar resultados\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    processor.save_results(format='numpy')  # Guarda en pickle Y numpy\n",
        "    \n",
        "    # Visualizar ejemplo\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" VISUALIZACI√ìN DE MUESTRA\")\n",
        "    print(\"=\"*80)\n",
        "    processor.visualize_sample()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n Resultados guardados en: {processor.output_path}\")\n",
        "    \n",
        "    return processor, results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EJECUTAR\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, results = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Extracci√≥n de features (8 subbandas x 13 features = 104 features por √©poca y por canal (3) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extracci√≥n de Features - VERSI√ìN THREADING\n",
        "Soluci√≥n definitiva para Windows usando ThreadPoolExecutor\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy import signal, stats\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ===================== CONFIGURACI√ìN =====================\n",
        "\n",
        "ALLOWED_CHANNELS = ['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal']\n",
        "\n",
        "# ===================== FUNCIONES DE FEATURES =====================\n",
        "\n",
        "def compute_psd_cached(signal_data):\n",
        "    \"\"\"Calcula PSD.\"\"\"\n",
        "    try:\n",
        "        nperseg = min(256, len(signal_data))\n",
        "        freqs, psd = signal.welch(signal_data, nperseg=nperseg, noverlap=nperseg//2)\n",
        "        psd_norm = psd / (np.sum(psd) + 1e-10)\n",
        "        return psd_norm[psd_norm > 1e-10]\n",
        "    except:\n",
        "        return np.array([1e-10])\n",
        "\n",
        "def compute_all_entropies(signal_data):\n",
        "    \"\"\"Calcula todas las entrop√≠as.\"\"\"\n",
        "    try:\n",
        "        p = compute_psd_cached(signal_data)\n",
        "        p_squared = p ** 2\n",
        "        p_squared = p_squared[p_squared > 1e-10]\n",
        "        \n",
        "        shannon = -np.sum(p_squared * np.log2(p_squared + 1e-10))\n",
        "        log_energy = -np.sum(np.log2(p_squared + 1e-10))\n",
        "        norm = -np.sum(p ** 2)\n",
        "        \n",
        "        threshold = np.mean(p)\n",
        "        thresh_ent = np.sum(p > threshold)\n",
        "        \n",
        "        N = len(p)\n",
        "        count_below = np.sum(p <= threshold)\n",
        "        min_sum = np.sum(np.minimum(p ** 2, threshold ** 2))\n",
        "        sure = N - count_below + min_sum\n",
        "        \n",
        "        return {\n",
        "            'shannon_entropy': shannon,\n",
        "            'log_energy_entropy': log_energy,\n",
        "            'norm_entropy': norm,\n",
        "            'threshold_entropy': thresh_ent,\n",
        "            'sure_entropy': sure\n",
        "        }\n",
        "    except:\n",
        "        return {k: 0.0 for k in ['shannon_entropy', 'log_energy_entropy', \n",
        "                                  'norm_entropy', 'threshold_entropy', 'sure_entropy']}\n",
        "\n",
        "def compute_statistical_features(signal_data):\n",
        "    \"\"\"Calcula estad√≠sticas.\"\"\"\n",
        "    try:\n",
        "        return {\n",
        "            'variance': np.var(signal_data, ddof=0),\n",
        "            'skewness': stats.skew(signal_data, bias=False),\n",
        "            'kurtosis': stats.kurtosis(signal_data, bias=False)\n",
        "        }\n",
        "    except:\n",
        "        return {'variance': 0.0, 'skewness': 0.0, 'kurtosis': 0.0}\n",
        "\n",
        "def dispersion_entropy_fast(signal_data, m=3, c=6, d=1):\n",
        "    \"\"\"Dispersion Entropy.\"\"\"\n",
        "    try:\n",
        "        N = len(signal_data)\n",
        "        mu = np.mean(signal_data)\n",
        "        sigma = np.std(signal_data)\n",
        "        \n",
        "        if sigma < 1e-10:\n",
        "            return 0.0\n",
        "        \n",
        "        y = stats.norm.cdf(signal_data, loc=mu, scale=sigma)\n",
        "        z = np.clip(np.round(c * y + 0.5).astype(np.int32), 1, c)\n",
        "        \n",
        "        max_idx = N - (m - 1) * d\n",
        "        if max_idx < 1:\n",
        "            return 0.0\n",
        "        \n",
        "        patterns = {}\n",
        "        for i in range(max_idx):\n",
        "            pattern = tuple(z[i + j * d] for j in range(m))\n",
        "            patterns[pattern] = patterns.get(pattern, 0) + 1\n",
        "        \n",
        "        probabilities = np.array(list(patterns.values())) / max_idx\n",
        "        probabilities = probabilities[probabilities > 1e-10]\n",
        "        \n",
        "        return -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def rcmde_fast(signal_data, m=3, c=6, d=1, tau=4):\n",
        "    \"\"\"RCMDE.\"\"\"\n",
        "    try:\n",
        "        N = len(signal_data)\n",
        "        n_segments = N // tau\n",
        "        \n",
        "        if n_segments < m:\n",
        "            return dispersion_entropy_fast(signal_data, m, c, d)\n",
        "        \n",
        "        truncated = signal_data[:n_segments * tau]\n",
        "        reshaped = truncated.reshape(n_segments, tau)\n",
        "        \n",
        "        entropies = []\n",
        "        for k in range(min(tau, 4)):\n",
        "            coarse_grained = reshaped[:, k:].mean(axis=1)\n",
        "            \n",
        "            if len(coarse_grained) >= m:\n",
        "                de = dispersion_entropy_fast(coarse_grained, m, c, d)\n",
        "                entropies.append(de)\n",
        "        \n",
        "        return np.mean(entropies) if entropies else 0.0\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def ar_coefficients_fast(signal_data, order=4):\n",
        "    \"\"\"AR coefficients.\"\"\"\n",
        "    try:\n",
        "        centered = signal_data - np.mean(signal_data)\n",
        "        r = np.correlate(centered, centered, mode='full')\n",
        "        r = r[len(r)//2:order+1]\n",
        "        r = r / (r[0] + 1e-10)\n",
        "        \n",
        "        from scipy.linalg import toeplitz, solve\n",
        "        R = toeplitz(r[:order])\n",
        "        return solve(R, r[1:], assume_a='pos', check_finite=False)\n",
        "    except:\n",
        "        return np.zeros(order)\n",
        "\n",
        "def extract_features_from_subband(subband_data, subband_name, channel_name):\n",
        "    \"\"\"Extrae features de una subbanda.\"\"\"\n",
        "    features = {}\n",
        "    prefix = f\"{channel_name}_{subband_name}_\"\n",
        "    \n",
        "    try:\n",
        "        if len(subband_data) < 10:\n",
        "            raise ValueError(\"Insufficient data\")\n",
        "        \n",
        "        entropies = compute_all_entropies(subband_data)\n",
        "        for key, value in entropies.items():\n",
        "            features[prefix + key] = value\n",
        "        \n",
        "        stats_features = compute_statistical_features(subband_data)\n",
        "        for key, value in stats_features.items():\n",
        "            features[prefix + key] = value\n",
        "        \n",
        "        features[prefix + 'rcmde'] = rcmde_fast(subband_data)\n",
        "        \n",
        "        ar_coeffs = ar_coefficients_fast(subband_data, order=4)\n",
        "        for i, coeff in enumerate(ar_coeffs):\n",
        "            features[prefix + f'ar_coeff_{i+1}'] = coeff\n",
        "            \n",
        "    except:\n",
        "        for feat in ['shannon_entropy', 'log_energy_entropy', 'norm_entropy',\n",
        "                     'threshold_entropy', 'sure_entropy', 'variance', \n",
        "                     'skewness', 'kurtosis', 'rcmde']:\n",
        "            features[prefix + feat] = 0.0\n",
        "        for i in range(4):\n",
        "            features[prefix + f'ar_coeff_{i+1}'] = 0.0\n",
        "    \n",
        "    return features\n",
        "\n",
        "def extract_features_from_epoch(dwt_coeffs_epoch, channel_name):\n",
        "    \"\"\"Extrae features de una √©poca.\"\"\"\n",
        "    all_features = {}\n",
        "    \n",
        "    for subband_name in ['D1', 'D2', 'D3', 'D4', 'A1', 'A2', 'A3', 'A4']:\n",
        "        subband_data = dwt_coeffs_epoch[subband_name]\n",
        "        features = extract_features_from_subband(subband_data, subband_name, channel_name)\n",
        "        all_features.update(features)\n",
        "    \n",
        "    return all_features\n",
        "\n",
        "# ===================== PROCESAMIENTO CON THREADING =====================\n",
        "\n",
        "class FileProcessor:\n",
        "    \"\"\"Procesa archivos usando threading (funciona en Windows).\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def process_file(self, npz_path, subject_id):\n",
        "        \"\"\"Procesa un archivo .npz.\"\"\"\n",
        "        try:\n",
        "            channel_name = npz_path.stem.replace(\"_dwt\", \"\")\n",
        "            \n",
        "            if channel_name not in ALLOWED_CHANNELS:\n",
        "                return []\n",
        "            \n",
        "            with np.load(npz_path) as data:\n",
        "                sfreq = float(data['sfreq'])\n",
        "                n_epochs = int(data['n_epochs'])\n",
        "                ch_type = 'eog' if 'EOG' in channel_name else 'eeg'\n",
        "                \n",
        "                channel_features = []\n",
        "                max_epochs = min(n_epochs, 1000)\n",
        "                \n",
        "                for epoch_idx in range(max_epochs):\n",
        "                    try:\n",
        "                        dwt_coeffs = {\n",
        "                            'D1': data['D1'][epoch_idx],\n",
        "                            'D2': data['D2'][epoch_idx],\n",
        "                            'D3': data['D3'][epoch_idx],\n",
        "                            'D4': data['D4'][epoch_idx],\n",
        "                            'A1': data['A1'][epoch_idx],\n",
        "                            'A2': data['A2'][epoch_idx],\n",
        "                            'A3': data['A3'][epoch_idx],\n",
        "                            'A4': data['A4'][epoch_idx]\n",
        "                        }\n",
        "                        \n",
        "                        features = extract_features_from_epoch(dwt_coeffs, channel_name)\n",
        "                        \n",
        "                        features['subject_id'] = subject_id\n",
        "                        features['channel_name'] = channel_name\n",
        "                        features['channel_type'] = ch_type\n",
        "                        features['epoch_num'] = epoch_idx\n",
        "                        features['sfreq'] = sfreq\n",
        "                        \n",
        "                        channel_features.append(features)\n",
        "                        \n",
        "                    except:\n",
        "                        continue\n",
        "            \n",
        "            return channel_features\n",
        "            \n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "def extract_features_threading(numpy_base_dir, output_dir, n_workers=12, save_every=10):\n",
        "    \"\"\"\n",
        "    Extracci√≥n usando THREADING en lugar de multiprocessing.\n",
        "    Funciona perfectamente en Windows con datos grandes.\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"EXTRACCI√ìN CON THREADING - 100% COMPATIBLE WINDOWS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    numpy_base_dir = Path(numpy_base_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    subject_dirs = sorted([d for d in numpy_base_dir.iterdir() if d.is_dir()])\n",
        "    n_subjects = len(subject_dirs)\n",
        "    \n",
        "    print(f\"\\n Configuraci√≥n:\")\n",
        "    print(f\"   - Sujetos: {n_subjects}\")\n",
        "    print(f\"   - Workers (threads): {n_workers}\")\n",
        "    print(f\"   - Checkpoint cada: {save_every} sujetos\")\n",
        "    print(f\"   - Canales: {', '.join(ALLOWED_CHANNELS)}\")\n",
        "    \n",
        "    processor = FileProcessor()\n",
        "    all_features = []\n",
        "    processed_subjects = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    print(f\"\\n Iniciando...\\n\")\n",
        "    \n",
        "    # Usar tqdm para barra de progreso\n",
        "    for subject_dir in tqdm(subject_dirs, desc=\"Extrayendo\", unit=\"sujeto\"):\n",
        "        subject_id = subject_dir.name\n",
        "        \n",
        "        npz_files = list(subject_dir.glob(\"*.npz\"))\n",
        "        npz_files = [f for f in npz_files \n",
        "                     if f.stem.replace(\"_dwt\", \"\") in ALLOWED_CHANNELS]\n",
        "        \n",
        "        if not npz_files:\n",
        "            continue\n",
        "        \n",
        "        subject_features = []\n",
        "        \n",
        "        # Threading en lugar de multiprocessing\n",
        "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
        "            futures = {\n",
        "                executor.submit(processor.process_file, npz_file, subject_id): npz_file \n",
        "                for npz_file in npz_files\n",
        "            }\n",
        "            \n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    result = future.result(timeout=180)\n",
        "                    subject_features.extend(result)\n",
        "                except Exception as e:\n",
        "                    failed_count += 1\n",
        "                    continue\n",
        "        \n",
        "        if subject_features:\n",
        "            all_features.extend(subject_features)\n",
        "        \n",
        "        processed_subjects += 1\n",
        "        \n",
        "        # Checkpoint\n",
        "        if processed_subjects % save_every == 0 and all_features:\n",
        "            df_temp = pd.DataFrame(all_features)\n",
        "            checkpoint_path = output_dir / f\"checkpoint_{processed_subjects}.pkl\"\n",
        "            df_temp.to_pickle(checkpoint_path)\n",
        "            tqdm.write(f\"    Checkpoint: {len(df_temp)} √©pocas guardadas\")\n",
        "        \n",
        "        if processed_subjects % 5 == 0:\n",
        "            gc.collect()\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"COMPLETADO\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    if failed_count > 0:\n",
        "        print(f\"\\n {failed_count} archivos fallaron (se omitieron)\")\n",
        "    \n",
        "    if not all_features:\n",
        "        print(\"\\n No se extrajeron features\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    df_features = pd.DataFrame(all_features)\n",
        "    \n",
        "    metadata_cols = ['subject_id', 'channel_name', 'channel_type', 'epoch_num', 'sfreq']\n",
        "    feature_cols = [col for col in df_features.columns if col not in metadata_cols]\n",
        "    df_features = df_features[metadata_cols + feature_cols]\n",
        "    \n",
        "    print(f\"\\n Extracci√≥n exitosa:\")\n",
        "    print(f\"  - √âpocas: {len(df_features):,}\")\n",
        "    print(f\"  - Features: {len(feature_cols)}\")\n",
        "    print(f\"  - Sujetos: {df_features['subject_id'].nunique()}\")\n",
        "    print(f\"  - Canales: {df_features['channel_name'].nunique()}\")\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "# ===================== AUXILIARES =====================\n",
        "\n",
        "def find_latest_numpy_dir(base_path):\n",
        "    \"\"\"Encuentra carpeta m√°s reciente.\"\"\"\n",
        "    base_path = Path(base_path)\n",
        "    numpy_dirs = list(base_path.glob(\"dwt_numpy_*\"))\n",
        "    \n",
        "    if not numpy_dirs:\n",
        "        raise FileNotFoundError(f\" No hay carpetas 'dwt_numpy_*' en: {base_path}\")\n",
        "    \n",
        "    numpy_dirs = sorted(numpy_dirs, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    print(f\"\\n {numpy_dirs[0].name}\")\n",
        "    return numpy_dirs[0]\n",
        "\n",
        "def add_sleep_stages(df_features, hypnogram_dir):\n",
        "    \"\"\"Agrega etapas de sue√±o.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"AGREGANDO ETAPAS DE SUE√ëO\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    import mne\n",
        "    hypnogram_dir = Path(hypnogram_dir)\n",
        "    \n",
        "    stage_map = {\n",
        "        'Sleep stage W': 0, 'Sleep stage 1': 1, 'Sleep stage 2': 2,\n",
        "        'Sleep stage 3': 3, 'Sleep stage 4': 3, 'Sleep stage R': 4,\n",
        "        'Sleep stage ?': -1, 'Movement time': -1\n",
        "    }\n",
        "    \n",
        "    df_features['sleep_stage'] = -1\n",
        "    \n",
        "    for subject_id in tqdm(df_features['subject_id'].unique(), desc=\"Hypnogramas\"):\n",
        "        hyp_files = list(hypnogram_dir.glob(f\"{subject_id}*-Hypnogram.edf\"))\n",
        "        \n",
        "        if not hyp_files:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            annotations = mne.read_annotations(hyp_files[0])\n",
        "            stages = [stage_map[desc] for desc in annotations.description \n",
        "                     if desc in stage_map and stage_map[desc] != -1]\n",
        "            \n",
        "            subject_mask = df_features['subject_id'] == subject_id\n",
        "            subject_epochs = df_features[subject_mask].groupby('epoch_num').first().index\n",
        "            \n",
        "            for epoch_num in subject_epochs:\n",
        "                if epoch_num < len(stages):\n",
        "                    epoch_mask = (df_features['subject_id'] == subject_id) & \\\n",
        "                                (df_features['epoch_num'] == epoch_num)\n",
        "                    df_features.loc[epoch_mask, 'sleep_stage'] = stages[epoch_num]\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    n_before = len(df_features)\n",
        "    df_features = df_features[df_features['sleep_stage'] != -1].copy()\n",
        "    n_after = len(df_features)\n",
        "    \n",
        "    print(f\"\\n √âpocas etiquetadas: {n_after}/{n_before}\")\n",
        "    \n",
        "    stage_names = {0: 'Wake', 1: 'S1', 2: 'S2', 3: 'S3/S4', 4: 'REM'}\n",
        "    stage_counts = df_features['sleep_stage'].value_counts().sort_index()\n",
        "    \n",
        "    total = len(df_features)\n",
        "    print(\"\\nDistribuci√≥n:\")\n",
        "    for stage, count in stage_counts.items():\n",
        "        percentage = (count / total) * 100\n",
        "        bar = '‚ñà' * int(percentage / 2)\n",
        "        print(f\"{stage_names.get(stage, f'Stage {stage}'):6} | {bar} {count:6d} ({percentage:5.1f}%)\")\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "def save_features(df_features, output_dir):\n",
        "    \"\"\"Guarda features en m√∫ltiples formatos optimizados.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"GUARDANDO FEATURES\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    output_dir = Path(output_dir)\n",
        "    \n",
        "    # 1. PARQUET (recomendado) - Compresi√≥n, r√°pido, universal\n",
        "    parquet_path = output_dir / \"features_complete.parquet\"\n",
        "    df_features.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
        "    size_mb = parquet_path.stat().st_size / (1024**2)\n",
        "    print(f\" Parquet: {parquet_path.name} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    # 2. PICKLE (backup r√°pido) - Solo Python pero ultra r√°pido\n",
        "    pkl_path = output_dir / \"features_complete.pkl\"\n",
        "    df_features.to_pickle(pkl_path, compression='gzip')\n",
        "    size_mb = pkl_path.stat().st_size / (1024**2)\n",
        "    print(f\" Pickle (comprimido): {pkl_path.name} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    # 3. CSV (opcional) - Solo si necesitas compartir con otros programas\n",
        "    # Comentado por defecto para ahorrar espacio\n",
        "    # csv_path = output_dir / \"features_complete.csv\"\n",
        "    # df_features.to_csv(csv_path, index=False)\n",
        "    # print(f\"‚úì CSV: {csv_path.name}\")\n",
        "    \n",
        "    # 4. Resumen en texto\n",
        "    info_path = output_dir / \"features_info.txt\"\n",
        "    with open(info_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"DATASET DE FEATURES - RESUMEN\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\\n\")\n",
        "        f.write(f\"√âpocas totales: {len(df_features):,}\\n\")\n",
        "        f.write(f\"Sujetos √∫nicos: {df_features['subject_id'].nunique()}\\n\")\n",
        "        f.write(f\"Canales: {', '.join(df_features['channel_name'].unique())}\\n\")\n",
        "        \n",
        "        metadata_cols = ['subject_id', 'channel_name', 'channel_type', 'epoch_num', 'sfreq', 'sleep_stage']\n",
        "        feature_cols = [c for c in df_features.columns if c not in metadata_cols]\n",
        "        f.write(f\"Features por √©poca: {len(feature_cols)}\\n\\n\")\n",
        "        \n",
        "        f.write(\"DISTRIBUCI√ìN DE ETAPAS DE SUE√ëO:\\n\")\n",
        "        if 'sleep_stage' in df_features.columns:\n",
        "            stage_names = {0: 'Wake', 1: 'Stage 1', 2: 'Stage 2', 3: 'Stage 3/4', 4: 'REM'}\n",
        "            stage_counts = df_features['sleep_stage'].value_counts().sort_index()\n",
        "            total = len(df_features)\n",
        "            for stage, count in stage_counts.items():\n",
        "                if stage >= 0:\n",
        "                    percentage = (count / total) * 100\n",
        "                    f.write(f\"  {stage_names.get(stage, f'Stage {stage}'):12} : {count:7,} ({percentage:5.1f}%)\\n\")\n",
        "        \n",
        "        f.write(f\"\\nPRIMEROS 5 FEATURE NAMES:\\n\")\n",
        "        for feat in feature_cols[:5]:\n",
        "            f.write(f\"  - {feat}\\n\")\n",
        "        f.write(f\"  ... ({len(feature_cols)-5} m√°s)\\n\")\n",
        "    \n",
        "    print(f\"‚úì Info: {info_path.name}\")\n",
        "    \n",
        "    print(f\"\\n Para cargar:\")\n",
        "    print(f\"   df = pd.read_parquet('{parquet_path.name}')  # Recomendado\")\n",
        "    print(f\"   df = pd.read_pickle('{pkl_path.name}')       # Alternativa r√°pida\")\n",
        "\n",
        "# ===================== MAIN =====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DWT_RESULTS_DIR = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\\dwt_results\"\n",
        "    HYPNOGRAM_DIR = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    OUTPUT_DIR = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\features_batch\"\n",
        "    \n",
        "    # Con threading puedes usar m√°s workers (12-24)\n",
        "    N_WORKERS =22\n",
        "    SAVE_EVERY = 10\n",
        "    \n",
        "    print(\"\\n EXTRACCI√ìN CON THREADING\")\n",
        "    print(f\"   Workers: {N_WORKERS}\")\n",
        "    \n",
        "    try:\n",
        "        NUMPY_BASE_DIR = find_latest_numpy_dir(DWT_RESULTS_DIR)\n",
        "    except FileNotFoundError as e:\n",
        "        print(str(e))\n",
        "        exit(1)\n",
        "    \n",
        "    subject_dirs = [d for d in NUMPY_BASE_DIR.iterdir() if d.is_dir()]\n",
        "    if not subject_dirs:\n",
        "        print(\" Carpeta vac√≠a\")\n",
        "        exit(1)\n",
        "    \n",
        "    print(f\"‚úì {len(subject_dirs)} sujetos\\n\")\n",
        "    \n",
        "    # Extracci√≥n con threading\n",
        "    df_features = extract_features_threading(\n",
        "        NUMPY_BASE_DIR, \n",
        "        OUTPUT_DIR,\n",
        "        n_workers=N_WORKERS,\n",
        "        save_every=SAVE_EVERY\n",
        "    )\n",
        "    \n",
        "    if df_features.empty:\n",
        "        print(\"\\n No se generaron features\")\n",
        "        exit(1)\n",
        "    \n",
        "    # Agregar etapas\n",
        "    df_features = add_sleep_stages(df_features, HYPNOGRAM_DIR)\n",
        "    \n",
        "    # Guardar\n",
        "    save_features(df_features, OUTPUT_DIR)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\" COMPLETADO\")\n",
        "    print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fix del script anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Verificador y Reparador de Sleep Stages\n",
        "Para el archivo features_complete_labeled.parquet\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def verify_labels(parquet_file):\n",
        "    \"\"\"\n",
        "    Verifica el estado de las etiquetas en el archivo parquet\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" VERIFICANDO ETIQUETAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\n Cargando: {Path(parquet_file).name}\")\n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    \n",
        "    print(f\"    Shape: {df.shape}\")\n",
        "    print(f\"    Tama√±o: {Path(parquet_file).stat().st_size / (1024*1024):.1f} MB\")\n",
        "    \n",
        "    # Verificar columnas\n",
        "    print(f\"\\n Columnas disponibles:\")\n",
        "    for col in df.columns:\n",
        "        print(f\"   - {col}\")\n",
        "    \n",
        "    # Verificar si existe columna de sleep stage\n",
        "    sleep_cols = [col for col in df.columns if 'sleep' in col.lower() or 'stage' in col.lower()]\n",
        "    \n",
        "    if not sleep_cols:\n",
        "        print(f\"\\n No se encontr√≥ columna de sleep stages\")\n",
        "        return df, False\n",
        "    \n",
        "    print(f\"\\n Columnas de sleep encontradas: {sleep_cols}\")\n",
        "    \n",
        "    # Analizar cada columna\n",
        "    for col in sleep_cols:\n",
        "        print(f\"\\n Analizando columna: {col}\")\n",
        "        print(f\"   Tipo: {df[col].dtype}\")\n",
        "        print(f\"   Valores √∫nicos: {df[col].nunique()}\")\n",
        "        print(f\"   Valores no-nulos: {df[col].notna().sum():,} / {len(df):,}\")\n",
        "        print(f\"   Porcentaje etiquetado: {(df[col].notna().sum() / len(df) * 100):.1f}%\")\n",
        "        \n",
        "        if df[col].notna().sum() > 0:\n",
        "            print(f\"\\n   Distribuci√≥n de valores:\")\n",
        "            value_counts = df[col].value_counts()\n",
        "            for val, count in value_counts.items():\n",
        "                print(f\"      {str(val):15s}: {count:8,}\")\n",
        "    \n",
        "    # Verificar metadata\n",
        "    print(f\"\\n Metadata del DataFrame:\")\n",
        "    print(f\"   Sujetos √∫nicos: {df['subject_id'].nunique() if 'subject_id' in df.columns else 'N/A'}\")\n",
        "    print(f\"   Canales √∫nicos: {df['channel'].nunique() if 'channel' in df.columns else 'N/A'}\")\n",
        "    \n",
        "    # Verificar muestra\n",
        "    print(f\"\\n Primeras 5 filas (columnas principales):\")\n",
        "    cols_to_show = ['subject_id', 'channel', 'epoch_idx'] + sleep_cols\n",
        "    cols_to_show = [c for c in cols_to_show if c in df.columns]\n",
        "    print(df[cols_to_show].head())\n",
        "    \n",
        "    # Determinar si necesita reparaci√≥n\n",
        "    needs_fix = False\n",
        "    if sleep_cols:\n",
        "        main_col = sleep_cols[0]\n",
        "        labeled_pct = (df[main_col].notna().sum() / len(df)) * 100\n",
        "        if labeled_pct < 50:\n",
        "            needs_fix = True\n",
        "            print(f\"\\n  NECESITA REPARACI√ìN: Solo {labeled_pct:.1f}% etiquetado\")\n",
        "        else:\n",
        "            print(f\"\\n BIEN ETIQUETADO: {labeled_pct:.1f}% completo\")\n",
        "    \n",
        "    return df, needs_fix\n",
        "\n",
        "\n",
        "def fix_labels_fast(df, data_path):\n",
        "    \"\"\"\n",
        "    Repara las etiquetas de manera eficiente\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"REPARANDO ETIQUETAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    data_path = Path(data_path)\n",
        "    \n",
        "    # Identificar columna de sleep stage\n",
        "    sleep_col = None\n",
        "    for col in df.columns:\n",
        "        if 'sleep' in col.lower() and 'stage' in col.lower():\n",
        "            sleep_col = col\n",
        "            break\n",
        "    \n",
        "    if sleep_col is None:\n",
        "        print(\"     Creando nueva columna 'sleep_stage'\")\n",
        "        sleep_col = 'sleep_stage'\n",
        "        df[sleep_col] = None\n",
        "    \n",
        "    print(f\"    Usando columna: {sleep_col}\")\n",
        "    \n",
        "    # Cargar hypnogramas\n",
        "    unique_subjects = df['subject_id'].unique()\n",
        "    print(f\"\\n Procesando {len(unique_subjects)} sujetos...\")\n",
        "    \n",
        "    hypnogram_cache = {}\n",
        "    \n",
        "    for subject_id in tqdm(unique_subjects, desc=\"Cargando hypnogramas\"):\n",
        "        # Buscar hypnogram\n",
        "        hypno_pattern = f\"{subject_id}*Hypnogram*.edf\"\n",
        "        hypno_files = list(data_path.glob(hypno_pattern))\n",
        "        \n",
        "        if not hypno_files:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            annotations = mne.read_annotations(hypno_files[0], verbose=False)\n",
        "            \n",
        "            sleep_stages = []\n",
        "            for desc in annotations.description:\n",
        "                if 'Sleep stage' in desc:\n",
        "                    stage = desc.replace('Sleep stage ', '').strip()\n",
        "                    \n",
        "                    stage_map = {\n",
        "                        'W': 'W',\n",
        "                        '?': 'Unknown',\n",
        "                        '1': 'S1',\n",
        "                        '2': 'S2',\n",
        "                        '3': 'S3',\n",
        "                        '4': 'S4',\n",
        "                        'R': 'REM'\n",
        "                    }\n",
        "                    \n",
        "                    mapped_stage = stage_map.get(stage, stage)\n",
        "                    sleep_stages.append(mapped_stage)\n",
        "            \n",
        "            hypnogram_cache[subject_id] = sleep_stages\n",
        "            \n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"    Hypnogramas cargados: {len(hypnogram_cache)}\")\n",
        "    \n",
        "    # Asignar etiquetas\n",
        "    print(f\"\\n  Asignando etiquetas...\")\n",
        "    \n",
        "    assigned = 0\n",
        "    for subject_id, sleep_stages in tqdm(hypnogram_cache.items(), desc=\"Asignando\"):\n",
        "        # M√°scara para este sujeto\n",
        "        mask = df['subject_id'] == subject_id\n",
        "        \n",
        "        # Asignar seg√∫n epoch_idx\n",
        "        for idx in df[mask].index:\n",
        "            epoch_idx = df.loc[idx, 'epoch_idx']\n",
        "            \n",
        "            if epoch_idx < len(sleep_stages):\n",
        "                df.loc[idx, sleep_col] = sleep_stages[epoch_idx]\n",
        "                assigned += 1\n",
        "    \n",
        "    print(f\"\\n Etiquetas asignadas: {assigned:,} / {len(df):,}\")\n",
        "    \n",
        "    # Mostrar distribuci√≥n\n",
        "    if assigned > 0:\n",
        "        print(f\"\\n Distribuci√≥n de sleep stages:\")\n",
        "        stage_counts = df[sleep_col].value_counts()\n",
        "        for stage, count in stage_counts.items():\n",
        "            if stage and pd.notna(stage):\n",
        "                pct = (count / assigned) * 100\n",
        "                print(f\"   {str(stage):10s}: {count:8,} ({pct:5.1f}%)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def save_fixed_file(df, original_file):\n",
        "    \"\"\"Guarda el archivo reparado\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GUARDANDO ARCHIVO REPARADO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    output_file = Path(original_file).parent / \"features_complete_labeled_FIXED.parquet\"\n",
        "    \n",
        "    print(f\"\\n Guardando en: {output_file.name}\")\n",
        "    df.to_parquet(output_file, index=False)\n",
        "    \n",
        "    size_mb = output_file.stat().st_size / (1024*1024)\n",
        "    print(f\"    Tama√±o: {size_mb:.1f} MB\")\n",
        "    \n",
        "    # Info\n",
        "    info_file = output_file.with_suffix('.txt')\n",
        "    with open(info_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"ARCHIVO REPARADO\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Shape: {df.shape}\\n\")\n",
        "        f.write(f\"Tama√±o: {size_mb:.1f} MB\\n\\n\")\n",
        "        \n",
        "        f.write(\"Sleep Stages:\\n\")\n",
        "        sleep_cols = [col for col in df.columns if 'sleep' in col.lower()]\n",
        "        if sleep_cols:\n",
        "            stage_counts = df[sleep_cols[0]].value_counts()\n",
        "            for stage, count in stage_counts.items():\n",
        "                f.write(f\"  {str(stage):15s}: {count:8,}\\n\")\n",
        "        \n",
        "        f.write(f\"\\nSujetos: {df['subject_id'].nunique()}\\n\")\n",
        "        f.write(f\"Canales: {df['channel'].nunique()}\\n\")\n",
        "        f.write(f\"√âpocas: {len(df):,}\\n\")\n",
        "    \n",
        "    print(f\"    Info guardada: {info_file.name}\")\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VERIFICADOR Y REPARADOR DE SLEEP STAGES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # RUTAS\n",
        "    parquet_file = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\features_batch\\features_complete_labeled.parquet\"\n",
        "    data_path = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    \n",
        "    # Verificar\n",
        "    if not Path(parquet_file).exists():\n",
        "        print(f\" No se encuentra: {parquet_file}\")\n",
        "        return\n",
        "    \n",
        "    if not Path(data_path).exists():\n",
        "        print(f\" No se encuentra: {data_path}\")\n",
        "        return\n",
        "    \n",
        "    # 1. Verificar estado actual\n",
        "    df, needs_fix = verify_labels(parquet_file)\n",
        "    \n",
        "    # 2. Decidir si reparar\n",
        "    if needs_fix:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"  El archivo necesita reparaci√≥n\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        try:\n",
        "            response = input(\"\\n¬øProceder con la reparaci√≥n? (s/n): \").strip().lower()\n",
        "            if response != 's':\n",
        "                print(\"‚ùå Reparaci√≥n cancelada\")\n",
        "                return df\n",
        "        except:\n",
        "            print(\"\\n‚úì Continuando autom√°ticamente...\")\n",
        "        \n",
        "        # Reparar\n",
        "        df_fixed = fix_labels_fast(df, data_path)\n",
        "        \n",
        "        # Guardar\n",
        "        output_file = save_fixed_file(df_fixed, parquet_file)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" REPARACI√ìN COMPLETADA\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\n Usa el nuevo archivo:\")\n",
        "        print(f\"   df = pd.read_parquet('{output_file.name}')\")\n",
        "        \n",
        "        return df_fixed\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" El archivo est√° correctamente etiquetado\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\n Puedes usarlo directamente:\")\n",
        "        print(f\"   df = pd.read_parquet('{Path(parquet_file).name}')\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fix r√°pido para asignar sleep stages al archivo features_complete_labeled.parquet\n",
        "Las etiquetas est√°n en -1, necesitamos leer los hypnogramas\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fix_sleep_stages_fast(features_file, data_path):\n",
        "    \"\"\"\n",
        "    Repara las sleep stages leyendo hypnogramas\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" REPARANDO SLEEP STAGES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Cargar features\n",
        "    print(f\"\\n Cargando: {Path(features_file).name}\")\n",
        "    df = pd.read_parquet(features_file)\n",
        "    print(f\"    Shape: {df.shape}\")\n",
        "    print(f\"    Sleep stages actuales: {df['sleep_stage'].unique()}\")\n",
        "    \n",
        "    # Cargar hypnogramas\n",
        "    data_path = Path(data_path)\n",
        "    print(f\"\\n Leyendo hypnogramas desde: {data_path}\")\n",
        "    \n",
        "    hypnogram_cache = {}\n",
        "    unique_subjects = df['subject_id'].unique()\n",
        "    \n",
        "    print(f\"   Sujetos a procesar: {len(unique_subjects)}\")\n",
        "    \n",
        "    for subject_id in tqdm(unique_subjects, desc=\"Cargando hypnogramas\"):\n",
        "        # Buscar hypnogram\n",
        "        hypno_pattern = f\"{subject_id}*Hypnogram*.edf\"\n",
        "        hypno_files = list(data_path.glob(hypno_pattern))\n",
        "        \n",
        "        if not hypno_files:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            annotations = mne.read_annotations(hypno_files[0], verbose=False)\n",
        "            \n",
        "            sleep_stages = []\n",
        "            for desc in annotations.description:\n",
        "                if 'Sleep stage' in desc:\n",
        "                    stage = desc.replace('Sleep stage ', '').strip()\n",
        "                    \n",
        "                    # Mapeo\n",
        "                    stage_map = {\n",
        "                        'W': 'W',\n",
        "                        '?': 'Unknown',\n",
        "                        '1': 'S1',\n",
        "                        '2': 'S2',\n",
        "                        '3': 'S3',\n",
        "                        '4': 'S4',\n",
        "                        'R': 'REM'\n",
        "                    }\n",
        "                    \n",
        "                    mapped_stage = stage_map.get(stage, stage)\n",
        "                    sleep_stages.append(mapped_stage)\n",
        "            \n",
        "            hypnogram_cache[subject_id] = sleep_stages\n",
        "            \n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n   ‚úì Hypnogramas cargados: {len(hypnogram_cache)}\")\n",
        "    \n",
        "    # Asignar etiquetas\n",
        "    print(f\"\\n Asignando sleep stages...\")\n",
        "    \n",
        "    assigned = 0\n",
        "    df['sleep_stage_fixed'] = None\n",
        "    \n",
        "    for subject_id in tqdm(hypnogram_cache.keys(), desc=\"Asignando\"):\n",
        "        sleep_stages = hypnogram_cache[subject_id]\n",
        "        \n",
        "        # M√°scara de sujeto\n",
        "        mask = df['subject_id'] == subject_id\n",
        "        subject_indices = df[mask].index\n",
        "        \n",
        "        # Asignar seg√∫n epoch_idx\n",
        "        for idx in subject_indices:\n",
        "            epoch_idx = df.loc[idx, 'epoch_idx']\n",
        "            \n",
        "            if epoch_idx < len(sleep_stages):\n",
        "                df.loc[idx, 'sleep_stage_fixed'] = sleep_stages[epoch_idx]\n",
        "                assigned += 1\n",
        "    \n",
        "    print(f\"\\n    √âpocas etiquetadas: {assigned:,} / {len(df):,}\")\n",
        "    \n",
        "    # Usar la columna fija\n",
        "    df['sleep_stage'] = df['sleep_stage_fixed']\n",
        "    df = df.drop(columns=['sleep_stage_fixed'])\n",
        "    \n",
        "    # Filtrar solo con etiquetas v√°lidas\n",
        "    valid_stages = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
        "    df = df[df['sleep_stage'].isin(valid_stages)]\n",
        "    \n",
        "    print(f\"    Shape final: {df.shape}\")\n",
        "    \n",
        "    # Mostrar distribuci√≥n\n",
        "    print(f\"\\n Distribuci√≥n de Sleep Stages:\")\n",
        "    stage_counts = df['sleep_stage'].value_counts()\n",
        "    for stage, count in stage_counts.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"      {stage:5s}: {count:7,} ({pct:5.1f}%)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def save_fixed_file(df, original_file):\n",
        "    \"\"\"Guarda archivo reparado\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GUARDANDO ARCHIVO REPARADO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    output_file = Path(original_file).parent / \"features_FIXED.parquet\"\n",
        "    \n",
        "    print(f\"\\n Guardando: {output_file.name}\")\n",
        "    df.to_parquet(output_file, index=False)\n",
        "    \n",
        "    size_mb = output_file.stat().st_size / (1024*1024)\n",
        "    print(f\"    Tama√±o: {size_mb:.1f} MB\")\n",
        "    print(f\"    Shape: {df.shape}\")\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" FIX R√ÅPIDO - Sleep Stages en -1\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Rutas\n",
        "    features_file = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\features_batch\\features_complete_labeled.parquet\"\n",
        "    data_path = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    \n",
        "    # Verificar\n",
        "    if not Path(features_file).exists():\n",
        "        print(f\" No se encuentra: {features_file}\")\n",
        "        return\n",
        "    \n",
        "    if not Path(data_path).exists():\n",
        "        print(f\" No se encuentra: {data_path}\")\n",
        "        return\n",
        "    \n",
        "    # Reparar\n",
        "    df_fixed = fix_sleep_stages_fast(features_file, data_path)\n",
        "    \n",
        "    # Guardar\n",
        "    output_file = save_fixed_file(df_fixed, features_file)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" REPARACI√ìN COMPLETADA\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n Usa el nuevo archivo:\")\n",
        "    print(f\"   {output_file.name}\")\n",
        "    print(f\"\\n   df = pd.read_parquet('{output_file.name}')\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Diagn√≥stico completo y fix de sleep stages\n",
        "Detecta autom√°ticamente el patr√≥n de nombres\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def diagnose_files(features_file, data_path):\n",
        "    \"\"\"Diagnostica los archivos para entender el problema\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" DIAGN√ìSTICO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 1. Ver subject_ids en el DataFrame\n",
        "    df = pd.read_parquet(features_file)\n",
        "    print(f\"\\nüìä DataFrame:\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Subject IDs √∫nicos: {df['subject_id'].nunique()}\")\n",
        "    print(f\"\\n   Primeros 10 subject_ids:\")\n",
        "    for sid in df['subject_id'].unique()[:10]:\n",
        "        print(f\"      '{sid}'\")\n",
        "    \n",
        "    # 2. Ver archivos hypnogram disponibles\n",
        "    data_path = Path(data_path)\n",
        "    hypno_files = list(data_path.glob(\"*Hypnogram*.edf\"))\n",
        "    \n",
        "    print(f\"\\n Archivos Hypnogram encontrados: {len(hypno_files)}\")\n",
        "    print(f\"\\n   Primeros 10 archivos:\")\n",
        "    for f in hypno_files[:10]:\n",
        "        print(f\"      {f.name}\")\n",
        "    \n",
        "    # 3. Intentar extraer patr√≥n\n",
        "    if hypno_files:\n",
        "        sample_name = hypno_files[0].name\n",
        "        print(f\"\\n Ejemplo de nombre de archivo:\")\n",
        "        print(f\"   {sample_name}\")\n",
        "        \n",
        "        # Extraer posible subject_id\n",
        "        # Patrones comunes: SC4001E0, ST7011J0, etc.\n",
        "        patterns = [\n",
        "            r'(SC\\d+[A-Z]\\d+)',  # SC4001E0\n",
        "            r'(ST\\d+[A-Z]\\d+)',  # ST7011J0\n",
        "            r'(\\d+)',             # Solo n√∫meros\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, sample_name)\n",
        "            if match:\n",
        "                print(f\"   Posible ID: '{match.group(1)}'\")\n",
        "    \n",
        "    return df, hypno_files\n",
        "\n",
        "\n",
        "def create_subject_mapping(df_subject_ids, hypno_files):\n",
        "    \"\"\"\n",
        "    Crea mapeo entre subject_ids del DataFrame y archivos hypnogram\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CREANDO MAPEO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    mapping = {}\n",
        "    \n",
        "    for subject_id in tqdm(df_subject_ids, desc=\"Mapeando sujetos\"):\n",
        "        # Buscar archivo que contenga este subject_id\n",
        "        found = False\n",
        "        \n",
        "        for hypno_file in hypno_files:\n",
        "            # Probar coincidencia exacta\n",
        "            if subject_id in hypno_file.name:\n",
        "                mapping[subject_id] = hypno_file\n",
        "                found = True\n",
        "                break\n",
        "            \n",
        "            # Probar sin guiones/espacios\n",
        "            clean_id = subject_id.replace('-', '').replace('_', '').replace(' ', '')\n",
        "            clean_name = hypno_file.name.replace('-', '').replace('_', '').replace(' ', '')\n",
        "            if clean_id in clean_name:\n",
        "                mapping[subject_id] = hypno_file\n",
        "                found = True\n",
        "                break\n",
        "        \n",
        "        if not found and len(mapping) < 5:\n",
        "            print(f\"     No se encontr√≥ hypnogram para: '{subject_id}'\")\n",
        "    \n",
        "    print(f\"\\n   ‚úì Mapeos exitosos: {len(mapping)} / {len(df_subject_ids)}\")\n",
        "    \n",
        "    if len(mapping) > 0:\n",
        "        print(f\"\\n   Ejemplos de mapeo:\")\n",
        "        for i, (sid, hfile) in enumerate(list(mapping.items())[:3]):\n",
        "            print(f\"      '{sid}' ‚Üí {hfile.name}\")\n",
        "    \n",
        "    return mapping\n",
        "\n",
        "\n",
        "def load_hypnogram(hypno_file):\n",
        "    \"\"\"Carga un archivo hypnogram\"\"\"\n",
        "    try:\n",
        "        annotations = mne.read_annotations(hypno_file, verbose=False)\n",
        "        \n",
        "        sleep_stages = []\n",
        "        for desc in annotations.description:\n",
        "            if 'Sleep stage' in desc:\n",
        "                stage = desc.replace('Sleep stage ', '').strip()\n",
        "                \n",
        "                stage_map = {\n",
        "                    'W': 'W',\n",
        "                    '?': 'Unknown',\n",
        "                    '1': 'S1',\n",
        "                    '2': 'S2',\n",
        "                    '3': 'S3',\n",
        "                    '4': 'S4',\n",
        "                    'R': 'REM'\n",
        "                }\n",
        "                \n",
        "                mapped_stage = stage_map.get(stage, stage)\n",
        "                sleep_stages.append(mapped_stage)\n",
        "        \n",
        "        return sleep_stages\n",
        "        \n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fix_sleep_stages(features_file, data_path):\n",
        "    \"\"\"Fix completo con diagn√≥stico autom√°tico\"\"\"\n",
        "    \n",
        "    # 1. Diagn√≥stico\n",
        "    df, hypno_files = diagnose_files(features_file, data_path)\n",
        "    \n",
        "    if len(hypno_files) == 0:\n",
        "        print(\"\\n No se encontraron archivos hypnogram\")\n",
        "        return None\n",
        "    \n",
        "    # 2. Crear mapeo\n",
        "    subject_ids = df['subject_id'].unique()\n",
        "    mapping = create_subject_mapping(subject_ids, hypno_files)\n",
        "    \n",
        "    if len(mapping) == 0:\n",
        "        print(\"\\n No se pudo mapear ning√∫n sujeto\")\n",
        "        print(\"\\n Posibles soluciones:\")\n",
        "        print(\"   1. Verifica que los subject_ids coincidan con nombres de archivos\")\n",
        "        print(\"   2. Los archivos deben contener 'Hypnogram' en el nombre\")\n",
        "        return None\n",
        "    \n",
        "    # 3. Cargar hypnogramas\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CARGANDO HYPNOGRAMAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    hypnogram_cache = {}\n",
        "    for subject_id, hypno_file in tqdm(mapping.items(), desc=\"Cargando\"):\n",
        "        stages = load_hypnogram(hypno_file)\n",
        "        if stages:\n",
        "            hypnogram_cache[subject_id] = stages\n",
        "    \n",
        "    print(f\"   ‚úì Hypnogramas cargados: {len(hypnogram_cache)}\")\n",
        "    \n",
        "    # 4. Asignar etiquetas\n",
        "    print(\"\\n  Asignando sleep stages...\")\n",
        "    \n",
        "    df['sleep_stage_new'] = None\n",
        "    assigned = 0\n",
        "    \n",
        "    for subject_id in tqdm(hypnogram_cache.keys(), desc=\"Asignando\"):\n",
        "        sleep_stages = hypnogram_cache[subject_id]\n",
        "        \n",
        "        mask = df['subject_id'] == subject_id\n",
        "        subject_indices = df[mask].index\n",
        "        \n",
        "        for idx in subject_indices:\n",
        "            epoch_idx = df.loc[idx, 'epoch_idx']\n",
        "            \n",
        "            if epoch_idx < len(sleep_stages):\n",
        "                df.loc[idx, 'sleep_stage_new'] = sleep_stages[epoch_idx]\n",
        "                assigned += 1\n",
        "    \n",
        "    print(f\"\\n    √âpocas etiquetadas: {assigned:,} / {len(df):,}\")\n",
        "    \n",
        "    # 5. Reemplazar columna\n",
        "    df['sleep_stage'] = df['sleep_stage_new']\n",
        "    df = df.drop(columns=['sleep_stage_new'])\n",
        "    \n",
        "    # 6. Filtrar v√°lidas\n",
        "    valid_stages = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
        "    df_valid = df[df['sleep_stage'].isin(valid_stages)]\n",
        "    \n",
        "    print(f\"   Shape final: {df_valid.shape}\")\n",
        "    \n",
        "    # 7. Distribuci√≥n\n",
        "    print(f\"\\n Distribuci√≥n de Sleep Stages:\")\n",
        "    stage_counts = df_valid['sleep_stage'].value_counts()\n",
        "    for stage, count in stage_counts.items():\n",
        "        pct = (count / len(df_valid)) * 100\n",
        "        print(f\"      {stage:5s}: {count:7,} ({pct:5.1f}%)\")\n",
        "    \n",
        "    return df_valid\n",
        "\n",
        "\n",
        "def save_fixed_file(df, original_file):\n",
        "    \"\"\"Guarda archivo reparado\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GUARDANDO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    output_file = Path(original_file).parent / \"features_FIXED.parquet\"\n",
        "    \n",
        "    df.to_parquet(output_file, index=False)\n",
        "    size_mb = output_file.stat().st_size / (1024*1024)\n",
        "    \n",
        "    print(f\"    Archivo: {output_file.name}\")\n",
        "    print(f\"    Tama√±o: {size_mb:.1f} MB\")\n",
        "    print(f\"    Shape: {df.shape}\")\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" DIAGN√ìSTICO Y FIX AUTOM√ÅTICO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    features_file = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\features_batch\\features_complete_labeled.parquet\"\n",
        "    data_path = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    \n",
        "    if not Path(features_file).exists():\n",
        "        print(f\" No se encuentra: {features_file}\")\n",
        "        return\n",
        "    \n",
        "    if not Path(data_path).exists():\n",
        "        print(f\" No se encuentra: {data_path}\")\n",
        "        return\n",
        "    \n",
        "    # Fix\n",
        "    df_fixed = fix_sleep_stages(features_file, data_path)\n",
        "    \n",
        "    if df_fixed is None or len(df_fixed) == 0:\n",
        "        print(\"\\n No se pudo reparar el archivo\")\n",
        "        return\n",
        "    \n",
        "    # Guardar\n",
        "    output_file = save_fixed_file(df_fixed, features_file)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETADO\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n Usa:\")\n",
        "    print(f\"   df = pd.read_parquet('{output_file.name}')\")\n",
        "    \n",
        "    return df_fixed\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NCA y selecci√≥n de mejores features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "NCA Feature Selection - An√°lisis exploratorio\n",
        "Selecciona 30 mejores features basado en varianza y correlaci√≥n\n",
        "Ya que no tenemos etiquetas, usamos m√©todos no supervisados\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "class FeatureSelectorUnsupervised:\n",
        "    \"\"\"\n",
        "    Selector de features usando m√©todos no supervisados\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, features_file, output_dir):\n",
        "        self.features_file = Path(features_file)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "    def load_data(self):\n",
        "        \"\"\"Carga datos\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" CARGANDO DATOS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        df = pd.read_parquet(self.features_file)\n",
        "        print(f\"    Shape: {df.shape}\")\n",
        "        print(f\"    Columnas: {df.columns.tolist()[:10]} ... (+{len(df.columns)-10} m√°s)\")\n",
        "        \n",
        "        # Detectar columnas de metadata autom√°ticamente\n",
        "        possible_meta = ['subject_id', 'epoch_idx', 'sleep_stage', 'channel', \n",
        "                        'subject', 'epoch', 'label', 'stage']\n",
        "        meta_cols = [col for col in df.columns if col in possible_meta]\n",
        "        \n",
        "        print(f\"    Columnas de metadata detectadas: {meta_cols}\")\n",
        "        \n",
        "        # Features son todas las dem√°s\n",
        "        feature_cols = [col for col in df.columns if col not in meta_cols]\n",
        "        \n",
        "        print(f\"    Features: {len(feature_cols)}\")\n",
        "        print(f\"    Sujetos: {df['subject_id'].nunique() if 'subject_id' in df.columns else 'N/A'}\")\n",
        "        \n",
        "        return df, feature_cols, meta_cols\n",
        "    \n",
        "    def select_features_by_variance_and_correlation(self, df, feature_cols, n_features=30):\n",
        "        \"\"\"\n",
        "        Selecciona features basado en:\n",
        "        1. Varianza (elimina features con poca variabilidad)\n",
        "        2. Correlaci√≥n (elimina redundantes)\n",
        "        3. PCA para encontrar las m√°s importantes\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SELECCI√ìN DE FEATURES - M√©todo H√≠brido\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Filtrar solo columnas num√©ricas\n",
        "        print(f\"\\nüîç Filtrando columnas num√©ricas...\")\n",
        "        df_features = df[feature_cols]\n",
        "        \n",
        "        # Identificar columnas num√©ricas\n",
        "        numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        print(f\"   Features num√©ricas: {len(numeric_cols)} / {len(feature_cols)}\")\n",
        "        \n",
        "        if len(numeric_cols) == 0:\n",
        "            print(\"    No hay columnas num√©ricas!\")\n",
        "            return [], None, None, None\n",
        "        \n",
        "        # Usar solo num√©ricas\n",
        "        X = df_features[numeric_cols].values\n",
        "        print(f\"   X shape: {X.shape}\")\n",
        "        \n",
        "        # Limpiar datos\n",
        "        print(f\"\\n Limpieza:\")\n",
        "        nan_count = np.isnan(X).sum() if X.dtype in [np.float32, np.float64] else 0\n",
        "        inf_count = np.isinf(X).sum() if X.dtype in [np.float32, np.float64] else 0\n",
        "        print(f\"   NaNs: {nan_count}\")\n",
        "        print(f\"   Infs: {inf_count}\")\n",
        "        \n",
        "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        \n",
        "        # Actualizar feature_cols con solo las num√©ricas\n",
        "        feature_cols = numeric_cols\n",
        "        \n",
        "        # Normalizar\n",
        "        print(f\"\\n Normalizando...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        \n",
        "        # 1. Filtrar por varianza\n",
        "        print(f\"\\n  1 Filtro por Varianza\")\n",
        "        selector = VarianceThreshold(threshold=0.01)  # Eliminar features con varianza muy baja\n",
        "        X_var = selector.fit_transform(X_scaled)\n",
        "        selected_mask = selector.get_support()\n",
        "        features_after_var = [f for f, m in zip(feature_cols, selected_mask) if m]\n",
        "        \n",
        "        print(f\"   Features despu√©s de filtro: {len(features_after_var)} / {len(feature_cols)}\")\n",
        "        \n",
        "        # 2. An√°lisis de correlaci√≥n\n",
        "        print(f\"\\n 2 An√°lisis de Correlaci√≥n\")\n",
        "        df_features = pd.DataFrame(X_var, columns=features_after_var)\n",
        "        corr_matrix = df_features.corr().abs()\n",
        "        \n",
        "        # Encontrar pares altamente correlacionados\n",
        "        upper_triangle = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "        \n",
        "        # Eliminar features con correlaci√≥n > 0.95\n",
        "        to_drop = [column for column in upper_triangle.columns \n",
        "                   if any(upper_triangle[column] > 0.95)]\n",
        "        \n",
        "        features_after_corr = [f for f in features_after_var if f not in to_drop]\n",
        "        X_corr = df_features[features_after_corr].values\n",
        "        \n",
        "        print(f\"   Features altamente correlacionadas eliminadas: {len(to_drop)}\")\n",
        "        print(f\"   Features restantes: {len(features_after_corr)}\")\n",
        "        \n",
        "        # 3. PCA para importancia\n",
        "        print(f\"\\n 3 PCA para Ranking de Importancia\")\n",
        "        pca = PCA(n_components=min(50, len(features_after_corr)))\n",
        "        X_pca = pca.fit_transform(X_corr)\n",
        "        \n",
        "        # Importancia basada en componentes principales\n",
        "        # Features que contribuyen m√°s a los primeros componentes son m√°s importantes\n",
        "        components_abs = np.abs(pca.components_)\n",
        "        \n",
        "        # Ponderar por varianza explicada\n",
        "        weighted_importance = np.zeros(len(features_after_corr))\n",
        "        for i, comp in enumerate(components_abs):\n",
        "            weighted_importance += comp * pca.explained_variance_ratio_[i]\n",
        "        \n",
        "        # Seleccionar top N\n",
        "        top_indices = np.argsort(weighted_importance)[-n_features:][::-1]\n",
        "        selected_features = [features_after_corr[i] for i in top_indices]\n",
        "        \n",
        "        print(f\"\\n    Top {n_features} features seleccionadas\")\n",
        "        \n",
        "        # Mostrar top 15\n",
        "        print(f\"\\n Top 15 Features:\")\n",
        "        for i, feat in enumerate(selected_features[:15], 1):\n",
        "            imp = weighted_importance[features_after_corr.index(feat)]\n",
        "            print(f\"      {i:2d}. {feat:60s} (imp: {imp:.6f})\")\n",
        "        \n",
        "        return selected_features, scaler, pca, weighted_importance\n",
        "    \n",
        "    def separate_by_channel_type(self, selected_features):\n",
        "        \"\"\"\n",
        "        Separa features por tipo de canal\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SEPARACI√ìN POR CANAL\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        eog_features = [f for f in selected_features if 'EOG' in f.upper()]\n",
        "        eeg_features = [f for f in selected_features if 'EEG' in f.upper()]\n",
        "        \n",
        "        print(f\"   EOG features: {len(eog_features)}\")\n",
        "        print(f\"   EEG features: {len(eeg_features)}\")\n",
        "        \n",
        "        return eog_features, eeg_features\n",
        "    \n",
        "    def save_results(self, df, selected_features, scaler, pca, output_name, description, meta_cols):\n",
        "        \"\"\"Guarda resultados\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\" GUARDANDO - {output_name}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        output_path = self.output_dir / output_name\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # 1. DataFrame con features seleccionadas\n",
        "        # Usar solo las columnas de metadata que existen\n",
        "        existing_meta = [col for col in meta_cols if col in df.columns]\n",
        "        cols_to_save = existing_meta + selected_features\n",
        "        df_selected = df[cols_to_save].copy()\n",
        "        \n",
        "        # Parquet\n",
        "        parquet_file = output_path / \"features_selected_30.parquet\"\n",
        "        df_selected.to_parquet(parquet_file, index=False)\n",
        "        size_mb = parquet_file.stat().st_size / (1024*1024)\n",
        "        print(f\"   ‚úì Parquet: {parquet_file.name} ({size_mb:.1f} MB)\")\n",
        "        \n",
        "        # Pickle\n",
        "        pkl_file = output_path / \"features_selected_30.pkl\"\n",
        "        df_selected.to_pickle(pkl_file, compression='gzip')\n",
        "        print(f\"   ‚úì Pickle: {pkl_file.name}\")\n",
        "        \n",
        "        # 2. Lista de features\n",
        "        features_file = output_path / \"selected_features_list.txt\"\n",
        "        with open(features_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(f\"FEATURES SELECCIONADAS - {description}\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "            f.write(f\"Total: {len(selected_features)}\\n\")\n",
        "            f.write(f\"M√©todo: Varianza + Correlaci√≥n + PCA\\n\\n\")\n",
        "            \n",
        "            for i, feat in enumerate(selected_features, 1):\n",
        "                f.write(f\"{i:2d}. {feat}\\n\")\n",
        "        print(f\"   ‚úì Lista: {features_file.name}\")\n",
        "        \n",
        "        # 3. Modelo\n",
        "        model_file = output_path / \"feature_selector.pkl\"\n",
        "        with open(model_file, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'scaler': scaler,\n",
        "                'pca': pca,\n",
        "                'selected_features': selected_features\n",
        "            }, f)\n",
        "        print(f\"   ‚úì Modelo: {model_file.name}\")\n",
        "        \n",
        "        # 4. Info\n",
        "        info_file = output_path / \"info.txt\"\n",
        "        with open(info_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(f\"INFORMACI√ìN - {description}\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "            \n",
        "            f.write(f\"Shape: {df_selected.shape}\\n\")\n",
        "            f.write(f\"√âpocas: {len(df_selected):,}\\n\")\n",
        "            f.write(f\"Features: {len(selected_features)}\\n\")\n",
        "            \n",
        "            if 'subject_id' in df_selected.columns:\n",
        "                f.write(f\"Sujetos: {df_selected['subject_id'].nunique()}\\n\\n\")\n",
        "                f.write(\"Sujetos:\\n\")\n",
        "                for subject in sorted(df_selected['subject_id'].unique()):\n",
        "                    n = (df_selected['subject_id'] == subject).sum()\n",
        "                    f.write(f\"  {subject}: {n:,} √©pocas\\n\")\n",
        "        print(f\"   ‚úì Info: {info_file.name}\")\n",
        "        \n",
        "        # 5. Visualizaci√≥n\n",
        "        self._plot_feature_importance(selected_features[:15], output_path)\n",
        "        \n",
        "        print(f\"\\n Guardado en: {output_path}\")\n",
        "        \n",
        "        return df_selected\n",
        "    \n",
        "    def _plot_feature_importance(self, top_features, output_path):\n",
        "        \"\"\"Plot\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        y_pos = np.arange(len(top_features))\n",
        "        plt.barh(y_pos, range(len(top_features), 0, -1), color='steelblue')\n",
        "        plt.yticks(y_pos, top_features, fontsize=8)\n",
        "        plt.xlabel('Ranking', fontsize=11, fontweight='bold')\n",
        "        plt.title('Top 15 Features Seleccionadas', fontsize=13, fontweight='bold')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_file = output_path / \"feature_ranking.png\"\n",
        "        plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"   ‚úì Gr√°fico: {plot_file.name}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" SELECCI√ìN DE 30 MEJORES FEATURES\")\n",
        "    print(\"   M√©todo: Varianza + Correlaci√≥n + PCA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Configuraci√≥n\n",
        "    features_file = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\features_batch\\features_complete_labeled.parquet\"\n",
        "    output_dir = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\"\n",
        "    \n",
        "    if not Path(features_file).exists():\n",
        "        print(f\" No se encuentra: {features_file}\")\n",
        "        return\n",
        "    \n",
        "    # Crear selector\n",
        "    selector = FeatureSelectorUnsupervised(features_file, output_dir)\n",
        "    \n",
        "    # Cargar datos\n",
        "    df, feature_cols, meta_cols = selector.load_data()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # PARTE 1: TODAS LAS FEATURES (EOG + EEG combinadas)\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" SELECCI√ìN GLOBAL: EOG + EEG\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    selected_all, scaler_all, pca_all, importance_all = \\\n",
        "        selector.select_features_by_variance_and_correlation(df, feature_cols, n_features=30)\n",
        "    \n",
        "    # Guardar\n",
        "    df_all_selected = selector.save_results(\n",
        "        df, selected_all, scaler_all, pca_all,\n",
        "        \"selected_features_all\", \"EOG + EEG (30 features)\", meta_cols\n",
        "    )\n",
        "    \n",
        "    # ========================================================================\n",
        "    # PARTE 2: SEPARAR POR TIPO (EOG vs EEG)\n",
        "    # ========================================================================\n",
        "    eog_features, eeg_features = selector.separate_by_channel_type(selected_all)\n",
        "    \n",
        "    if len(eog_features) > 0:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\" SUBSET: Solo EOG ({len(eog_features)} features)\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        df_eog_selected = selector.save_results(\n",
        "            df, eog_features, scaler_all, pca_all,\n",
        "            \"selected_features_eog_only\", f\"Solo EOG ({len(eog_features)} features)\", meta_cols\n",
        "        )\n",
        "    \n",
        "    if len(eeg_features) > 0:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\" SUBSET: Solo EEG ({len(eeg_features)} features)\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        df_eeg_selected = selector.save_results(\n",
        "            df, eeg_features, scaler_all, pca_all,\n",
        "            \"selected_features_eeg_only\", f\"Solo EEG ({len(eeg_features)} features)\", meta_cols\n",
        "        )\n",
        "    \n",
        "    # ========================================================================\n",
        "    # RESUMEN\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETADO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\n Resultados en: {output_dir}\")\n",
        "    print(f\"\\n Carpetas creadas:\")\n",
        "    print(f\"   1. selected_features_all/       - EOG + EEG (30 features)\")\n",
        "    if len(eog_features) > 0:\n",
        "        print(f\"   2. selected_features_eog_only/  - Solo EOG ({len(eog_features)} features)\")\n",
        "    if len(eeg_features) > 0:\n",
        "        print(f\"   3. selected_features_eeg_only/  - Solo EEG ({len(eeg_features)} features)\")\n",
        "    \n",
        "    print(f\"\\n  NOTA: Las etiquetas de sleep_stage est√°n en -1\")\n",
        "    print(f\"   Necesitar√°s cargar las etiquetas reales desde los hypnogramas\")\n",
        "    print(f\"   o desde otro archivo para poder entrenar modelos.\")\n",
        "    \n",
        "    print(f\"\\n Para usar:\")\n",
        "    print(f\"   df = pd.read_parquet('selected_features_all/features_selected_30.parquet')\")\n",
        "    print(f\"   # Asignar etiquetas manualmente despu√©s\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fix completo: Lee hypnogramas correctamente y actualiza archivos NCA\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def parse_sleep_edf_hypnogram(hypno_file):\n",
        "    \"\"\"\n",
        "    Parser especializado para hypnogramas de Sleep-EDF\n",
        "    Formato: +30630Sleep stage W (tiempo en d√©cimas de segundo + duraci√≥n + stage)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Leer el archivo raw\n",
        "        with open(hypno_file, 'rb') as f:\n",
        "            content = f.read()\n",
        "        \n",
        "        # Buscar anotaciones (despu√©s del header EDF)\n",
        "        annotations_start = content.find(b'Sleep stage')\n",
        "        if annotations_start == -1:\n",
        "            return None\n",
        "        \n",
        "        # Extraer todas las anotaciones\n",
        "        annotations_text = content[annotations_start:].decode('latin-1', errors='ignore')\n",
        "        \n",
        "        # Buscar patrones: \"Sleep stage X\" donde X puede ser W, 1, 2, 3, 4, R, ?\n",
        "        pattern = r'Sleep stage ([W1234R\\?])'\n",
        "        matches = re.findall(pattern, annotations_text)\n",
        "        \n",
        "        if not matches:\n",
        "            return None\n",
        "        \n",
        "        # Mapear etiquetas\n",
        "        stage_map = {\n",
        "            'W': 'W',\n",
        "            '?': 'Unknown',\n",
        "            '1': 'S1',\n",
        "            '2': 'S2',\n",
        "            '3': 'S3',\n",
        "            '4': 'S4',\n",
        "            'R': 'REM'\n",
        "        }\n",
        "        \n",
        "        sleep_stages = [stage_map.get(stage, stage) for stage in matches]\n",
        "        \n",
        "        return sleep_stages\n",
        "        \n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_subject_mapping(subject_ids, data_path):\n",
        "    \"\"\"\n",
        "    Mapea subject_ids con archivos hypnogram\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üîó MAPEANDO SUJETOS CON HYPNOGRAMAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    data_path = Path(data_path)\n",
        "    hypno_files = list(data_path.glob(\"*Hypnogram*.edf\"))\n",
        "    \n",
        "    print(f\"   Archivos hypnogram encontrados: {len(hypno_files)}\")\n",
        "    \n",
        "    mapping = {}\n",
        "    \n",
        "    for subject_id in tqdm(subject_ids, desc=\"Mapeando\"):\n",
        "        # Patr√≥n base: SC4001E (sin √∫ltimo d√≠gito)\n",
        "        base_pattern = subject_id[:-1]\n",
        "        \n",
        "        for hypno_file in hypno_files:\n",
        "            if base_pattern in hypno_file.name:\n",
        "                mapping[subject_id] = hypno_file\n",
        "                break\n",
        "    \n",
        "    print(f\"   ‚úì Mapeos exitosos: {len(mapping)} / {len(subject_ids)}\")\n",
        "    \n",
        "    if len(mapping) > 0:\n",
        "        print(f\"\\n   Ejemplos:\")\n",
        "        for i, (sid, hfile) in enumerate(list(mapping.items())[:5]):\n",
        "            print(f\"      '{sid}' ‚Üí {hfile.name}\")\n",
        "    \n",
        "    return mapping\n",
        "\n",
        "\n",
        "def load_all_hypnograms(mapping):\n",
        "    \"\"\"\n",
        "    Carga todos los hypnogramas\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CARGANDO HYPNOGRAMAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    hypnogram_cache = {}\n",
        "    failed = []\n",
        "    \n",
        "    for subject_id, hypno_file in tqdm(mapping.items(), desc=\"Cargando\"):\n",
        "        stages = parse_sleep_edf_hypnogram(hypno_file)\n",
        "        \n",
        "        if stages:\n",
        "            hypnogram_cache[subject_id] = stages\n",
        "        else:\n",
        "            failed.append(subject_id)\n",
        "    \n",
        "    print(f\"\\n   ‚úì Exitosos: {len(hypnogram_cache)}\")\n",
        "    if failed:\n",
        "        print(f\"     Fallidos: {len(failed)}\")\n",
        "        for sid in failed[:5]:\n",
        "            print(f\"      - {sid}\")\n",
        "    \n",
        "    return hypnogram_cache\n",
        "\n",
        "\n",
        "def assign_labels_to_dataframe(df, hypnogram_cache):\n",
        "    \"\"\"\n",
        "    Asigna labels del hypnogram cache al DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"  ASIGNANDO ETIQUETAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Crear nueva columna\n",
        "    df['sleep_stage_corrected'] = None\n",
        "    assigned = 0\n",
        "    \n",
        "    # Por cada sujeto con hypnogram\n",
        "    for subject_id in tqdm(hypnogram_cache.keys(), desc=\"Asignando\"):\n",
        "        stages = hypnogram_cache[subject_id]\n",
        "        \n",
        "        # Obtener filas de este sujeto\n",
        "        mask = df['subject_id'] == subject_id\n",
        "        subject_indices = df[mask].index\n",
        "        \n",
        "        # Asignar por √≠ndice de √©poca\n",
        "        # Asumiendo que las filas est√°n ordenadas por √©poca\n",
        "        for i, idx in enumerate(subject_indices):\n",
        "            if i < len(stages):\n",
        "                df.loc[idx, 'sleep_stage_corrected'] = stages[i]\n",
        "                assigned += 1\n",
        "    \n",
        "    print(f\"\\n    √âpocas etiquetadas: {assigned:,} / {len(df):,}\")\n",
        "    \n",
        "    # Reemplazar columna original\n",
        "    df['sleep_stage'] = df['sleep_stage_corrected']\n",
        "    df = df.drop(columns=['sleep_stage_corrected'])\n",
        "    \n",
        "    # Filtrar solo v√°lidas\n",
        "    valid_stages = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
        "    df_valid = df[df['sleep_stage'].isin(valid_stages)].copy()\n",
        "    \n",
        "    print(f\"   ‚úì √âpocas v√°lidas: {len(df_valid):,}\")\n",
        "    \n",
        "    # Distribuci√≥n\n",
        "    print(f\"\\n Distribuci√≥n:\")\n",
        "    stage_counts = df_valid['sleep_stage'].value_counts()\n",
        "    for stage, count in sorted(stage_counts.items()):\n",
        "        pct = (count / len(df_valid)) * 100\n",
        "        print(f\"      {stage:5s}: {count:7,} ({pct:5.1f}%)\")\n",
        "    \n",
        "    return df_valid\n",
        "\n",
        "\n",
        "def update_nca_folder(folder_path, hypnogram_cache, data_path):\n",
        "    \"\"\"\n",
        "    Actualiza un folder NCA con las etiquetas correctas\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" ACTUALIZANDO: {folder_path.name}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    parquet_file = folder_path / \"features_selected_30.parquet\"\n",
        "    \n",
        "    if not parquet_file.exists():\n",
        "        print(f\"   ‚ö†Ô∏è  No se encuentra: {parquet_file.name}\")\n",
        "        return None\n",
        "    \n",
        "    # Cargar\n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    print(f\"   ‚úì Cargado: {df.shape}\")\n",
        "    print(f\"   ‚úì Sleep stages actuales: {df['sleep_stage'].unique()}\")\n",
        "    \n",
        "    # Asignar labels\n",
        "    df_fixed = assign_labels_to_dataframe(df, hypnogram_cache)\n",
        "    \n",
        "    if len(df_fixed) == 0:\n",
        "        print(f\"   ‚ùå No quedaron filas v√°lidas\")\n",
        "        return None\n",
        "    \n",
        "    # Guardar actualizado\n",
        "    print(f\"\\n Guardando versi√≥n corregida...\")\n",
        "    \n",
        "    # Backup del original\n",
        "    backup_file = folder_path / \"features_selected_30_BACKUP.parquet\"\n",
        "    df.to_parquet(backup_file, index=False)\n",
        "    print(f\"   ‚úì Backup: {backup_file.name}\")\n",
        "    \n",
        "    # Sobrescribir con versi√≥n corregida\n",
        "    df_fixed.to_parquet(parquet_file, index=False)\n",
        "    size_mb = parquet_file.stat().st_size / (1024*1024)\n",
        "    print(f\"   ‚úì Actualizado: {parquet_file.name} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Tambi√©n pickle\n",
        "    pkl_file = folder_path / \"features_selected_30.pkl\"\n",
        "    df_fixed.to_pickle(pkl_file, compression='gzip')\n",
        "    print(f\"   ‚úì Actualizado: {pkl_file.name}\")\n",
        "    \n",
        "    # Actualizar info.txt\n",
        "    info_file = folder_path / \"info_UPDATED.txt\"\n",
        "    with open(info_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"ARCHIVO ACTUALIZADO CON ETIQUETAS CORRECTAS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Shape: {df_fixed.shape}\\n\")\n",
        "        f.write(f\"√âpocas: {len(df_fixed):,}\\n\")\n",
        "        f.write(f\"Sujetos: {df_fixed['subject_id'].nunique()}\\n\\n\")\n",
        "        \n",
        "        f.write(\"Distribuci√≥n de Sleep Stages:\\n\")\n",
        "        stage_counts = df_fixed['sleep_stage'].value_counts()\n",
        "        for stage, count in sorted(stage_counts.items()):\n",
        "            pct = (count / len(df_fixed)) * 100\n",
        "            f.write(f\"  {stage:5s}: {count:7,} ({pct:5.1f}%)\\n\")\n",
        "        \n",
        "        f.write(f\"\\nSujetos:\\n\")\n",
        "        for subject in sorted(df_fixed['subject_id'].unique()):\n",
        "            n = (df_fixed['subject_id'] == subject).sum()\n",
        "            f.write(f\"  {subject}: {n:,} √©pocas\\n\")\n",
        "    \n",
        "    print(f\"   ‚úì Info: {info_file.name}\")\n",
        "    \n",
        "    return df_fixed\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" FIX COMPLETO: ETIQUETAS + ACTUALIZACI√ìN NCA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Rutas\n",
        "    nca_base_dir = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\"\n",
        "    data_path = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\sleep-cassette\"\n",
        "    \n",
        "    nca_folders = [\n",
        "        \"selected_features_all\",\n",
        "        \"selected_features_eog_only\",\n",
        "        \"selected_features_eeg_only\"\n",
        "    ]\n",
        "    \n",
        "    # Verificar data_path\n",
        "    if not Path(data_path).exists():\n",
        "        print(f\"‚ùå No se encuentra: {data_path}\")\n",
        "        return\n",
        "    \n",
        "    # Cargar una carpeta para obtener subject_ids\n",
        "    first_folder = Path(nca_base_dir) / nca_folders[0]\n",
        "    if not first_folder.exists():\n",
        "        print(f\"‚ùå No se encuentra: {first_folder}\")\n",
        "        return\n",
        "    \n",
        "    parquet_file = first_folder / \"features_selected_30.parquet\"\n",
        "    if not parquet_file.exists():\n",
        "        print(f\"‚ùå No se encuentra: {parquet_file}\")\n",
        "        return\n",
        "    \n",
        "    # Leer para obtener subject_ids\n",
        "    print(f\"\\n Leyendo: {parquet_file}\")\n",
        "    df_sample = pd.read_parquet(parquet_file)\n",
        "    subject_ids = df_sample['subject_id'].unique()\n",
        "    print(f\"   ‚úì Sujetos encontrados: {len(subject_ids)}\")\n",
        "    \n",
        "    # Crear mapeo\n",
        "    mapping = create_subject_mapping(subject_ids, data_path)\n",
        "    \n",
        "    if len(mapping) == 0:\n",
        "        print(\"\\n No se pudo mapear ning√∫n sujeto\")\n",
        "        return\n",
        "    \n",
        "    # Cargar hypnogramas\n",
        "    hypnogram_cache = load_all_hypnograms(mapping)\n",
        "    \n",
        "    if len(hypnogram_cache) == 0:\n",
        "        print(\"\\n No se pudo cargar ning√∫n hypnogram\")\n",
        "        return\n",
        "    \n",
        "    # Actualizar cada carpeta NCA\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" ACTUALIZANDO CARPETAS NCA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for folder_name in nca_folders:\n",
        "        folder_path = Path(nca_base_dir) / folder_name\n",
        "        \n",
        "        if folder_path.exists():\n",
        "            df_updated = update_nca_folder(folder_path, hypnogram_cache, data_path)\n",
        "        else:\n",
        "            print(f\"\\n  No existe: {folder_name}\")\n",
        "    \n",
        "    # Resumen final\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" ACTUALIZACI√ìN COMPLETADA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\n Carpetas actualizadas:\")\n",
        "    for folder_name in nca_folders:\n",
        "        folder_path = Path(nca_base_dir) / folder_name\n",
        "        if folder_path.exists():\n",
        "            parquet = folder_path / \"features_selected_30.parquet\"\n",
        "            if parquet.exists():\n",
        "                df_check = pd.read_parquet(parquet)\n",
        "                valid_count = df_check['sleep_stage'].isin(['W','S1','S2','S3','S4','REM']).sum()\n",
        "                print(f\"   ‚úì {folder_name}\")\n",
        "                print(f\"      - √âpocas: {len(df_check):,}\")\n",
        "                print(f\"      - V√°lidas: {valid_count:,}\")\n",
        "                print(f\"      - Backup guardado: features_selected_30_BACKUP.parquet\")\n",
        "    \n",
        "    print(f\"\\n Para usar:\")\n",
        "    print(f\"   import pandas as pd\")\n",
        "    print(f\"   df = pd.read_parquet('selected_features_all/features_selected_30.parquet')\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   # Ahora sleep_stage tiene valores correctos: W, S1, S2, S3, S4, REM\")\n",
        "    print(f\"   print(df['sleep_stage'].value_counts())\")\n",
        "    \n",
        "    print(f\"\\n Para entrenar:\")\n",
        "    print(f\"   # Separar por sujetos (no por √©pocas)\")\n",
        "    print(f\"   from sklearn.model_selection import train_test_split\")\n",
        "    print(f\"   subjects = df['subject_id'].unique()\")\n",
        "    print(f\"   train_subj, test_subj = train_test_split(subjects, test_size=0.2, random_state=42)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Diagn√≥stico: ¬øPor qu√© solo se etiquetaron 21K de 450K √©pocas?\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def diagnose_dataframe(parquet_file):\n",
        "    \"\"\"\n",
        "    Investiga la estructura del DataFrame para entender las √©pocas\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" DIAGN√ìSTICO DE ESTRUCTURA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    \n",
        "    print(f\"\\n Informaci√≥n b√°sica:\")\n",
        "    print(f\"   Total de filas: {len(df):,}\")\n",
        "    print(f\"   Columnas: {df.columns.tolist()}\")\n",
        "    \n",
        "    # Analizar por sujeto\n",
        "    print(f\"\\nüë§ An√°lisis por sujeto:\")\n",
        "    print(f\"   Sujetos √∫nicos: {df['subject_id'].nunique()}\")\n",
        "    \n",
        "    # Ver un sujeto ejemplo\n",
        "    first_subject = df['subject_id'].iloc[0]\n",
        "    subject_df = df[df['subject_id'] == first_subject]\n",
        "    \n",
        "    print(f\"\\n Ejemplo - Sujeto: {first_subject}\")\n",
        "    print(f\"   Filas de este sujeto: {len(subject_df):,}\")\n",
        "    \n",
        "    # Ver si hay √≠ndices de √©poca\n",
        "    if 'epoch_idx' in df.columns:\n",
        "        print(f\"   Valores √∫nicos de epoch_idx: {subject_df['epoch_idx'].nunique()}\")\n",
        "        print(f\"   Rango epoch_idx: {subject_df['epoch_idx'].min()} - {subject_df['epoch_idx'].max()}\")\n",
        "    \n",
        "    # Ver primeras filas\n",
        "    print(f\"\\n Primeras 10 filas del sujeto:\")\n",
        "    display_cols = ['subject_id']\n",
        "    if 'epoch_idx' in df.columns:\n",
        "        display_cols.append('epoch_idx')\n",
        "    if 'sleep_stage' in df.columns:\n",
        "        display_cols.append('sleep_stage')\n",
        "    \n",
        "    # Agregar primeras 3 columnas de features\n",
        "    feature_cols = [col for col in df.columns if col not in ['subject_id', 'epoch_idx', 'sleep_stage']]\n",
        "    display_cols.extend(feature_cols[:3])\n",
        "    \n",
        "    print(subject_df[display_cols].head(10).to_string())\n",
        "    \n",
        "    # Hip√≥tesis\n",
        "    print(f\"\\n HIP√ìTESIS:\")\n",
        "    avg_rows_per_subject = len(df) / df['subject_id'].nunique()\n",
        "    print(f\"   Promedio filas por sujeto: {avg_rows_per_subject:.0f}\")\n",
        "    \n",
        "    if avg_rows_per_subject > 200:\n",
        "        print(f\"     Esto es MUY ALTO para √©pocas de sue√±o (t√≠pico: 100-200)\")\n",
        "        print(f\"   \")\n",
        "        print(f\"   Posibles causas:\")\n",
        "        print(f\"   1. Hay filas duplicadas por canal (EOG, EEG Fpz-Cz, EEG Pz-Oz)\")\n",
        "        print(f\"   2. Las features est√°n 'pivoteadas' pero epoch_idx se repite\")\n",
        "        print(f\"   3. Cada fila es una ventana/segmento, no una √©poca completa\")\n",
        "    \n",
        "    # Verificar duplicados\n",
        "    if 'epoch_idx' in df.columns:\n",
        "        print(f\"\\nüîç Verificando duplicados de √©poca:\")\n",
        "        dup_check = subject_df.groupby('epoch_idx').size()\n",
        "        if dup_check.max() > 1:\n",
        "            print(f\"     √âpocas duplicadas encontradas!\")\n",
        "            print(f\"   M√°ximo de filas por epoch_idx: {dup_check.max()}\")\n",
        "            print(f\"   Ejemplo √©poca duplicada:\")\n",
        "            dup_epoch = dup_check[dup_check > 1].index[0]\n",
        "            print(subject_df[subject_df['epoch_idx'] == dup_epoch][display_cols].to_string())\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def propose_solution(df):\n",
        "    \"\"\"\n",
        "    Propone soluci√≥n seg√∫n el diagn√≥stico\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" SOLUCI√ìN PROPUESTA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    if 'epoch_idx' in df.columns:\n",
        "        # Verificar si hay duplicados\n",
        "        test_subject = df['subject_id'].iloc[0]\n",
        "        test_df = df[df['subject_id'] == test_subject]\n",
        "        duplicates = test_df.groupby('epoch_idx').size().max() > 1\n",
        "        \n",
        "        if duplicates:\n",
        "            print(f\"\\n‚úÖ SOLUCI√ìN: Agregar/promediar filas duplicadas por √©poca\")\n",
        "            print(f\"   \")\n",
        "            print(f\"   Estrategia:\")\n",
        "            print(f\"   1. Agrupar por (subject_id, epoch_idx)\")\n",
        "            print(f\"   2. Promediar features num√©ricas\")\n",
        "            print(f\"   3. Tomar primera sleep_stage\")\n",
        "            print(f\"   4. Resultado: 1 fila por √©poca real\")\n",
        "            \n",
        "            return \"aggregate\"\n",
        "    \n",
        "    print(f\"\\n  Se necesita m√°s informaci√≥n para proponer soluci√≥n\")\n",
        "    print(f\"   Por favor comparte:\")\n",
        "    print(f\"   1. df.head(20) del archivo original\")\n",
        "    print(f\"   2. Confirmaci√≥n de cu√°ntas √©pocas REALES tiene cada sujeto\")\n",
        "    \n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "def aggregate_duplicates(df):\n",
        "    \"\"\"\n",
        "    Agrega filas duplicadas por √©poca\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üîß AGREGANDO DUPLICADOS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    if 'epoch_idx' not in df.columns:\n",
        "        print(\"    No hay columna epoch_idx\")\n",
        "        return df\n",
        "    \n",
        "    # Identificar columnas\n",
        "    group_cols = ['subject_id', 'epoch_idx']\n",
        "    meta_cols = ['sleep_stage']\n",
        "    feature_cols = [col for col in df.columns \n",
        "                   if col not in group_cols + meta_cols]\n",
        "    \n",
        "    print(f\"   Agrupando por: {group_cols}\")\n",
        "    print(f\"   Features a promediar: {len(feature_cols)}\")\n",
        "    \n",
        "    # Agregar\n",
        "    agg_dict = {}\n",
        "    \n",
        "    # Features: promedio\n",
        "    for col in feature_cols:\n",
        "        if df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
        "            agg_dict[col] = 'mean'\n",
        "    \n",
        "    # Sleep stage: primera ocurrencia\n",
        "    if 'sleep_stage' in df.columns:\n",
        "        agg_dict['sleep_stage'] = 'first'\n",
        "    \n",
        "    df_agg = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
        "    \n",
        "    print(f\"\\n   Antes: {len(df):,} filas\")\n",
        "    print(f\"    Despu√©s: {len(df_agg):,} filas\")\n",
        "    print(f\"   Reducci√≥n: {(1 - len(df_agg)/len(df))*100:.1f}%\")\n",
        "    \n",
        "    return df_agg\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" DIAGN√ìSTICO: ¬øPor qu√© solo 21K de 450K √©pocas?\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    parquet_file = r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\\selected_features_all\\features_selected_30.parquet\"\n",
        "    \n",
        "    if not Path(parquet_file).exists():\n",
        "        print(f\" No se encuentra: {parquet_file}\")\n",
        "        return\n",
        "    \n",
        "    # Diagn√≥stico\n",
        "    df = diagnose_dataframe(parquet_file)\n",
        "    \n",
        "    # Proponer soluci√≥n\n",
        "    solution_type = propose_solution(df)\n",
        "    \n",
        "    if solution_type == \"aggregate\":\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"¬øQuieres agregar duplicados ahora? (s/n)\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        try:\n",
        "            response = input().strip().lower()\n",
        "            \n",
        "            if response == 's':\n",
        "                # Agregar\n",
        "                df_fixed = aggregate_duplicates(df)\n",
        "                \n",
        "                # Guardar\n",
        "                output_file = Path(parquet_file).parent / \"features_selected_30_AGGREGATED.parquet\"\n",
        "                df_fixed.to_parquet(output_file, index=False)\n",
        "                \n",
        "                print(f\"\\n Guardado: {output_file.name}\")\n",
        "                print(f\"   Shape: {df_fixed.shape}\")\n",
        "                print(f\"   Ahora tienes 1 fila por √©poca real\")\n",
        "                \n",
        "                # Reasignar labels\n",
        "                print(f\" Ahora ejecuta el script de fix de labels nuevamente\")\n",
        "                print(f\"   pero usando este archivo agregado\")\n",
        "        except:\n",
        "            print(\"\\n   Modo autom√°tico, no se agreg√≥\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" SIGUIENTE PASO\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Comparte la salida de este diagn√≥stico\")\n",
        "    print(f\"para confirmar la soluci√≥n correcta.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Limpieza final: Remueve NaN y verifica que todo est√© correcto\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def analyze_and_clean_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Analiza y limpia un folder NCA\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" ANALIZANDO: {folder_path.name}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    parquet_file = folder_path / \"features_selected_30.parquet\"\n",
        "    \n",
        "    if not parquet_file.exists():\n",
        "        print(f\"    No existe: {parquet_file.name}\")\n",
        "        return None\n",
        "    \n",
        "    # Cargar\n",
        "    df = pd.read_parquet(parquet_file)\n",
        "    \n",
        "    print(f\"\\nüìä Estado actual:\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Sujetos: {df['subject_id'].nunique()}\")\n",
        "    \n",
        "    # Verificar sleep_stage\n",
        "    print(f\"\\n Sleep stages:\")\n",
        "    if 'sleep_stage' in df.columns:\n",
        "        stage_counts = df['sleep_stage'].value_counts()\n",
        "        print(f\"   Valores √∫nicos: {df['sleep_stage'].nunique()}\")\n",
        "        for stage, count in sorted(stage_counts.items()):\n",
        "            print(f\"      {stage}: {count:,}\")\n",
        "        \n",
        "        # Ver si hay -1 o None\n",
        "        invalid = df['sleep_stage'].isin([-1, 'Unknown', None, np.nan]).sum()\n",
        "        if invalid > 0:\n",
        "            print(f\"     Inv√°lidas: {invalid}\")\n",
        "    \n",
        "    # Verificar NaN en features\n",
        "    print(f\"\\n An√°lisis de NaN:\")\n",
        "    feature_cols = [col for col in df.columns \n",
        "                   if col not in ['subject_id', 'epoch_idx', 'sleep_stage']]\n",
        "    \n",
        "    nan_per_col = df[feature_cols].isna().sum()\n",
        "    cols_with_nan = nan_per_col[nan_per_col > 0]\n",
        "    \n",
        "    if len(cols_with_nan) > 0:\n",
        "        print(f\"     Columnas con NaN: {len(cols_with_nan)} / {len(feature_cols)}\")\n",
        "        print(f\"   Top 10 columnas con m√°s NaN:\")\n",
        "        for col, count in cols_with_nan.nlargest(10).items():\n",
        "            pct = (count / len(df)) * 100\n",
        "            print(f\"      {col[:50]:50s}: {count:6,} ({pct:5.1f}%)\")\n",
        "        \n",
        "        # NaN por fila\n",
        "        nan_per_row = df[feature_cols].isna().sum(axis=1)\n",
        "        rows_with_nan = (nan_per_row > 0).sum()\n",
        "        print(f\"\\n   Filas con alg√∫n NaN: {rows_with_nan:,} / {len(df):,}\")\n",
        "    else:\n",
        "        print(f\"    No hay NaN\")\n",
        "    \n",
        "    # Verificar epoch_idx\n",
        "    print(f\"\\n Verificar epoch_idx:\")\n",
        "    if 'epoch_idx' in df.columns:\n",
        "        print(f\"    Existe\")\n",
        "    else:\n",
        "        print(f\"     No existe - creando...\")\n",
        "        # Crear epoch_idx secuencial por sujeto\n",
        "        df['epoch_idx'] = df.groupby('subject_id').cumcount()\n",
        "        print(f\"    Creado\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    \"\"\"\n",
        "    Limpia el DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" LIMPIEZA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    original_len = len(df)\n",
        "    \n",
        "    # 1. Remover filas con sleep_stage inv√°lida\n",
        "    valid_stages = ['W', 'S1', 'S2', 'S3', 'S4', 'REM']\n",
        "    df = df[df['sleep_stage'].isin(valid_stages)].copy()\n",
        "    \n",
        "    removed_invalid = original_len - len(df)\n",
        "    if removed_invalid > 0:\n",
        "        print(f\"   ‚úì Removidas {removed_invalid:,} filas con sleep_stage inv√°lida\")\n",
        "    \n",
        "    # 2. Remover filas con muchos NaN\n",
        "    feature_cols = [col for col in df.columns \n",
        "                   if col not in ['subject_id', 'epoch_idx', 'sleep_stage']]\n",
        "    \n",
        "    # Contar NaN por fila\n",
        "    nan_per_row = df[feature_cols].isna().sum(axis=1)\n",
        "    \n",
        "    # Remover filas con m√°s del 50% de NaN\n",
        "    threshold = len(feature_cols) * 0.5\n",
        "    mask_valid = nan_per_row < threshold\n",
        "    \n",
        "    df_clean = df[mask_valid].copy()\n",
        "    \n",
        "    removed_nan = len(df) - len(df_clean)\n",
        "    if removed_nan > 0:\n",
        "        print(f\"   ‚úì Removidas {removed_nan:,} filas con >50% NaN\")\n",
        "    \n",
        "    # 3. Rellenar NaN restantes con media por columna\n",
        "    for col in feature_cols:\n",
        "        if df_clean[col].isna().sum() > 0:\n",
        "            mean_val = df_clean[col].mean()\n",
        "            df_clean[col] = df_clean[col].fillna(mean_val)\n",
        "    \n",
        "    print(f\"\\n    DataFrame limpio:\")\n",
        "    print(f\"      Filas: {original_len:,} ‚Üí {len(df_clean):,}\")\n",
        "    print(f\"      Reducci√≥n: {removed_invalid + removed_nan:,} ({(removed_invalid + removed_nan)/original_len*100:.1f}%)\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def save_cleaned(df, folder_path):\n",
        "    \"\"\"\n",
        "    Guarda DataFrame limpio\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GUARDANDO VERSI√ìN LIMPIA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Backup\n",
        "    parquet_file = folder_path / \"features_selected_30.parquet\"\n",
        "    backup_file = folder_path / \"features_selected_30_BEFORE_CLEAN.parquet\"\n",
        "    \n",
        "    if parquet_file.exists():\n",
        "        pd.read_parquet(parquet_file).to_parquet(backup_file, index=False)\n",
        "        print(f\"   ‚úì Backup: {backup_file.name}\")\n",
        "    \n",
        "    # Guardar limpio\n",
        "    df.to_parquet(parquet_file, index=False)\n",
        "    size_mb = parquet_file.stat().st_size / (1024*1024)\n",
        "    print(f\"   ‚úì Actualizado: {parquet_file.name} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Pickle\n",
        "    pkl_file = folder_path / \"features_selected_30.pkl\"\n",
        "    df.to_pickle(pkl_file, compression='gzip')\n",
        "    print(f\"   ‚úì Actualizado: {pkl_file.name}\")\n",
        "    \n",
        "    # Info actualizada\n",
        "    info_file = folder_path / \"info_FINAL.txt\"\n",
        "    with open(info_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"ARCHIVO FINAL - LIMPIO Y LISTO\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Shape: {df.shape}\\n\")\n",
        "        f.write(f\"√âpocas: {len(df):,}\\n\")\n",
        "        f.write(f\"Features: {len([c for c in df.columns if c not in ['subject_id', 'epoch_idx', 'sleep_stage']])}\\n\")\n",
        "        f.write(f\"Sujetos: {df['subject_id'].nunique()}\\n\\n\")\n",
        "        \n",
        "        f.write(\"Distribuci√≥n de Sleep Stages:\\n\")\n",
        "        stage_counts = df['sleep_stage'].value_counts()\n",
        "        for stage, count in sorted(stage_counts.items()):\n",
        "            pct = (count / len(df)) * 100\n",
        "            f.write(f\"  {stage:5s}: {count:7,} ({pct:5.1f}%)\\n\")\n",
        "        \n",
        "        f.write(f\"\\nSujetos (primeros 20):\\n\")\n",
        "        for subject in sorted(df['subject_id'].unique())[:20]:\n",
        "            n = (df['subject_id'] == subject).sum()\n",
        "            f.write(f\"  {subject}: {n:,} √©pocas\\n\")\n",
        "        \n",
        "        f.write(f\"\\nCalidad de datos:\\n\")\n",
        "        f.write(f\"  - Sin NaN: ‚úì\\n\")\n",
        "        f.write(f\"  - Sleep stages v√°lidas: ‚úì\\n\")\n",
        "        f.write(f\"  - epoch_idx presente: {'‚úì' if 'epoch_idx' in df.columns else '‚úó'}\\n\")\n",
        "    \n",
        "    print(f\"   ‚úì Info: {info_file.name}\")\n",
        "    \n",
        "    # Resumen final\n",
        "    print(f\"\\n RESUMEN FINAL:\")\n",
        "    print(f\"   √âpocas totales: {len(df):,}\")\n",
        "    print(f\"   Sujetos: {df['subject_id'].nunique()}\")\n",
        "    print(f\"   Promedio √©pocas/sujeto: {len(df) / df['subject_id'].nunique():.1f}\")\n",
        "    print(f\"\\n   Distribuci√≥n sleep stages:\")\n",
        "    for stage, count in sorted(df['sleep_stage'].value_counts().items()):\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"      {stage:5s}: {count:6,} ({pct:5.1f}%)\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" LIMPIEZA FINAL - Remover NaN y Verificar\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    base_dir = Path(r\"C:\\Users\\Alfredo Sempertegui\\Documents\\Proyecto IC\\sleep_edf\")\n",
        "    \n",
        "    folders = [\n",
        "        \"selected_features_all\",\n",
        "        \"selected_features_eog_only\", \n",
        "        \"selected_features_eeg_only\"\n",
        "    ]\n",
        "    \n",
        "    for folder_name in folders:\n",
        "        folder_path = base_dir / folder_name\n",
        "        \n",
        "        if not folder_path.exists():\n",
        "            print(f\"\\n  No existe: {folder_name}\")\n",
        "            continue\n",
        "        \n",
        "        # Analizar\n",
        "        df = analyze_and_clean_folder(folder_path)\n",
        "        \n",
        "        if df is None:\n",
        "            continue\n",
        "        \n",
        "        # Limpiar\n",
        "        df_clean = clean_dataframe(df)\n",
        "        \n",
        "        # Guardar\n",
        "        save_cleaned(df_clean, folder_path)\n",
        "    \n",
        "    # Resumen final\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PROCESO COMPLETADO\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\n Archivos actualizados en:\")\n",
        "    print(f\"   {base_dir}\")\n",
        "    \n",
        "    print(f\"\\n Para usar:\")\n",
        "    print(f\"   import pandas as pd\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   # Cargar dataset\")\n",
        "    print(f\"   df = pd.read_parquet('selected_features_all/features_selected_30.parquet')\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   # Verificar\")\n",
        "    print(f\"   print(f'Shape: {{df.shape}}')\")\n",
        "    print(f\"   print(f'Sujetos: {{df[\\\"subject_id\\\"].nunique()}}')\")\n",
        "    print(f\"   print(df['sleep_stage'].value_counts())\")\n",
        "    print(f\"   print(f'NaN: {{df.isna().sum().sum()}}')\")\n",
        "    \n",
        "    print(f\"\\n Para entrenar:\")\n",
        "    print(f\"   from sklearn.model_selection import train_test_split\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   # Separar por SUJETOS\")\n",
        "    print(f\"   subjects = df['subject_id'].unique()\")\n",
        "    print(f\"   train_subj, test_subj = train_test_split(subjects, test_size=0.2, random_state=42)\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   train_df = df[df['subject_id'].isin(train_subj)]\")\n",
        "    print(f\"   test_df = df[df['subject_id'].isin(test_subj)]\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   X_train = train_df.drop(['subject_id', 'sleep_stage', 'epoch_idx'], axis=1)\")\n",
        "    print(f\"   y_train = train_df['sleep_stage']\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "041c1685cdbb44ffb5d47fba8588f54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05c235ebc6604d12a5946ca5ef95f066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08750b3a66e84f75984cce56b1a3db83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be397daaa8447eb8cb5090bab3827be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e446dbb2a1548308109c120a97169d1",
            "value": "‚Äá306/306‚Äá[01:19&lt;00:00,‚Äá‚Äá6.71it/s]"
          }
        },
        "0be397daaa8447eb8cb5090bab3827be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "207f03b76ad4446c854deeda9c31966e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9247b092f3a486cac7084a0da4eb9c5",
            "max": 306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c05319a1cb4e4588859a473f9c71e8bf",
            "value": 306
          }
        },
        "216e16dd0a2e4710aa984942824cd6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b751836603d3447a8085251efbe1d694",
              "IPY_MODEL_685632201e954ce99c7396fe78e935cc",
              "IPY_MODEL_785d39f7e8ae4184bd545ccce20d8cbb"
            ],
            "layout": "IPY_MODEL_e80bfaababd346dfaecfa8be4f37f8e8"
          }
        },
        "281a3a9723dd44edbb93f07792d5e777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb036bb86f314f9a91555c362a765015",
              "IPY_MODEL_207f03b76ad4446c854deeda9c31966e",
              "IPY_MODEL_08750b3a66e84f75984cce56b1a3db83"
            ],
            "layout": "IPY_MODEL_a87fed03256345a49713edaf2acca196"
          }
        },
        "33878e102ca444559d79a175782d7911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "479c9464d9034922baa92992683e224f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685632201e954ce99c7396fe78e935cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73002ecdd6d48eeadb39ebb3eaff44a",
            "max": 153,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05c235ebc6604d12a5946ca5ef95f066",
            "value": 3
          }
        },
        "785d39f7e8ae4184bd545ccce20d8cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1868960b65a455195fa210e0dbf4b07",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33878e102ca444559d79a175782d7911",
            "value": "‚Äá3/153‚Äá[06:37&lt;5:28:39,‚Äá131.46s/it]"
          }
        },
        "7fb19ed10203467c9bac0266da891094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e446dbb2a1548308109c120a97169d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a73002ecdd6d48eeadb39ebb3eaff44a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a87fed03256345a49713edaf2acca196": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b751836603d3447a8085251efbe1d694": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f125abd2a45243c98eaf96bc203224ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7fb19ed10203467c9bac0266da891094",
            "value": "Procesando‚Äápacientes:‚Äá‚Äá‚Äá2%"
          }
        },
        "c05319a1cb4e4588859a473f9c71e8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1868960b65a455195fa210e0dbf4b07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9247b092f3a486cac7084a0da4eb9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80bfaababd346dfaecfa8be4f37f8e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb036bb86f314f9a91555c362a765015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_479c9464d9034922baa92992683e224f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_041c1685cdbb44ffb5d47fba8588f54c",
            "value": "Descargando:‚Äá100%"
          }
        },
        "f125abd2a45243c98eaf96bc203224ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
