{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7db5fa",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# üì¶ INSTALACI√ìN Y CONFIGURACI√ìN COMPLETA\n",
    "# ================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üöÄ CONFIGURANDO ENTORNO COMPLETO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================\n",
    "# PASO 1: INSTALAR DEPENDENCIAS\n",
    "# ================================================\n",
    "print(\"\\n[1/4] Instalando dependencias principales...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'pip', 'install', '-q', \n",
    "    'mne', 'pyedflib', 'numpy', 'pandas', 'scipy', 'tqdm', 'ipywidgets'\n",
    "])\n",
    "print(\"   ‚úÖ Dependencias instaladas\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 2: CONFIGURAR IPYWIDGETS PARA BARRAS\n",
    "# ================================================\n",
    "print(\"\\n[2/4] Configurando ipywidgets para barras de progreso...\")\n",
    "try:\n",
    "    subprocess.check_call([\n",
    "        sys.executable, '-m', 'pip', 'install', '-q', '--upgrade', 'ipywidgets'\n",
    "    ])\n",
    "    print(\"   ‚úÖ ipywidgets actualizado\")\n",
    "    \n",
    "    # Intentar habilitar extensi√≥n\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'jupyter', 'nbextension', \n",
    "            'enable', '--py', 'widgetsnbextension', '--sys-prefix'\n",
    "        ])\n",
    "        print(\"   ‚úÖ Extensi√≥n de Jupyter habilitada\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  Extensi√≥n no habilitada (puede no ser necesario)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Advertencia: {e}\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 3: IMPORTAR LIBRER√çAS\n",
    "# ================================================\n",
    "print(\"\\n[3/4] Importando librer√≠as...\")\n",
    "\n",
    "# Imports principales\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Configurar salida para Jupyter\n",
    "sys.stdout.flush()\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Silenciar warnings molestos\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(\"   ‚úÖ Librer√≠as importadas\")\n",
    "\n",
    "# ================================================\n",
    "# PASO 4: CONFIGURAR TQDM\n",
    "# ================================================\n",
    "print(\"\\n[4/4] Configurando barras de progreso...\")\n",
    "\n",
    "# Intentar usar tqdm.notebook (barras visuales)\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    TQDM_DISPONIBLE = \"notebook\"\n",
    "    print(\"   ‚úÖ tqdm.notebook disponible (barras visuales)\")\n",
    "    \n",
    "    # Hacer prueba r√°pida\n",
    "    print(\"\\n   üß™ Probando barra de progreso:\")\n",
    "    for _ in tqdm(range(3), desc=\"   Test\", leave=False):\n",
    "        time.sleep(0.3)\n",
    "    print(\"   ‚úÖ Barras funcionando correctamente!\")\n",
    "    \n",
    "except:\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        TQDM_DISPONIBLE = \"texto\"\n",
    "        print(\"   ‚úÖ tqdm disponible (barras de texto)\")\n",
    "    except:\n",
    "        TQDM_DISPONIBLE = \"manual\"\n",
    "        print(\"   ‚ö†Ô∏è  tqdm no disponible, se usar√°n barras manuales\")\n",
    "        \n",
    "        # Crear clase tqdm manual como fallback\n",
    "        class tqdm:\n",
    "            def __init__(self, iterable=None, total=None, desc=\"\", leave=True, **kwargs):\n",
    "                self.iterable = iterable if iterable is not None else range(total)\n",
    "                self.total = total or (len(iterable) if iterable else 0)\n",
    "                self.desc = desc\n",
    "                self.n = 0\n",
    "                self.start = time.time()\n",
    "                self.leave = leave\n",
    "            \n",
    "            def __iter__(self):\n",
    "                for item in self.iterable:\n",
    "                    yield item\n",
    "                    self.update(1)\n",
    "                if self.leave:\n",
    "                    self.close()\n",
    "            \n",
    "            def update(self, n=1):\n",
    "                self.n += n\n",
    "                elapsed = time.time() - self.start\n",
    "                pct = (self.n / self.total * 100) if self.total > 0 else 0\n",
    "                rate = self.n / elapsed if elapsed > 0 else 0\n",
    "                eta = (self.total - self.n) / rate if rate > 0 else 0\n",
    "                \n",
    "                bar_len = 40\n",
    "                filled = int(bar_len * self.n / self.total) if self.total > 0 else 0\n",
    "                bar = '‚ñà' * filled + '‚ñë' * (bar_len - filled)\n",
    "                \n",
    "                sys.stdout.write(\n",
    "                    f'\\r{self.desc}: |{bar}| {self.n}/{self.total} '\n",
    "                    f'[{pct:.1f}%] [{elapsed:.0f}s<{eta:.0f}s, {rate:.2f}it/s]'\n",
    "                )\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            def close(self):\n",
    "                sys.stdout.write('\\n')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "# ================================================\n",
    "# RESUMEN DE CONFIGURACI√ìN\n",
    "# ================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CONFIGURACI√ìN COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìã Informaci√≥n del sistema:\")\n",
    "print(f\"   ‚Ä¢ Python:      {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ MNE:         {mne.__version__}\")\n",
    "print(f\"   ‚Ä¢ pandas:      {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ numpy:       {np.__version__}\")\n",
    "print(f\"   ‚Ä¢ Cores:       {mp.cpu_count()}\")\n",
    "print(f\"   ‚Ä¢ Tqdm:        {TQDM_DISPONIBLE}\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef339a",
   "metadata": {},
   "source": [
    "## Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# === CONFIGURACI√ìN ===\n",
    "RAW_DIR = Path(r\"C:\\Users\\Mart√≠n\\Desktop\\TransporteProyectoIC\\TransporteProyectoIC\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\")\n",
    "OUTPUT_DIR = RAW_DIR / \"ventanas_out\"\n",
    "WINDOWS_DIR = OUTPUT_DIR / \"ventanas_extraidas\"\n",
    "ANALYSIS_DIR = OUTPUT_DIR / \"analisis_canales\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WINDOWS_DIR.mkdir(exist_ok=True)\n",
    "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Par√°metros\n",
    "WINDOW_SIZE = 30.0\n",
    "OVERLAP = 15.0\n",
    "STRIDE = WINDOW_SIZE - OVERLAP\n",
    "N_WORKERS = max(1, int(mp.cpu_count() * 0.75))  # Usa 75% de cores\n",
    "\n",
    "print(f\"üìÅ RAW_DIR:      {RAW_DIR}\")\n",
    "print(f\"üìÅ OUTPUT_DIR:   {OUTPUT_DIR}\")\n",
    "print(f\"‚öôÔ∏è  Ventana={WINDOW_SIZE}s | Overlap={OVERLAP}s\")\n",
    "print(f\"‚ö° Workers={N_WORKERS}/{mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a519f75",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "def key_from_psg(name: str) -> str:\n",
    "    return Path(name).name.split('-')[0][:7]\n",
    "\n",
    "def key_from_hyp(name: str) -> str:\n",
    "    return Path(name).name.split('-')[0][:7]\n",
    "\n",
    "def encontrar_pares(raw_dir: Path):\n",
    "    psgs, hyps = {}, {}\n",
    "    for fp in raw_dir.glob(\"*.edf\"):\n",
    "        nm = fp.name\n",
    "        if not nm.startswith(\"SC\"):\n",
    "            continue\n",
    "        if nm.endswith(\"-PSG.edf\"):\n",
    "            psgs[key_from_psg(nm)] = fp\n",
    "        elif nm.endswith(\"-Hypnogram.edf\"):\n",
    "            hyps[key_from_hyp(nm)] = fp\n",
    "    keys = sorted(set(psgs) & set(hyps))\n",
    "    return [(k, psgs[k], hyps[k]) for k in keys]\n",
    "\n",
    "def leer_hypnograma_mne(hyp_path: Path) -> pd.DataFrame:\n",
    "    ann = mne.read_annotations(str(hyp_path))\n",
    "    etapas = []\n",
    "    for desc, onset, dur in zip(ann.description, ann.onset, ann.duration):\n",
    "        if \"Sleep stage\" in desc:\n",
    "            st = desc.replace(\"Sleep stage\", \"\").strip()\n",
    "            if st in {\"W\", \"1\", \"2\", \"3\", \"4\", \"R\"}:\n",
    "                etapas.append({\n",
    "                    \"inicio\": float(onset),\n",
    "                    \"duracion\": float(dur),\n",
    "                    \"etapa\": st\n",
    "                })\n",
    "    df = pd.DataFrame(etapas)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"Hipnograma vac√≠o: {hyp_path}\")\n",
    "    return df\n",
    "\n",
    "def _to_datetime(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, tuple) and len(x) == 2:\n",
    "        return datetime.fromtimestamp(x[0]) + timedelta(microseconds=x[1])\n",
    "    try:\n",
    "        return pd.to_datetime(x).to_pydatetime()\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def calcular_offset_segundos(psg_path: Path, hyp_path: Path) -> float:\n",
    "    raw_psg = mne.io.read_raw_edf(str(psg_path), preload=False, verbose=False)\n",
    "    raw_hyp = mne.io.read_raw_edf(str(hyp_path), preload=False, verbose=False)\n",
    "    t_psg = _to_datetime(raw_psg.info.get('meas_date'))\n",
    "    t_hyp = _to_datetime(raw_hyp.info.get('meas_date'))\n",
    "    if t_psg is None or t_hyp is None:\n",
    "        return 0.0\n",
    "    return (t_hyp - t_psg).total_seconds()\n",
    "\n",
    "def analizar_canales(psg_path: Path):\n",
    "    raw = mne.io.read_raw_edf(str(psg_path), preload=False, verbose=False)\n",
    "    info = {}\n",
    "    for ch in raw.ch_names:\n",
    "        ch_up = ch.upper()\n",
    "        if 'EEG' in ch_up:\n",
    "            tipo = 'EEG'\n",
    "        elif 'EOG' in ch_up:\n",
    "            tipo = 'EOG'\n",
    "        elif 'EMG' in ch_up:\n",
    "            tipo = 'EMG'\n",
    "        elif 'ECG' in ch_up or 'EKG' in ch_up:\n",
    "            tipo = 'ECG'\n",
    "        elif 'EVENT' in ch_up or 'MARKER' in ch_up:\n",
    "            tipo = 'EVENTO'\n",
    "        else:\n",
    "            tipo = 'OTRO'\n",
    "        \n",
    "        info[ch] = {\n",
    "            'tipo': tipo,\n",
    "            'freq': float(raw.info['sfreq']),\n",
    "            'n_samples': int(raw.n_times),\n",
    "            'duracion': float(raw.times[-1]) if raw.n_times > 0 else 0.0\n",
    "        }\n",
    "    return info\n",
    "\n",
    "print(\"‚úÖ Funciones auxiliares cargadas\")\n",
    "\n",
    "\n",
    "\n",
    "# Funci√≥n para extraer ventanas de un canal espec√≠fico\n",
    "\n",
    "def extraer_ventanas_por_canal(psg_path, hyp_df, canal_nombre, window_size, stride, hyp_offset=0.0):\n",
    "    raw = mne.io.read_raw_edf(str(psg_path), preload=True, verbose=False)\n",
    "    data, _ = raw[canal_nombre, :]\n",
    "    x = data.flatten()\n",
    "    fs = float(raw.info['sfreq'])\n",
    "    \n",
    "    win_samps = int(round(window_size * fs))\n",
    "    stride_samp = int(round(stride * fs))\n",
    "    \n",
    "    if win_samps <= 0 or stride_samp <= 0 or len(x) < win_samps:\n",
    "        return {\n",
    "            'ventanas': np.empty((0, 0)),\n",
    "            'etiquetas': [],\n",
    "            'tiempos_inicio': [],\n",
    "            'freq_muestreo': fs,\n",
    "            'nombre_canal': canal_nombre\n",
    "        }\n",
    "    \n",
    "    starts = hyp_df['inicio'].to_numpy(dtype=float) + hyp_offset\n",
    "    ends = (hyp_df['inicio'] + hyp_df['duracion']).to_numpy(dtype=float) + hyp_offset\n",
    "    intervals = pd.IntervalIndex.from_arrays(starts, ends, closed='left')\n",
    "    \n",
    "    n_vent = 1 + (len(x) - win_samps) // stride_samp\n",
    "    ventanas = np.empty((n_vent, win_samps), dtype=x.dtype)\n",
    "    etiquetas, tiempos_inicio = [], []\n",
    "    keep = np.ones(n_vent, dtype=bool)\n",
    "    \n",
    "    for i in range(n_vent):\n",
    "        s = i * stride_samp\n",
    "        e = s + win_samps\n",
    "        if e > len(x):\n",
    "            keep[i] = False\n",
    "            continue\n",
    "        \n",
    "        ventanas[i, :] = x[s:e]\n",
    "        t_ini = i * stride\n",
    "        t_mid = t_ini + window_size / 2.0\n",
    "        idx = intervals.get_indexer([t_mid])[0]\n",
    "        \n",
    "        if idx == -1:\n",
    "            keep[i] = False\n",
    "        else:\n",
    "            etiquetas.append(hyp_df.iloc[idx]['etapa'])\n",
    "            tiempos_inicio.append(t_ini)\n",
    "    \n",
    "    ventanas = ventanas[keep]\n",
    "    \n",
    "    return {\n",
    "        'ventanas': ventanas,\n",
    "        'etiquetas': etiquetas,\n",
    "        'tiempos_inicio': tiempos_inicio,\n",
    "        'freq_muestreo': fs,\n",
    "        'nombre_canal': canal_nombre\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cb594",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae145e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA: Cargar y visualizar ventanas (.npz o .pkl)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ID2LABEL = {0:\"W\", 1:\"N1\", 2:\"N2\", 3:\"N3\", 4:\"REM\"}\n",
    "\n",
    "def _load_npz(path):\n",
    "    d = np.load(path, allow_pickle=False)\n",
    "    # Estructura esperada de la Celda 6 TURBO (np.savez_compressed):\n",
    "    # X (n_vent, n_samps) float16, y (n_vent) uint8, t (n_vent) float32, fs float32, canal str\n",
    "    out = {\n",
    "        \"ventanas\": d[\"X\"].astype(np.float32, copy=False),   # para graficar m√°s c√≥modo\n",
    "        \"etiquetas\": d[\"y\"].astype(np.uint8, copy=False),\n",
    "        \"tiempos_inicio\": d[\"t\"].astype(np.float32, copy=False),\n",
    "        \"freq_muestreo\": float(d[\"fs\"]),\n",
    "        \"nombre_canal\": str(d[\"canal\"])\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def _load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # {'ventanas', 'etiquetas', 'tiempos_inicio', 'freq_muestreo', 'nombre_canal'}\n",
    "    # Si las etiquetas vienen como strings, convertimos a IDs para homogeneizar\n",
    "    if data and isinstance(data.get(\"etiquetas\", []), list) and data[\"etiquetas\"] and isinstance(data[\"etiquetas\"][0], str):\n",
    "        label_map = {\"W\":0, \"1\":1, \"N1\":1, \"2\":2, \"N2\":2, \"3\":3, \"4\":3, \"N3\":3, \"R\":4, \"REM\":4}\n",
    "        y = np.array([label_map.get(s, 255) for s in data[\"etiquetas\"]], dtype=np.uint8)\n",
    "    else:\n",
    "        y = np.array(data.get(\"etiquetas\", []), dtype=np.uint8)\n",
    "    out = {\n",
    "        \"ventanas\": np.asarray(data.get(\"ventanas\", []), dtype=np.float32),\n",
    "        \"etiquetas\": y,\n",
    "        \"tiempos_inicio\": np.asarray(data.get(\"tiempos_inicio\", []), dtype=np.float32),\n",
    "        \"freq_muestreo\": float(data.get(\"freq_muestreo\", 100.0)),\n",
    "        \"nombre_canal\": data.get(\"nombre_canal\", \"CANAL\")\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def cargar_ventanas(paciente: str, canal: str, return_ids: bool=False, mmap_npz: bool=True):\n",
    "    base = WINDOWS_DIR / f\"{paciente}_{canal.replace(' ', '_')}\"\n",
    "    npz_path, pkl_path = base.with_suffix(\".npz\"), base.with_suffix(\".pkl\")\n",
    "\n",
    "    if npz_path.exists():\n",
    "        d = np.load(npz_path, allow_pickle=False, mmap_mode='r' if mmap_npz else None)\n",
    "        X = d[\"X\"].astype(np.float32, copy=False)\n",
    "        y = d[\"y\"].astype(np.uint8,  copy=False)\n",
    "        t = d[\"t\"].astype(np.float32, copy=False)\n",
    "        fs = float(d[\"fs\"]); canal_name = str(d[\"canal\"])\n",
    "        fmt = \".npz\"\n",
    "    elif pkl_path.exists():\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            raw = pickle.load(f)\n",
    "        map_ = {\"W\":0,\"1\":1,\"N1\":1,\"2\":2,\"N2\":2,\"3\":3,\"4\":3,\"N3\":3,\"R\":4,\"REM\":4}\n",
    "        y = np.array([map_.get(s,255) for s in raw[\"etiquetas\"]], dtype=np.uint8) \\\n",
    "            if raw.get(\"etiquetas\") and isinstance(raw[\"etiquetas\"][0], str) else np.asarray(raw[\"etiquetas\"], np.uint8)\n",
    "        X = np.asarray(raw[\"ventanas\"], dtype=np.float32)\n",
    "        t = np.asarray(raw[\"tiempos_inicio\"], dtype=np.float32)\n",
    "        fs = float(raw.get(\"freq_muestreo\", 100.0)); canal_name = raw.get(\"nombre_canal\",\"CANAL\")\n",
    "        fmt = \".pkl\"\n",
    "    else:\n",
    "        print(f\"‚ùå No se encontr√≥ ni {npz_path.name} ni {pkl_path.name}\")\n",
    "        return None\n",
    "\n",
    "    ID2LABEL = {0:\"W\",1:\"N1\",2:\"N2\",3:\"N3\",4:\"REM\"}\n",
    "    n = X.shape[0]; total = max(1, n)\n",
    "    if return_ids:\n",
    "        y_out = y\n",
    "        dist_keys = [ID2LABEL.get(int(v), f\"id{int(v)}\") for v in y]\n",
    "    else:\n",
    "        y_out = np.array([ID2LABEL.get(int(v), f\"id{int(v)}\") for v in y], dtype=object)\n",
    "        dist_keys = y_out\n",
    "\n",
    "    from collections import Counter\n",
    "    dist = Counter(dist_keys)\n",
    "    print(f\"‚úÖ Cargado ({fmt}): {paciente} - {canal_name}\")\n",
    "    print(f\"   ‚Ä¢ Ventanas: {X.shape} | Fs: {fs} Hz\")\n",
    "    print(f\"   ‚Ä¢ Distribuci√≥n:\")\n",
    "    for k,c in sorted(dist.items()):\n",
    "        print(f\"     {k}: {c} ({100.0*c/total:.1f}%)\")\n",
    "\n",
    "    return {\"ventanas\": X, \"etiquetas\": y_out, \"tiempos_inicio\": t,\n",
    "            \"freq_muestreo\": fs, \"nombre_canal\": canal_name}\n",
    "\n",
    "\n",
    "def visualizar_ventanas(data: dict, indices=[0, 1, 2], figsize=(15, 8)):\n",
    "    \"\"\"\n",
    "    Visualiza ventanas espec√≠ficas de un dataset cargado con cargar_ventanas().\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(\"‚ö†Ô∏è 'data' es None\")\n",
    "        return\n",
    "\n",
    "    X = data['ventanas']\n",
    "    etiquetas = data['etiquetas']\n",
    "    fs = float(data['freq_muestreo'])\n",
    "\n",
    "    if X is None or len(X) == 0:\n",
    "        print(\"‚ö†Ô∏è No hay ventanas para mostrar.\")\n",
    "        return\n",
    "\n",
    "    idx_validos = [i for i in indices if 0 <= i < len(X)]\n",
    "    if not idx_validos:\n",
    "        print(\"‚ö†Ô∏è √çndices fuera de rango.\")\n",
    "        return\n",
    "\n",
    "    n_plots = len(idx_validos)\n",
    "    fig, axes = plt.subplots(n_plots, 1, figsize=figsize)\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, idx in zip(axes, idx_validos):\n",
    "        ventana = X[idx]\n",
    "        t = np.arange(len(ventana)) / fs\n",
    "        ax.plot(t, ventana, linewidth=0.8)\n",
    "        ax.set_title(f\"Ventana {idx} ‚Äî Etapa: {etiquetas[idx]}\", fontweight='bold')\n",
    "        ax.set_xlabel(\"Tiempo (s)\")\n",
    "        ax.set_ylabel(\"Amplitud\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(0, t[-1] if len(t) else 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Funciones de carga/visualizaci√≥n listas (compatibles con .npz y .pkl)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "# Cargar resumen global\n",
    "df = pd.read_csv(ANALYSIS_DIR / \"resumen_global.csv\")\n",
    "\n",
    "pares = (\n",
    "    df[[\"Paciente\", \"Canal\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "conteo_global = Counter()\n",
    "\n",
    "def extraer_etiquetas(data):\n",
    "    \"\"\"Devuelve una lista 1D de etiquetas como str, sin or booleano ni prints.\"\"\"\n",
    "    # (X, y)\n",
    "    if isinstance(data, (list, tuple)) and len(data) == 2:\n",
    "        y = data[1]\n",
    "    # dict con claves comunes\n",
    "    elif isinstance(data, dict):\n",
    "        y = None\n",
    "        for k in (\"etiquetas\", \"labels\", \"y\", \"stage\", \"stages\", \"etapa\", \"etapas\"):\n",
    "            if k in data:\n",
    "                y = data[k]\n",
    "                break\n",
    "        if y is None:\n",
    "            return None\n",
    "    # DataFrame\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        y = None\n",
    "        for k in (\"etiquetas\", \"labels\", \"y\", \"stage\", \"stages\", \"etapa\", \"etapas\"):\n",
    "            if k in data.columns:\n",
    "                y = data[k].values\n",
    "                break\n",
    "        if y is None:\n",
    "            # fallback: √∫ltima columna\n",
    "            y = data.iloc[:, -1].values\n",
    "    # Serie\n",
    "    elif isinstance(data, pd.Series):\n",
    "        y = data.values\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    y = np.asarray(y).ravel()   # asegurar 1D\n",
    "    y = [str(e) for e in y]     # a texto\n",
    "    return y\n",
    "\n",
    "# Recorrer todos los pacientes/canales \n",
    "for _, row in pares.iterrows():\n",
    "    paciente = row[\"Paciente\"]\n",
    "    canal = row[\"Canal\"]\n",
    "\n",
    "    # Silenciar prints internos de cargar_ventanas (si los hubiera)\n",
    "    sink = io.StringIO()\n",
    "    try:\n",
    "        with redirect_stdout(sink):\n",
    "            data = cargar_ventanas(paciente, canal)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    etiquetas = extraer_etiquetas(data)\n",
    "    if not etiquetas:\n",
    "        continue\n",
    "\n",
    "    conteo_global.update(etiquetas)\n",
    "\n",
    "# C√°lculo y √∫nico print final\n",
    "total = sum(conteo_global.values())\n",
    "if total == 0:\n",
    "    print(\"No se encontraron etiquetas para calcular proporciones.\")\n",
    "else:\n",
    "    # Orden est√°ndar de sue√±o primero, luego cualquier extra que aparezca\n",
    "    orden_std = [\"W\", \"N1\", \"N2\", \"N3\", \"REM\"]\n",
    "    extras = sorted([e for e in conteo_global.keys() if e not in orden_std])\n",
    "    orden_final = orden_std + extras\n",
    "\n",
    "    # √önico output:\n",
    "    print(\" | \".join([f\"{etapa}: {conteo_global.get(etapa, 0) / total * 100:.2f}%\" for etapa in orden_final]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40de0aa",
   "metadata": {},
   "source": [
    "## Exportar a otros formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ec171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA: Exportar a NumPy (versi√≥n en memoria)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "LABEL2ID = {\"W\":0, \"N1\":1, \"N2\":2, \"N3\":3, \"REM\":4}\n",
    "ID2LABEL = {v:k for k,v in LABEL2ID.items()}\n",
    "\n",
    "def exportar_a_numpy_mem(paciente: str, canal: str, dtype=\"float32\"):\n",
    "    \"\"\"\n",
    "    Exporta un (paciente, canal) a arrays NumPy en memoria.\n",
    "    Retorna (X, y, meta) sin guardar a disco.\n",
    "    \"\"\"\n",
    "    data = cargar_ventanas(paciente, canal)\n",
    "    if data is None:\n",
    "        print(f\"‚ö†Ô∏è No se pudo cargar {paciente}-{canal}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Ventanas y etiquetas\n",
    "    X = np.asarray(data[\"ventanas\"], dtype=np.float32)\n",
    "    if dtype == \"float16\":\n",
    "        X = X.astype(np.float16, copy=False)\n",
    "\n",
    "    y_in = data[\"etiquetas\"]\n",
    "    if np.issubdtype(np.array(y_in).dtype, np.integer):\n",
    "        y = np.array(y_in, dtype=np.uint8)\n",
    "    else:\n",
    "        y = np.array([LABEL2ID.get(str(lbl), 255) for lbl in y_in], dtype=np.uint8)\n",
    "\n",
    "    meta = {\n",
    "        \"fs\": float(data[\"freq_muestreo\"]),\n",
    "        \"canal\": data[\"nombre_canal\"],\n",
    "        \"paciente\": paciente,\n",
    "        \"shape\": tuple(X.shape),\n",
    "        \"dtype\": str(X.dtype),\n",
    "        \"label_map\": LABEL2ID\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Exportado en memoria: {paciente}-{canal}\")\n",
    "    print(f\"   ‚Ä¢ X shape: {X.shape} ({X.dtype})\")\n",
    "    print(f\"   ‚Ä¢ y √∫nicos: {sorted(np.unique(y))}\")\n",
    "    return X, y, meta\n",
    "\n",
    "# ===================== Ejemplo de uso =====================\n",
    "resumen_csv = ANALYSIS_DIR / \"resumen_global.csv\"\n",
    "df = pd.read_csv(resumen_csv)\n",
    "\n",
    "eeg_row = df[df[\"Canal\"].str.contains(\"EEG\", case=False)].iloc[0]\n",
    "X_mem, y_mem, meta_mem = exportar_a_numpy_mem(eeg_row[\"Paciente\"], eeg_row[\"Canal\"], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb553d",
   "metadata": {},
   "source": [
    "## Verificaci√≥n de integridad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01042925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA: Verificaci√≥n de integridad de archivos\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def verificar_integridad(window_dir: Path = WINDOWS_DIR):\n",
    "    \"\"\"\n",
    "    Verifica que todos los archivos de ventanas (.npz o .pkl)\n",
    "    sean v√°lidos y consistentes.\n",
    "    \"\"\"\n",
    "    print(\"üîç VERIFICACI√ìN DE INTEGRIDAD DE ARCHIVOS\\n\")\n",
    "\n",
    "    archivos_npz = list(window_dir.glob(\"*.npz\"))\n",
    "    archivos_pkl = list(window_dir.glob(\"*.pkl\"))\n",
    "    total_archivos = len(archivos_npz) + len(archivos_pkl)\n",
    "    print(f\"üìÅ Directorio: {window_dir}\")\n",
    "    print(f\"   ‚Ä¢ .npz encontrados: {len(archivos_npz)}\")\n",
    "    print(f\"   ‚Ä¢ .pkl encontrados: {len(archivos_pkl)}\")\n",
    "    print(f\"   ‚Ä¢ Total archivos:  {total_archivos}\\n\")\n",
    "\n",
    "    errores = []\n",
    "    validos = 0\n",
    "    total_ventanas = 0\n",
    "\n",
    "    for archivo in archivos_npz + archivos_pkl:\n",
    "        try:\n",
    "            if archivo.suffix == \".npz\":\n",
    "                data = np.load(archivo, allow_pickle=False)\n",
    "                # verificar claves esperadas\n",
    "                for k in [\"X\", \"y\", \"t\", \"fs\", \"canal\"]:\n",
    "                    assert k in data.keys(), f\"Falta clave '{k}'\"\n",
    "                n_ventanas = data[\"X\"].shape[0]\n",
    "                assert n_ventanas == len(data[\"y\"]) == len(data[\"t\"]), \"Longitudes inconsistentes\"\n",
    "                assert np.isfinite(data[\"X\"]).all(), \"Hay NaNs en X\"\n",
    "            else:  # .pkl\n",
    "                with open(archivo, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                for k in [\"ventanas\", \"etiquetas\", \"tiempos_inicio\", \"freq_muestreo\"]:\n",
    "                    assert k in data, f\"Falta clave '{k}'\"\n",
    "                n_ventanas = len(data[\"ventanas\"])\n",
    "                assert n_ventanas == len(data[\"etiquetas\"]) == len(data[\"tiempos_inicio\"]), \"Longitudes inconsistentes\"\n",
    "                assert np.isfinite(np.asarray(data[\"ventanas\"])).all(), \"Hay NaNs en ventanas\"\n",
    "\n",
    "            total_ventanas += n_ventanas\n",
    "            validos += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            errores.append((archivo.name, str(e)))\n",
    "\n",
    "    # ====== Reporte final ======\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úÖ Archivos v√°lidos: {validos}/{total_archivos}\")\n",
    "    print(f\"üìä Total de ventanas revisadas: {total_ventanas:,}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if errores:\n",
    "        print(f\"\\n‚ö†Ô∏è  Se detectaron {len(errores)} errores. Primeros 5:\")\n",
    "        for nombre, err in errores[:5]:\n",
    "            print(f\"   ‚Ä¢ {nombre}: {err}\")\n",
    "        if len(errores) > 5:\n",
    "            print(f\"   ... y {len(errores) - 5} m√°s.\\n\")\n",
    "    else:\n",
    "        print(\"\\n‚ú® ¬°Todos los archivos son v√°lidos y consistentes!\\n\")\n",
    "\n",
    "    return validos, errores\n",
    "\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "validos, errores = verificar_integridad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31fb3",
   "metadata": {},
   "source": [
    "## Resumen y limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76801188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA FINAL: RESUMEN COMPLETO + (opc) LIMPIEZA\n",
    "# ================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def human(nbytes):\n",
    "    for unit in ['B','KB','MB','GB','TB']:\n",
    "        if nbytes < 1024 or unit == 'TB':\n",
    "            return f\"{nbytes:.2f} {unit}\"\n",
    "        nbytes /= 1024\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìã RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rutas base conocidas\n",
    "resumen_path = ANALYSIS_DIR / \"resumen_global.csv\"\n",
    "windows_dir  = WINDOWS_DIR\n",
    "datasets_dir = Path.cwd() / \"datasets_cnn\"\n",
    "numpy_dir    = OUTPUT_DIR / \"numpy_exports\"\n",
    "\n",
    "# ---------- Cargar resumen_global ----------\n",
    "if not resumen_path.exists():\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ {resumen_path}\")\n",
    "df = pd.read_csv(resumen_path)\n",
    "\n",
    "# ---------- Contar y pesar ventanas (.npz/.pkl) ----------\n",
    "archivos_pkl = list(windows_dir.glob(\"*.pkl\"))\n",
    "archivos_npz = list(windows_dir.glob(\"*.npz\"))\n",
    "total_archivos = len(archivos_pkl) + len(archivos_npz)\n",
    "size_windows = sum(f.stat().st_size for f in archivos_pkl + archivos_npz)\n",
    "\n",
    "# ---------- (Opcional) pesar datasets derivados ----------\n",
    "size_datasets = sum(f.stat().st_size for f in datasets_dir.rglob(\"*\") if f.is_file()) if datasets_dir.exists() else 0\n",
    "size_numpy    = sum(f.stat().st_size for f in numpy_dir.rglob(\"*\")    if f.is_file()) if numpy_dir.exists()    else 0\n",
    "\n",
    "print(f\"\\n‚úÖ Procesamiento completado\")\n",
    "print(f\"\\nüìÅ Estructura de salida:\")\n",
    "print(f\"   {OUTPUT_DIR}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ ventanas_extraidas/  ({total_archivos} archivos: {len(archivos_pkl)} .pkl, {len(archivos_npz)} .npz)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ analisis_canales/ -> resumen_global.csv\")\n",
    "if datasets_dir.exists():\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ datasets_cnn/ (existe)\")\n",
    "if numpy_dir.exists():\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ numpy_exports/ (existe)\")\n",
    "\n",
    "# ---------- Estad√≠sticas globales ----------\n",
    "print(f\"\\nüìä Estad√≠sticas globales (resumen_global):\")\n",
    "print(f\"   ‚Ä¢ Pacientes procesados: {df['Paciente'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Canales totales:      {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Ventanas totales:     {df['N_Ventanas'].sum():,}\")\n",
    "\n",
    "if 'Tipo' in df.columns:\n",
    "    tipos = ', '.join(sorted(df['Tipo'].dropna().unique()))\n",
    "else:\n",
    "    def inferir_tipo(c):\n",
    "        u = str(c).upper()\n",
    "        if 'EEG' in u: return 'EEG'\n",
    "        if 'EOG' in u: return 'EOG'\n",
    "        if 'EMG' in u: return 'EMG'\n",
    "        if 'ECG' in u or 'EKG' in u: return 'ECG'\n",
    "        if 'RESP' in u or 'AIRFLOW' in u: return 'RESP'\n",
    "        return 'OTRO'\n",
    "    tipos = ', '.join(sorted(df['Canal'].apply(inferir_tipo).unique()))\n",
    "print(f\"   ‚Ä¢ Tipos de canales:     {tipos}\")\n",
    "\n",
    "# ---------- Espacio en disco ----------\n",
    "print(f\"\\nüíæ Espacio en disco (aprox.):\")\n",
    "print(f\"   ‚Ä¢ ventanas_extraidas: {human(size_windows)}\")\n",
    "if datasets_dir.exists():\n",
    "    print(f\"   ‚Ä¢ datasets_cnn:       {human(size_datasets)}\")\n",
    "if numpy_dir.exists():\n",
    "    print(f\"   ‚Ä¢ numpy_exports:      {human(size_numpy)}\")\n",
    "total_all = size_windows + size_datasets + size_numpy\n",
    "print(f\"   ‚Ä¢ TOTAL:              {human(total_all)}\")\n",
    "\n",
    "# ---------- Top archivos m√°s pesados en ventanas_extraidas ----------\n",
    "if total_archivos > 0:\n",
    "    top = sorted(archivos_pkl + archivos_npz, key=lambda p: p.stat().st_size, reverse=True)[:10]\n",
    "    print(f\"\\nüì¶ Top 10 archivos m√°s pesados en ventanas_extraidas:\")\n",
    "    for f in top:\n",
    "        print(f\"   - {f.name:60s} {human(f.stat().st_size)}\")\n",
    "\n",
    "# ---------- (Opcional) limpieza segura ----------\n",
    "DELETE_TEMP = False   # ‚¨ÖÔ∏è cambia a True si quieres borrar cach√©s temporales\n",
    "TEMP_FOLDERS = [\n",
    "    Path.cwd() / \"stft_cache\",\n",
    "    Path.cwd() / \"stft_cache_stream\",\n",
    "]\n",
    "if DELETE_TEMP:\n",
    "    print(\"\\nüßπ Eliminando cach√©s temporales...\")\n",
    "    for d in TEMP_FOLDERS:\n",
    "        if d.exists():\n",
    "            for f in d.rglob(\"*\"):\n",
    "                try:\n",
    "                    if f.is_file(): f.unlink()\n",
    "                except Exception: pass\n",
    "            try:\n",
    "                for sub in sorted(d.glob(\"**/*\"), reverse=True):\n",
    "                    if sub.is_dir(): sub.rmdir()\n",
    "                d.rmdir()\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(\"   ‚úÖ Limpieza completada\")\n",
    "\n",
    "print(\"\\n‚ú® Listo. Puedes decidir qu√© conservar o limpiar en base a este resumen.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311badf",
   "metadata": {},
   "source": [
    "# Creaci√≥n de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PIPELINE STREAMING ROBUSTO \n",
    "# - Cachea STFT por paciente/canal en .npy (float16)\n",
    "# - √çndice liviano (index.json)\n",
    "# - Ensamble con opci√≥n de GUARDA EN SHARDS para archivos grandes\n",
    "# - Expone x1..x5, y1..y5, meta1..meta5 en memoria si as√≠ se desea\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, pickle, shutil, math, gc\n",
    "from pathlib import Path\n",
    "from scipy.signal import stft, get_window, resample\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========= PAR√ÅMETROS DE CONTROL =========\n",
    "BUILD_WHICH = [1,2,3,4,5]     # qu√© datasets construir (1..5)\n",
    "SAVE_DATASETS = False          # True: guarda a disco; False: solo variables en RAM\n",
    "SAVE_FORMAT = \"npz\"           # \"npz\" (comprimido) o \"npy\"\n",
    "SHARD_MAX_BYTES = 1_200_000_000  # ~1.2GB por shard para evitar OSError en Windows\n",
    "OUT_DTYPE = \"float32\"         # dtype final del dataset ensamblado\n",
    "LIMIT_PATIENTS = None         # ej. 20 para pruebas; None = todos\n",
    "\n",
    "# ========= CONFIG BASE =========\n",
    "LABEL2ID = {\"W\":0, \"N1\":1, \"N2\":2, \"N3\":3, \"REM\":4}\n",
    "ID2LABEL = {v:k for k,v in LABEL2ID.items()}\n",
    "\n",
    "CHANNEL_PATTERNS = {\n",
    "    \"EEG1\": [\"EEG Fpz-Cz\", \"Fpz-Cz\"],\n",
    "    \"EEG2\": [\"EEG Pz-Oz\", \"Pz-Oz\"],\n",
    "    \"EOG\" : [\"EOG\", \"EOG horizontal\", \"EOG horizontal derivation\"],\n",
    "    \"EMG\" : [\"EMG\", \"EMG submental\", \"Submental EMG\"]\n",
    "}\n",
    "CHANNEL_BANDS = {\n",
    "    \"EEG1\": (0.3, 35.0),\n",
    "    \"EEG2\": (0.3, 35.0),\n",
    "    \"EOG\" : (0.1, 15.0),\n",
    "    \"EMG\" : (10.0, 100.0),\n",
    "}\n",
    "\n",
    "# Rejilla com√∫n\n",
    "H_COMMON = 128\n",
    "W_TARGET = 15\n",
    "WIN_SEC  = 30.0\n",
    "SEG_SEC  = 2.0\n",
    "HOP_SEC  = 2.0\n",
    "NPERSEG_FIXED = 256\n",
    "WINDOW_TYPE   = \"hamming\"\n",
    "\n",
    "# Rutas de tu pipeline \n",
    "resumen_csv = ANALYSIS_DIR / \"resumen_global.csv\"\n",
    "cache_dir   = Path(\"stft_cache_stream\")\n",
    "ds_dir      = Path(\"datasets_cnn\")\n",
    "ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Loaders de ventanas (npz/pkl) ----------\n",
    "def _load_npz(path: Path):\n",
    "    d = np.load(path, allow_pickle=False)\n",
    "    return {\"X\": d[\"X\"], \"y\": d[\"y\"].astype(np.uint8),\n",
    "            \"t\": d[\"t\"].astype(np.float32), \"fs\": float(d[\"fs\"]),\n",
    "            \"canal\": str(d[\"canal\"])}\n",
    "\n",
    "def _load_pkl(path: Path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    if isinstance(data.get(\"etiquetas\", []), list) and data[\"etiquetas\"]:\n",
    "        y = []\n",
    "        for s in data[\"etiquetas\"]:\n",
    "            y.append(int(s) if isinstance(s, (int, np.integer)) else LABEL2ID.get(str(s), 255))\n",
    "        y = np.array(y, dtype=np.uint8)\n",
    "    else:\n",
    "        y = np.array(data.get(\"etiquetas\", []), dtype=np.uint8)\n",
    "    return {\"X\": np.asarray(data[\"ventanas\"], dtype=np.float32),\n",
    "            \"y\": y,\n",
    "            \"t\": np.asarray(data[\"tiempos_inicio\"], dtype=np.float32),\n",
    "            \"fs\": float(data.get(\"freq_muestreo\", 100.0)),\n",
    "            \"canal\": str(data.get(\"nombre_canal\", \"CANAL\"))}\n",
    "\n",
    "def load_channel_file(paciente: str, canal_nombre: str, windows_dir: Path):\n",
    "    base = windows_dir / f\"{paciente}_{canal_nombre.replace(' ', '_')}\"\n",
    "    npz_path, pkl_path = base.with_suffix(\".npz\"), base.with_suffix(\".pkl\")\n",
    "    if npz_path.exists(): return _load_npz(npz_path), \".npz\"\n",
    "    if pkl_path.exists(): return _load_pkl(pkl_path), \".pkl\"\n",
    "    return None, None\n",
    "\n",
    "def pick_channel_name(df_patient: pd.DataFrame, aliases: list[str]) -> str | None:\n",
    "    names = list(df_patient[\"Canal\"].unique())\n",
    "    u_names = [n.upper() for n in names]\n",
    "    for alias in aliases:\n",
    "        alias_u = alias.upper()\n",
    "        for n, u in zip(names, u_names):\n",
    "            if u == alias_u: return n\n",
    "        for n, u in zip(names, u_names):\n",
    "            if alias_u in u: return n\n",
    "    return None\n",
    "\n",
    "# ---------- STFT ‚Üí rejilla com√∫n ----------\n",
    "def stft_to_grid(x, fs, fmin, fmax, H_out=H_COMMON, W_out=W_TARGET):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    expected_len = int(round(WIN_SEC * fs))\n",
    "    if len(x) != expected_len:\n",
    "        x = x[:expected_len] if len(x) > expected_len else np.pad(x, (0, expected_len - len(x)), mode=\"constant\")\n",
    "    nperseg = int(NPERSEG_FIXED)\n",
    "    hop_samps = int(round(HOP_SEC * fs))\n",
    "    noverlap = max(0, nperseg - hop_samps)\n",
    "    nfft = 1\n",
    "    while nfft < nperseg: nfft <<= 1\n",
    "    f, t, Z = stft(x, fs=fs,\n",
    "                   window=get_window(WINDOW_TYPE, nperseg, fftbins=True),\n",
    "                   nperseg=nperseg, noverlap=noverlap, nfft=nfft,\n",
    "                   boundary=None, padded=False, detrend=False, return_onesided=True)\n",
    "    P = np.log10(np.maximum(np.abs(Z)**2, 1e-12)).astype(np.float32)\n",
    "    mask = (f >= fmin) & (f <= fmax)\n",
    "    P_band = P[mask, :]\n",
    "    if P_band.shape[0] != H_out: P_band = resample(P_band, H_out, axis=0)\n",
    "    if P_band.shape[1] != W_out: P_band = resample(P_band, W_out, axis=1)\n",
    "    return P_band\n",
    "\n",
    "# ============================================================\n",
    "# 1) CACHE STREAMING (guarda por paciente/canal)\n",
    "# ============================================================\n",
    "def compute_stft_cache_streaming(\n",
    "    analysis_csv: Path,\n",
    "    windows_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    max_patients: int | None = LIMIT_PATIENTS,\n",
    "    force_recompute: bool = False,\n",
    "    save_dtype: str = \"float16\"\n",
    "):\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    index_path = cache_dir / \"index.json\"\n",
    "\n",
    "    if index_path.exists() and not force_recompute:\n",
    "        with open(index_path, \"r\") as f: return json.load(f)\n",
    "\n",
    "    if force_recompute and cache_dir.exists():\n",
    "        for p in cache_dir.glob(\"*\"):\n",
    "            if p.is_file() and p.name != \"index.json\": p.unlink()\n",
    "            elif p.is_dir(): shutil.rmtree(p)\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(analysis_csv)\n",
    "    patients = list(df[\"Paciente\"].unique())\n",
    "    if max_patients: patients = patients[:max_patients]\n",
    "\n",
    "    index = {k: {} for k in CHANNEL_PATTERNS.keys()}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö° CACHE STREAMING DE STFT POR PACIENTE/CANAL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Procesando {len(patients)} pacientes √ó 4 canales...\")\n",
    "\n",
    "    for p in tqdm(patients, desc=\"üîÑ Pacientes\"):\n",
    "        dpf = df[df[\"Paciente\"] == p]\n",
    "        for ch_key in CHANNEL_PATTERNS.keys():\n",
    "            ch_name = pick_channel_name(dpf, CHANNEL_PATTERNS[ch_key])\n",
    "            if ch_name is None: continue\n",
    "\n",
    "            out_dir = cache_dir / ch_key\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            x_path, y_path, t_path = out_dir / f\"{p}_X.npy\", out_dir / f\"{p}_y.npy\", out_dir / f\"{p}_t.npy\"\n",
    "\n",
    "            if x_path.exists() and y_path.exists() and t_path.exists():\n",
    "                index[ch_key][p] = {\"X\": str(x_path), \"y\": str(y_path), \"t\": str(t_path)}\n",
    "                continue\n",
    "\n",
    "            dfile, _ = load_channel_file(p, ch_name, WINDOWS_DIR)\n",
    "            if dfile is None: continue\n",
    "\n",
    "            fs = dfile[\"fs\"]\n",
    "            fmin, fmax = CHANNEL_BANDS[ch_key]\n",
    "            Xraw, y, t = dfile[\"X\"], dfile[\"y\"], dfile[\"t\"]\n",
    "\n",
    "            n_win = Xraw.shape[0]\n",
    "            X_stft = np.empty((n_win, H_COMMON, W_TARGET), dtype=np.float32)\n",
    "            for i in range(n_win):\n",
    "                X_stft[i] = stft_to_grid(Xraw[i], fs, fmin, fmax)\n",
    "\n",
    "            if save_dtype == \"float16\": X_stft = X_stft.astype(np.float16)\n",
    "\n",
    "            np.save(x_path, X_stft)\n",
    "            np.save(y_path, y.astype(np.uint8))\n",
    "            np.save(t_path, t.astype(np.float32))\n",
    "            index[ch_key][p] = {\"X\": str(x_path), \"y\": str(y_path), \"t\": str(t_path)}\n",
    "\n",
    "            # liberar RAM por paciente\n",
    "            del X_stft, Xraw, y, t\n",
    "            gc.collect()\n",
    "\n",
    "    with open(index_path, \"w\") as f: json.dump(index, f, indent=2)\n",
    "    print(f\"\\n‚úÖ Cach√© listo en {cache_dir} (index.json)\")\n",
    "    return index\n",
    "\n",
    "# ============================================================\n",
    "# Guardado seguro: shards para arrays grandes\n",
    "# ============================================================\n",
    "def _save_array_safely(base_path: Path, X: np.ndarray, y: np.ndarray, meta: dict,\n",
    "                       fmt=\"npz\", shard_max_bytes=SHARD_MAX_BYTES):\n",
    "    base_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    H, W, C = X.shape[1], X.shape[2], X.shape[3]\n",
    "    bytes_per_sample = X.dtype.itemsize * H * W * C + y.dtype.itemsize\n",
    "    n = X.shape[0]\n",
    "    if n == 0:\n",
    "        # guardar vac√≠o\n",
    "        if fmt == \"npz\":\n",
    "            np.savez_compressed(base_path.parent / f\"{base_path.stem}_X.npz\", X=X)\n",
    "            np.savez_compressed(base_path.parent / f\"{base_path.stem}_y.npz\", y=y)\n",
    "        else:\n",
    "            np.save(base_path.parent / f\"{base_path.stem}_X.npy\", X)\n",
    "            np.save(base_path.parent / f\"{base_path.stem}_y.npy\", y)\n",
    "        with open(base_path.parent / f\"{base_path.stem}_meta.pkl\", \"wb\") as f:\n",
    "            pickle.dump(meta, f)\n",
    "        return {\"shards\": []}\n",
    "\n",
    "    samples_per_shard = max(1, shard_max_bytes // bytes_per_sample)\n",
    "    n_shards = math.ceil(n / samples_per_shard)\n",
    "\n",
    "    manifest = {\"shards\": [], \"format\": fmt, \"n\": int(n), \"HWC\": [H,W,C]}\n",
    "    for s in range(n_shards):\n",
    "        a, b = s * samples_per_shard, min(n, (s+1) * samples_per_shard)\n",
    "        Xs, ys = X[a:b], y[a:b]\n",
    "        shard_tag = f\"{base_path.stem}_shard{s:02d}\"\n",
    "        if fmt == \"npz\":\n",
    "            np.savez_compressed(base_path.parent / f\"{shard_tag}.npz\", X=Xs, y=ys)\n",
    "        else:\n",
    "            np.save(base_path.parent / f\"{shard_tag}_X.npy\", Xs)\n",
    "            np.save(base_path.parent / f\"{shard_tag}_y.npy\", ys)\n",
    "        manifest[\"shards\"].append({\"start\": int(a), \"end\": int(b), \"tag\": shard_tag})\n",
    "\n",
    "    with open(base_path.parent / f\"{base_path.stem}_meta.pkl\", \"wb\") as f:\n",
    "        pickle.dump(meta, f)\n",
    "    with open(base_path.parent / f\"{base_path.stem}_manifest.json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    return manifest\n",
    "\n",
    "# ============================================================\n",
    "# 2) ENSAMBLAR DATASET DESDE CACH√â (con opci√≥n shards)\n",
    "# ============================================================\n",
    "def assemble_dataset_from_cache_streaming(\n",
    "    index: dict,\n",
    "    required_keys: list[str],\n",
    "    save_path: Path | None = None,\n",
    "    out_dtype: str = OUT_DTYPE,\n",
    "    save_format: str = SAVE_FORMAT,\n",
    "    shard_max_bytes: int = SHARD_MAX_BYTES\n",
    "):\n",
    "    print(f\"\\nüî® Ensamblando dataset: {required_keys}\")\n",
    "\n",
    "    valid_patients = set(index[required_keys[0]].keys())\n",
    "    for k in required_keys[1:]:\n",
    "        valid_patients &= set(index[k].keys())\n",
    "    valid_patients = sorted(list(valid_patients))\n",
    "    print(f\"   Pacientes v√°lidos: {len(valid_patients)}\")\n",
    "    if not valid_patients:\n",
    "        raise ValueError(\"‚ùå No hay pacientes con todos los canales requeridos\")\n",
    "\n",
    "    X_list, y_list, counts = [], [], []\n",
    "\n",
    "    for p in tqdm(valid_patients, desc=\"Ensamblando\", leave=False):\n",
    "        times_rounded = {}\n",
    "        for k in required_keys:\n",
    "            t = np.load(index[k][p][\"t\"]).astype(np.float32)\n",
    "            times_rounded[k] = np.round(t, 4)\n",
    "\n",
    "        common = set(times_rounded[required_keys[0]])\n",
    "        for k in required_keys[1:]:\n",
    "            common &= set(times_rounded[k])\n",
    "        if not common: continue\n",
    "        common_sorted = np.array(sorted(list(common)), dtype=np.float32)\n",
    "\n",
    "        idx_maps = {}\n",
    "        for k in required_keys:\n",
    "            t = times_rounded[k]\n",
    "            t2idx = {float(tt): i for i, tt in enumerate(t)}\n",
    "            idx_maps[k] = [t2idx[float(tt)] for tt in common_sorted]\n",
    "\n",
    "        patient_specs_list, patient_labels = [], None\n",
    "        for k in required_keys:\n",
    "            X_ch = np.load(index[k][p][\"X\"])\n",
    "            X_aligned = X_ch[idx_maps[k]]  # (n, H, W)\n",
    "            patient_specs_list.append(X_aligned[..., None])  # (n,H,W,1)\n",
    "            if patient_labels is None:\n",
    "                y_ch = np.load(index[k][p][\"y\"])\n",
    "                patient_labels = y_ch[idx_maps[k]]\n",
    "\n",
    "        patient_specs = np.concatenate(patient_specs_list, axis=-1)  # (n,H,W,C)\n",
    "        if out_dtype == \"float32\" and patient_specs.dtype != np.float32:\n",
    "            patient_specs = patient_specs.astype(np.float32)\n",
    "\n",
    "        X_list.append(patient_specs)\n",
    "        y_list.append(patient_labels)\n",
    "        counts.append((p, int(patient_specs.shape[0])))\n",
    "\n",
    "        # liberar por paciente\n",
    "        del patient_specs_list, patient_labels, X_ch, X_aligned, y_ch\n",
    "        gc.collect()\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0) if X_list else np.empty((0,H_COMMON,W_TARGET,len(required_keys)), dtype=np.float32)\n",
    "    y = np.concatenate(y_list, axis=0) if y_list else np.empty((0,), dtype=np.uint8)\n",
    "    del X_list, y_list; gc.collect()\n",
    "\n",
    "    meta = {\n",
    "        \"shape\": tuple(X.shape),\n",
    "        \"labels_unique\": sorted(list(map(int, np.unique(y)))) if y.size else [],\n",
    "        \"label_map\": ID2LABEL,\n",
    "        \"counts_per_patient\": counts,\n",
    "        \"channels_used\": {k: CHANNEL_PATTERNS[k] for k in required_keys},\n",
    "        \"channel_bands\": {k: CHANNEL_BANDS[k] for k in required_keys},\n",
    "        \"grid\": {\"H\": H_COMMON, \"W\": W_TARGET}\n",
    "    }\n",
    "\n",
    "    if save_path and SAVE_DATASETS:\n",
    "        print(\"   üíæ Guardando con shards seguros...\")\n",
    "        _ = _save_array_safely(save_path, X, y, meta, fmt=save_format, shard_max_bytes=shard_max_bytes)\n",
    "        print(f\"   ‚úÖ Guardado en {save_path.parent}\")\n",
    "\n",
    "    return X, y, meta\n",
    "\n",
    "# ============================================================\n",
    "# 3) EJECUCI√ìN: CREA x1..x5, y1..y5, meta1..meta5\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ CREACI√ìN STREAMING DE 5 DATASETS (ROBUSTA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìç PASO 1/2: Cachear STFT por paciente/canal\")\n",
    "index = compute_stft_cache_streaming(\n",
    "    analysis_csv=resumen_csv,\n",
    "    windows_dir=WINDOWS_DIR,\n",
    "    cache_dir=cache_dir,\n",
    "    max_patients=LIMIT_PATIENTS,\n",
    "    force_recompute=False,\n",
    "    save_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìç PASO 2/2: Ensamblar datasets desde el cach√©\")\n",
    "# Definici√≥n de los 5\n",
    "recipes = {\n",
    "    1: ([\"EEG1\",\"EEG2\",\"EOG\",\"EMG\"], \"ds1_4ch\"),\n",
    "    2: ([\"EEG1\",\"EEG2\",\"EOG\"],       \"ds2_eeg_eog\"),\n",
    "    3: ([\"EEG1\"],                    \"ds3_eeg1\"),\n",
    "    4: ([\"EEG1\",\"EEG2\",\"EMG\"],       \"ds4_eeg_emg\"),\n",
    "    5: ([\"EOG\",\"EMG\"],               \"ds5_eog_emg\"),\n",
    "}\n",
    "\n",
    "# Helpers para exponer variables x1..x5\n",
    "globals_map = {}\n",
    "for i in [1,2,3,4,5]:\n",
    "    if i not in BUILD_WHICH: \n",
    "        globals()[f\"x{i}\"] = None; globals()[f\"y{i}\"] = None; globals()[f\"meta{i}\"] = None\n",
    "        continue\n",
    "    keys, fname = recipes[i]\n",
    "    save_path = (ds_dir / fname) if SAVE_DATASETS else None\n",
    "    Xi, Yi, Metai = assemble_dataset_from_cache_streaming(\n",
    "        index, keys, save_path=save_path, out_dtype=OUT_DTYPE,\n",
    "        save_format=SAVE_FORMAT, shard_max_bytes=SHARD_MAX_BYTES\n",
    "    )\n",
    "    globals()[f\"x{i}\"] = Xi\n",
    "    globals()[f\"y{i}\"] = Yi\n",
    "    globals()[f\"meta{i}\"] = Metai\n",
    "    print(f\"   ‚¨ÜÔ∏è Listo dataset {i}: shape={Metai['shape']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESUMEN DE DATASETS CREADOS\")\n",
    "print(\"=\"*70)\n",
    "for i, desc in zip([1,2,3,4,5], [\"EEG1+EEG2+EOG+EMG\",\"EEG1+EEG2+EOG\",\"EEG1 only\",\"EEG1+EEG2+EMG\",\"EOG+EMG\"]):\n",
    "    Mi = globals()[f\"meta{i}\"]\n",
    "    if Mi is None:\n",
    "        print(f\"üóÇÔ∏è  Dataset {i}: {desc} ‚Äî (NO construido)\")\n",
    "        continue\n",
    "    print(f\"\\nüóÇÔ∏è  Dataset {i}: {desc}\")\n",
    "    print(f\"   ‚Ä¢ Shape: {Mi['shape']}\")\n",
    "    print(f\"   ‚Ä¢ Etiquetas: {[ID2LABEL[j] for j in Mi['labels_unique']]}\")\n",
    "    print(f\"   ‚Ä¢ Pacientes: {len(Mi['counts_per_patient'])}\")\n",
    "\n",
    "print(\"\\n‚úÖ Variables disponibles: x1..x5, y1..y5, meta1..meta5\")\n",
    "print(f\"üíæ Guardado en shards: {'S√≠' if SAVE_DATASETS else 'No (solo RAM)'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9da71",
   "metadata": {},
   "source": [
    "## Separaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SPLIT POR PACIENTE 60/20/20 (robusto a memoria/disco)\n",
    "# - Si x*, y*, meta* ya existen, los usa.\n",
    "# - Si no, carga datasets desde datasets_cnn/.\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "DS_DIR = Path(\"datasets_cnn\")\n",
    "\n",
    "def _load_ds_from_disk(tag: str):\n",
    "    \"\"\"Carga X, y, meta de disco: tag='ds1_4ch', 'ds2_eeg_eog', ...\"\"\"\n",
    "    x_path = DS_DIR / f\"{tag}_X.npy\"\n",
    "    y_path = DS_DIR / f\"{tag}_y.npy\"\n",
    "    m_path = DS_DIR / f\"{tag}_meta.pkl\"\n",
    "    if not x_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {x_path}\")\n",
    "    if not y_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {y_path}\")\n",
    "    if not m_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe {m_path}\")\n",
    "    X = np.load(x_path, mmap_mode=None)     \n",
    "    y = np.load(y_path)\n",
    "    with open(m_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    return X, y, meta\n",
    "\n",
    "def _ensure_loaded(var_triplet, fallback_tag):\n",
    "    \"\"\"Si (X,y,meta) no est√°n en memoria, los carga desde disco.\"\"\"\n",
    "    X, y, meta = var_triplet\n",
    "    if \"X\" not in locals() and \"y\" not in locals():  # no sirve dentro de funci√≥n\n",
    "        pass\n",
    "    if X is None or y is None or meta is None:\n",
    "        return _load_ds_from_disk(fallback_tag)\n",
    "    return X, y, meta\n",
    "\n",
    "# Intenta usar variables en memoria; si no existen, carga de disco.\n",
    "try:\n",
    "    x1, y1, meta1\n",
    "except NameError:\n",
    "    x1 = y1 = meta1 = None\n",
    "try:\n",
    "    x2, y2, meta2\n",
    "except NameError:\n",
    "    x2 = y2 = meta2 = None\n",
    "try:\n",
    "    x3, y3, meta3\n",
    "except NameError:\n",
    "    x3 = y3 = meta3 = None\n",
    "try:\n",
    "    x4, y4, meta4\n",
    "except NameError:\n",
    "    x4 = y4 = meta4 = None\n",
    "try:\n",
    "    x5, y5, meta5\n",
    "except NameError:\n",
    "    x5 = y5 = meta5 = None\n",
    "\n",
    "x1, y1, meta1 = _ensure_loaded((x1,y1,meta1), \"ds1_4ch\")\n",
    "x2, y2, meta2 = _ensure_loaded((x2,y2,meta2), \"ds2_eeg_eog\")\n",
    "x3, y3, meta3 = _ensure_loaded((x3,y3,meta3), \"ds3_eeg1\")\n",
    "x4, y4, meta4 = _ensure_loaded((x4,y4,meta4), \"ds4_eeg_emg\")\n",
    "x5, y5, meta5 = _ensure_loaded((x5,y5,meta5), \"ds5_eog_emg\")\n",
    "\n",
    "def make_patient_ids(meta):\n",
    "    \"\"\"Construye vector (N,) de IDs de paciente a partir de meta['counts_per_patient'].\"\"\"\n",
    "    ids = []\n",
    "    for patient, n in meta[\"counts_per_patient\"]:\n",
    "        ids.extend([patient] * int(n))\n",
    "    return np.array(ids, dtype=object)  # object para mantener strings completos\n",
    "\n",
    "def split_by_patient(y, patient_ids, test_size=0.20, val_size=0.20, random_state=42):\n",
    "    \"\"\"Devuelve dict con √≠ndices 'train', 'val', 'test' (sin fuga entre pacientes).\"\"\"\n",
    "    N = len(y)\n",
    "    assert len(patient_ids) == N, \"Desalineaci√≥n patient_ids vs y\"\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_val_idx, test_idx = next(gss1.split(np.zeros(N), y, groups=patient_ids))\n",
    "\n",
    "    rel_val = val_size / (1.0 - test_size)  # ej. 0.20/0.80=0.25\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=rel_val, random_state=random_state + 1)\n",
    "    pv = patient_ids[train_val_idx]\n",
    "    yv = y[train_val_idx]\n",
    "    sub_train_idx, val_idx_sub = next(gss2.split(np.zeros(len(train_val_idx)), yv, groups=pv))\n",
    "\n",
    "    train_idx = train_val_idx[sub_train_idx]\n",
    "    val_idx   = train_val_idx[val_idx_sub]\n",
    "\n",
    "    # sanity checks\n",
    "    assert set(train_idx).isdisjoint(test_idx) and set(val_idx).isdisjoint(test_idx)\n",
    "    assert set(train_idx).isdisjoint(val_idx)\n",
    "\n",
    "    return {\"train\": train_idx, \"val\": val_idx, \"test\": test_idx}\n",
    "\n",
    "def print_split_summary(y, patient_ids, splits, name, label_names={0:\"W\",1:\"N1\",2:\"N2\",3:\"N3\",4:\"REM\"}):\n",
    "    p_train = set(np.unique(patient_ids[splits[\"train\"]]))\n",
    "    p_val   = set(np.unique(patient_ids[splits[\"val\"]]))\n",
    "    p_test  = set(np.unique(patient_ids[splits[\"test\"]]))\n",
    "\n",
    "    print(f\"\\n====== {name}: PACIENTES POR SPLIT ======\")\n",
    "    print(f\"Train: {len(p_train)} | Val: {len(p_val)} | Test: {len(p_test)}\")\n",
    "    print(\"Intersecciones (deben ser 0):\",\n",
    "          len(p_train & p_val), len(p_train & p_test), len(p_val & p_test))\n",
    "\n",
    "    print(\"------ Distribuci√≥n de clases (por ventanas) ------\")\n",
    "    for split_name, idx in splits.items():\n",
    "        yy = y[idx]\n",
    "        uniq, cnt = np.unique(yy, return_counts=True)\n",
    "        total = len(yy)\n",
    "        nice = \", \".join([f\"{label_names.get(int(k),k)}: {int(v)} ({(int(v)/total*100):.1f}%)\"\n",
    "                          for k, v in sorted(zip(uniq, cnt), key=lambda z:int(z[0]))])\n",
    "        print(f\"{split_name:>5} -> N={total} | {nice}\")\n",
    "\n",
    "# Construir IDs de paciente\n",
    "patient_ids1 = make_patient_ids(meta1)\n",
    "patient_ids2 = make_patient_ids(meta2)\n",
    "patient_ids3 = make_patient_ids(meta3)\n",
    "patient_ids4 = make_patient_ids(meta4)\n",
    "patient_ids5 = make_patient_ids(meta5)\n",
    "\n",
    "# Asegurar alineaci√≥n\n",
    "assert len(patient_ids1) == len(y1) == x1.shape[0]\n",
    "assert len(patient_ids2) == len(y2) == x2.shape[0]\n",
    "assert len(patient_ids3) == len(y3) == x3.shape[0]\n",
    "assert len(patient_ids4) == len(y4) == x4.shape[0]\n",
    "assert len(patient_ids5) == len(y5) == x5.shape[0]\n",
    "\n",
    "# Ejecutar splits\n",
    "splits1 = split_by_patient(y1, patient_ids1, 0.20, 0.20, 42)\n",
    "splits2 = split_by_patient(y2, patient_ids2, 0.20, 0.20, 42)\n",
    "splits3 = split_by_patient(y3, patient_ids3, 0.20, 0.20, 42)\n",
    "splits4 = split_by_patient(y4, patient_ids4, 0.20, 0.20, 42)\n",
    "splits5 = split_by_patient(y5, patient_ids5, 0.20, 0.20, 42)\n",
    "\n",
    "# Resumenes\n",
    "print_split_summary(y1, patient_ids1, splits1, \"DS1 EEG1+EEG2+EOG+EMG\")\n",
    "print_split_summary(y2, patient_ids2, splits2, \"DS2 EEG1+EEG2+EOG\")\n",
    "print_split_summary(y3, patient_ids3, splits3, \"DS3 EEG1 only\")\n",
    "print_split_summary(y4, patient_ids4, splits4, \"DS4 EEG1+EEG2+EMG\")\n",
    "print_split_summary(y5, patient_ids5, splits5, \"DS5 EOG+EMG\")\n",
    "\n",
    "\n",
    "# (Opcional) Guardar √≠ndices por dataset para reproducibilidad\n",
    "#SAVE_SPLITS = True\n",
    "#if SAVE_SPLITS:\n",
    "#    sp_dir = DS_DIR / \"splits\"\n",
    "#    sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "#    for k, sp in enumerate([splits1, splits2, splits3, splits4, splits5], start=1):\n",
    "#        np.save(sp_dir / f\"ds{k}_train_idx.npy\", sp[\"train\"].astype(np.uint32))\n",
    "#        np.save(sp_dir / f\"ds{k}_val_idx.npy\",   sp[\"val\"].astype(np.uint32))\n",
    "#        np.save(sp_dir / f\"ds{k}_test_idx.npy\",  sp[\"test\"].astype(np.uint32))\n",
    "#    print(f\"\\nüíæ √çndices guardados en {sp_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e798a",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MApooling2D(nn.Module):\n",
    "    \"\"\"Multi-scale Pooling: concat(MaxPool2d, AvgPool2d) ‚Üí duplica canales.\"\"\"\n",
    "    def __init__(self, kernel_size, stride, padding=1):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max_pool(x), self.avg_pool(x)], dim=1)  # (B, 2*C, H', W')\n",
    "\n",
    "class MCBlock(nn.Module):\n",
    "    \"\"\"Multi-scale Convolutional Block (4 ramas en paralelo). Salida: 240 canales.\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 48, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            MApooling2D(kernel_size=3, stride=1, padding=1),           # duplica canales: in‚Üí2*in\n",
    "            nn.Conv2d(in_channels * 2, 32, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b4 = self.branch4(x)\n",
    "        return torch.cat([b1, b2, b3, b4], dim=1)  # (B, 64+96+48+32=240, H, W)\n",
    "\n",
    "class SleepStageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Entrada esperada por defecto: (B, in_ch, H=121, W=15)\n",
    "    Para tus 4 canales: in_ch=4. Si usas subsets (e.g., EEG1 solo), cambia in_ch.\n",
    "    Arquitectura: Conv ‚Üí MApool ‚Üí MCBlock ‚Üí MApool ‚Üí GAP ‚Üí FC(480‚Üínum_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, in_ch=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv inicial\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 256, kernel_size=3, stride=2, padding=1),  # (B,256, ~61, ~8) con H=121,W=15\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        # MApooling duplica canales: 256 ‚Üí 512\n",
    "        self.m_pool1 = MApooling2D(kernel_size=3, stride=2, padding=1)  # (B,512, ~31, ~4)\n",
    "\n",
    "        # MCBlock: 512 ‚Üí 240 canales\n",
    "        self.mc_block = nn.Sequential(\n",
    "            MCBlock(in_channels=512),\n",
    "            nn.BatchNorm2d(240)\n",
    "        )  # (B,240, ~31, ~4)\n",
    "\n",
    "        # Segundo MApooling: 240 ‚Üí 480 canales\n",
    "        self.m_pool2 = MApooling2D(kernel_size=3, stride=2, padding=1)  # (B,480, ~16, ~2)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Global Average Pooling ‚Üí (B,480,1,1) independiza de H/W exactos\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Clasificador final. in_features = 480 fijo tras m_pool2 + GAP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(480, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in_ch, H, W)  t√≠pico: (B,4,121,15)\n",
    "        x = self.conv1(x)          # (B,256,‚âà61,‚âà8)\n",
    "        x = self.m_pool1(x)        # (B,512,‚âà31,‚âà4)\n",
    "        x = self.mc_block(x)       # (B,240,‚âà31,‚âà4)\n",
    "        x = self.m_pool2(x)        # (B,480,‚âà16,‚âà2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.global_avg_pool(x)  # (B,480,1,1)\n",
    "        x = self.classifier(x)       # (B,num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b8722",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENTRENAMIENTO =====\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# ---------- Dataset a NIVEL M√ìDULO (evita errores de pickle con num_workers>0) ----------\n",
    "if 'SpectroDataset' not in globals():\n",
    "    class SpectroDataset(torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Espera X con shape (N, H, W, C) y y con shape (N,).\n",
    "        Devuelve tensores listos para Conv2d: (C, H, W) y etiqueta long.\n",
    "        \"\"\"\n",
    "        def __init__(self, X, y, indices=None, dtype=np.float32):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            self.indices = np.arange(len(y)) if indices is None else np.asarray(indices)\n",
    "            self.dtype = dtype\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "\n",
    "        def __getitem__(self, i):\n",
    "            idx = int(self.indices[i])\n",
    "            x = np.asarray(self.X[idx], dtype=self.dtype)   # (H, W, C)\n",
    "            x = np.transpose(x, (2, 0, 1))                  # -> (C, H, W)\n",
    "            y_i = int(self.y[idx])\n",
    "            return torch.from_numpy(x), torch.tensor(y_i, dtype=torch.long)\n",
    "\n",
    "# ---------- EarlyStopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode='max', min_delta=0.0):\n",
    "        self.patience = int(patience)\n",
    "        self.mode = mode\n",
    "        self.min_delta = float(min_delta)\n",
    "        self.best = -np.inf if mode == 'max' else np.inf\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, metric):\n",
    "        if metric is None or (isinstance(metric, float) and (math.isnan(metric) or math.isinf(metric))):\n",
    "            self.num_bad_epochs += 1\n",
    "            return self.num_bad_epochs >= self.patience\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            if metric - self.best > self.min_delta:\n",
    "                self.best, self.num_bad_epochs = metric, 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "        else:\n",
    "            if self.best - metric > self.min_delta:\n",
    "                self.best, self.num_bad_epochs = metric, 0\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "        return self.num_bad_epochs >= self.patience\n",
    "\n",
    "# ---------- evaluate ----------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"‚ö†Ô∏è  Loss NaN/Inf detectado en evaluaci√≥n; batch omitido\")\n",
    "            continue\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total_correct += (pred == yb).sum().item()\n",
    "        total_loss += float(loss.item())\n",
    "        total += yb.size(0)\n",
    "    val_loss = total_loss / max(1, len(loader))\n",
    "    val_acc = total_correct / max(1, total)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# ---------- train_one_epoch ----------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, grad_clip=None, scaler=None):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total, nan_count = 0.0, 0, 0, 0\n",
    "    autocast_ctx = (\n",
    "        torch.autocast(device_type='cuda', dtype=torch.float16) if (scaler is not None and device.type == 'cuda')\n",
    "        else torch.cuda.amp.autocast(enabled=False)\n",
    "    )\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with autocast_ctx:\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total_correct += (pred == yb).sum().item()\n",
    "        total_loss += float(loss.item())\n",
    "        total += yb.size(0)\n",
    "\n",
    "    if nan_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  {nan_count} batch(es) con NaN/Inf fueron omitidos\")\n",
    "    train_loss = total_loss / max(1, len(loader))\n",
    "    train_acc = total_correct / max(1, total)\n",
    "    return train_acc, train_loss\n",
    "\n",
    "# ---------- Pesos de clase ----------\n",
    "def compute_class_weights(y_train, method='sqrt_inverse', clip_range=(0.5, 2.5)):\n",
    "    uniq, cnt = np.unique(y_train, return_counts=True)\n",
    "    n_cls = int(np.max(y_train)) + 1\n",
    "    freq = cnt / cnt.sum()\n",
    "    if method == 'inverse':\n",
    "        w = 1.0 / np.maximum(freq, 1e-8)\n",
    "    elif method == 'sqrt_inverse':\n",
    "        w = 1.0 / np.sqrt(np.maximum(freq, 1e-8))\n",
    "    elif method == 'log_inverse':\n",
    "        w = 1.0 / np.log1p(freq * 100)\n",
    "    elif method == 'manual':\n",
    "        return dict(zip(uniq.astype(int), cnt.tolist()))\n",
    "    else:\n",
    "        raise ValueError(\"method debe ser 'inverse', 'sqrt_inverse', 'log_inverse' o 'manual'\")\n",
    "    w = w / w.sum() * len(uniq)\n",
    "    if clip_range is not None:\n",
    "        w = np.clip(w, clip_range[0], clip_range[1])\n",
    "    cw_np = np.ones(n_cls, dtype=np.float32)\n",
    "    for k, weight in zip(uniq.astype(int), w):\n",
    "        cw_np[k] = float(weight)\n",
    "    return cw_np\n",
    "\n",
    "# ---------- Entrenador ----------\n",
    "def train_sleep_model(\n",
    "    model,\n",
    "    X, y, splits,\n",
    "    batch_size=128,\n",
    "    lr=1e-3,\n",
    "    epochs=100,\n",
    "    optimizer_type='adam',\n",
    "    criterion_name=\"ce\",\n",
    "    label_smoothing=0.0,\n",
    "    focal_gamma=2.0,\n",
    "    use_gpu=True,\n",
    "    num_workers=0,                     \n",
    "    class_weights=None,\n",
    "    weight_clip_range=(0.5, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    amp=False,\n",
    "    save_path=\"best_model.pt\",\n",
    "    early_stopping_tolerance=15,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    early_stopping_min_delta=0.0\n",
    "):\n",
    "    # --- Device\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üöÄ Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"üíª Usando CPU\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_idx, val_idx, test_idx = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
    "\n",
    "    # --- Pesos de clase\n",
    "    cw_np = None\n",
    "    if class_weights is not None:\n",
    "        if isinstance(class_weights, str):\n",
    "            cw_np = compute_class_weights(y[train_idx], method=class_weights, clip_range=weight_clip_range)\n",
    "            uniq, cnt = np.unique(y[train_idx], return_counts=True)\n",
    "            print(\"\\nüìä Distribuci√≥n TRAIN:\")\n",
    "            for cls, count in zip(uniq.astype(int), cnt):\n",
    "                print(f\"   Clase {cls}: {count:7d} ({count/cnt.sum()*100:5.2f}%)\")\n",
    "            print(\"‚öñÔ∏è  Pesos de clase:\", [f\"{w:.3f}\" for w in cw_np])\n",
    "        elif isinstance(class_weights, dict):\n",
    "            n_cls = int(np.max(y)) + 1\n",
    "            cw_np = np.ones(n_cls, dtype=np.float32)\n",
    "            for k, w in class_weights.items(): cw_np[int(k)] = float(w)\n",
    "        elif isinstance(class_weights, (list, np.ndarray)):\n",
    "            cw_np = np.array(class_weights, dtype=np.float32)\n",
    "\n",
    "        if cw_np is not None and np.any(cw_np > 5.0):\n",
    "            print(f\"‚ö†Ô∏è  WARNING: Pesos muy altos (max={cw_np.max():.2f}). Considera clip m√°s estricto.\")\n",
    "\n",
    "    weight_tensor = torch.tensor(cw_np, dtype=torch.float32, device=device) if cw_np is not None else None\n",
    "\n",
    "    # --- Criterio\n",
    "    def make_ce(weight_tensor=None, label_smoothing=0.0):\n",
    "        if label_smoothing and label_smoothing > 0.0:\n",
    "            return nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=float(label_smoothing))\n",
    "        return nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, gamma=2.0, alpha=None):\n",
    "            super().__init__()\n",
    "            self.gamma = float(gamma)\n",
    "            self.alpha = alpha\n",
    "            self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        def forward(self, logits, target):\n",
    "            logits = torch.clamp(logits, min=-50, max=50)\n",
    "            ce = torch.clamp(self.ce(logits, target), min=1e-7, max=50)\n",
    "            pt = torch.clamp(torch.exp(-ce), min=1e-7, max=0.9999)\n",
    "            loss = (1 - pt) ** self.gamma * ce\n",
    "            if self.alpha is not None:\n",
    "                loss = self.alpha[target] * loss\n",
    "            mask = ~(torch.isnan(loss) | torch.isinf(loss))\n",
    "            return loss[mask].mean() if mask.any() else torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "    if criterion_name == \"ce\":\n",
    "        criterion = make_ce(weight_tensor=weight_tensor)\n",
    "    elif criterion_name == \"ce_smooth\":\n",
    "        criterion = make_ce(weight_tensor=weight_tensor, label_smoothing=label_smoothing)\n",
    "    elif criterion_name == \"focal\":\n",
    "        criterion = FocalLoss(gamma=focal_gamma, alpha=weight_tensor)\n",
    "    else:\n",
    "        raise ValueError(\"criterion_name inv√°lido\")\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # --- Datasets / Loaders\n",
    "    train_ds = SpectroDataset(X, y, indices=train_idx)\n",
    "    val_ds   = SpectroDataset(X, y, indices=val_idx)\n",
    "    test_ds  = SpectroDataset(X, y, indices=test_idx)\n",
    "\n",
    "    # Sampler balanceado para TRAIN\n",
    "    y_train_subset = y[train_idx]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    inv_freq = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = inv_freq[y_train_subset]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    pin_mem = (device.type == 'cuda')\n",
    "    pw = num_workers > 0\n",
    "    common_loader_kwargs = dict(num_workers=num_workers, pin_memory=pin_mem, persistent_workers=pw, prefetch_factor=(2 if pw else None))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=False, **common_loader_kwargs)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, **common_loader_kwargs)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, **common_loader_kwargs)\n",
    "\n",
    "    # --- Optimizador\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_type == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_type debe ser 'adam', 'sgd' o 'adamw'\")\n",
    "\n",
    "    print(f\"üéØ Optimizador: {optimizer_type.upper()}  |  üìà LR: {lr}  |  üì¶ Batch: {batch_size}  |  üî¢ √âpocas: {epochs}\")\n",
    "    print(f\"‚úÇÔ∏è  Grad clip: {grad_clip if grad_clip else 'OFF'}  |  ‚ö° AMP: {'ON' if (amp and device.type=='cuda') else 'OFF'}\")\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type == 'cuda'))\n",
    "\n",
    "    monitor_mode = 'max' if early_stopping_metric == 'val_acc' else 'min'\n",
    "    es = EarlyStopping(patience=early_stopping_tolerance, mode=monitor_mode, min_delta=early_stopping_min_delta)\n",
    "\n",
    "    history = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "    best_val_metric = -np.inf if monitor_mode == 'max' else np.inf\n",
    "    best_state = None\n",
    "\n",
    "    # --- Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_acc, train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, grad_clip=grad_clip, scaler=scaler)\n",
    "        val_acc, val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        monitor_value = val_acc if early_stopping_metric == \"val_acc\" else val_loss\n",
    "        improved = (monitor_value > best_val_metric) if monitor_mode == 'max' else (monitor_value < best_val_metric)\n",
    "        if improved:\n",
    "            best_val_metric = monitor_value\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            torch.save(best_state, save_path)\n",
    "\n",
    "        history[\"train_acc\"].append(train_acc); history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_acc\"].append(val_acc);     history[\"val_loss\"].append(val_loss)\n",
    "        history[\"lr\"].append(lr)\n",
    "\n",
    "        star = \" ‚≠ê\" if improved else \"\"\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | Train[L {train_loss:.4f} A {train_acc:.4f}] | \"\n",
    "              f\"Val[L {val_loss:.4f} A {val_acc:.4f}] | Best {early_stopping_metric} {best_val_metric:.4f}{star}\")\n",
    "\n",
    "        if es.step(monitor_value):\n",
    "            print(f\"\\nüõë Early stopping: {early_stopping_tolerance} √©pocas sin mejora en {early_stopping_metric}\")\n",
    "            break\n",
    "\n",
    "    # --- Evaluaci√≥n final\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_acc, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ RESULTADOS FINALES\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"Best Val {early_stopping_metric}: {best_val_metric:.4f}\")\n",
    "    print(f\"üíæ Modelo guardado en: {save_path}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    return model, history, {\"test_acc\": test_acc, \"test_loss\": test_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5879351",
   "metadata": {},
   "source": [
    "# Ejecuci√≥n (1 sola vez)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b6d8a",
   "metadata": {},
   "source": [
    "## Primer dataset: EEG1,EEG2,EOG,EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fddeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ENTRENAMIENTO + CURVAS + EVALUACI√ìN COMPLETA \n",
    "# ===============================================================\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- CONFIGURACI√ìN --------\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Reemplaza X, y, splits seg√∫n dataset (x1,y1,splits1), (x2,y2,splits2), etc.\n",
    "X, y, splits = x1, y1, splits1\n",
    "DATASET_NAME = \"EEG1+EEG2+EOG+EMG\"\n",
    "\n",
    "# -------- ENTRENAMIENTO --------\n",
    "try:\n",
    "    model = SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "except TypeError:\n",
    "    model = SleepStageModel(num_classes=5)\n",
    "\n",
    "model, hist, results = train_sleep_model(\n",
    "    model, X, y, splits,\n",
    "    lr=1e-4,\n",
    "    batch_size=256,\n",
    "    epochs=35,\n",
    "    criterion_name='ce',\n",
    "    class_weights=None,  \n",
    "    weight_clip_range=(0.1, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    use_gpu=True,\n",
    "    amp=False,\n",
    "    num_workers=0,\n",
    "    early_stopping_tolerance=8,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    save_path=\"NUL\"  # para no guardar en disco\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS GENERALES ({DATASET_NAME}):\")\n",
    "print(f\"   Test Accuracy: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {results['test_loss']:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -------- CURVAS DE ENTRENAMIENTO --------\n",
    "epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist[\"train_loss\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_loss\"], 'b-', label='Validation')\n",
    "plt.title('Loss evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist[\"train_acc\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_acc\"], 'b-', label='Validation')\n",
    "plt.title('Accuracy evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- EVALUACI√ìN DETALLADA --------\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)\n",
    "        x = np.transpose(x, (2,0,1))   # (C,F,T)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(SpectroDataset(X, y, splits[\"test\"]),\n",
    "                         batch_size=256, shuffle=False, num_workers=0,\n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval(); yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        pred = model(xb).argmax(1).cpu().numpy()\n",
    "        yp.append(pred); yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "y_pred, y_true = predict(model, test_loader)\n",
    "\n",
    "# --- m√©tricas por clase ---\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=range(5), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "cm_norm = cm / np.maximum(cm.sum(1, keepdims=True), 1)\n",
    "\n",
    "# --- tabla de m√©tricas ---\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"etapa\": labels,\n",
    "    \"precision\": np.round(prec,3),\n",
    "    \"recall\": np.round(rec,3),\n",
    "    \"f1_score\": np.round(f1,3),\n",
    "    \"soporte\": support\n",
    "})\n",
    "display(df_metrics.style.set_caption(f\"M√©tricas por etapa - {DATASET_NAME}\").format(precision=3))\n",
    "\n",
    "# --- m√©tricas globales ---\n",
    "acc_global = accuracy_score(y_true, y_pred)\n",
    "kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy global: {acc_global:.3f}\")\n",
    "print(f\"‚úÖ Cohen‚Äôs Œ∫: {kappa_global:.3f}\")\n",
    "\n",
    "# --- matriz de confusi√≥n ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de confusi√≥n (normalizada) ‚Äî {DATASET_NAME}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dec836",
   "metadata": {},
   "source": [
    "## Segundo dataset: EEG1,EEG2 y EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ENTRENAMIENTO + CURVAS + EVALUACI√ìN COMPLETA \n",
    "# ===============================================================\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- CONFIGURACI√ìN --------\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Reemplaza X, y, splits seg√∫n dataset (x1,y1,splits1), (x2,y2,splits2), etc.\n",
    "X, y, splits = x2, y2, splits2\n",
    "DATASET_NAME = \"EEG1+EEG2+EOG\"\n",
    "\n",
    "# -------- ENTRENAMIENTO --------\n",
    "try:\n",
    "    model = SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "except TypeError:\n",
    "    model = SleepStageModel(num_classes=5)\n",
    "\n",
    "model, hist, results = train_sleep_model(\n",
    "    model, X, y, splits,\n",
    "    lr=1e-4,\n",
    "    batch_size=256,\n",
    "    epochs=35,\n",
    "    criterion_name='ce',\n",
    "    class_weights=None,  \n",
    "    weight_clip_range=(0.1, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    use_gpu=True,\n",
    "    amp=False,\n",
    "    num_workers=0,\n",
    "    early_stopping_tolerance=8,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    save_path=\"NUL\"  # ‚ö†Ô∏è para no guardar en disco\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS GENERALES ({DATASET_NAME}):\")\n",
    "print(f\"   Test Accuracy: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {results['test_loss']:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -------- CURVAS DE ENTRENAMIENTO --------\n",
    "epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist[\"train_loss\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_loss\"], 'b-', label='Validation')\n",
    "plt.title('Loss evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist[\"train_acc\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_acc\"], 'b-', label='Validation')\n",
    "plt.title('Accuracy evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- EVALUACI√ìN DETALLADA --------\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)\n",
    "        x = np.transpose(x, (2,0,1))   # (C,F,T)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(SpectroDataset(X, y, splits[\"test\"]),\n",
    "                         batch_size=256, shuffle=False, num_workers=0,\n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval(); yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        pred = model(xb).argmax(1).cpu().numpy()\n",
    "        yp.append(pred); yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "y_pred, y_true = predict(model, test_loader)\n",
    "\n",
    "# --- m√©tricas por clase ---\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=range(5), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "cm_norm = cm / np.maximum(cm.sum(1, keepdims=True), 1)\n",
    "\n",
    "# --- tabla de m√©tricas ---\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"etapa\": labels,\n",
    "    \"precision\": np.round(prec,3),\n",
    "    \"recall\": np.round(rec,3),\n",
    "    \"f1_score\": np.round(f1,3),\n",
    "    \"soporte\": support\n",
    "})\n",
    "display(df_metrics.style.set_caption(f\"M√©tricas por etapa - {DATASET_NAME}\").format(precision=3))\n",
    "\n",
    "# --- m√©tricas globales ---\n",
    "acc_global = accuracy_score(y_true, y_pred)\n",
    "kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy global: {acc_global:.3f}\")\n",
    "print(f\"‚úÖ Cohen‚Äôs Œ∫: {kappa_global:.3f}\")\n",
    "\n",
    "# --- matriz de confusi√≥n ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de confusi√≥n (normalizada) ‚Äî {DATASET_NAME}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7917f",
   "metadata": {},
   "source": [
    "## Tercer dataset: S√≥lo el EEG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bef14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ENTRENAMIENTO + CURVAS + EVALUACI√ìN COMPLETA \n",
    "# ===============================================================\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- CONFIGURACI√ìN --------\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Reemplaza X, y, splits seg√∫n dataset (x1,y1,splits1), (x2,y2,splits2), etc.\n",
    "X, y, splits = x3, y3, splits3\n",
    "DATASET_NAME = \"EEG1\"\n",
    "\n",
    "# -------- ENTRENAMIENTO --------\n",
    "try:\n",
    "    model = SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "except TypeError:\n",
    "    model = SleepStageModel(num_classes=5)\n",
    "\n",
    "model, hist, results = train_sleep_model(\n",
    "    model, X, y, splits,\n",
    "    lr=5e-6,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    criterion_name='ce',\n",
    "    class_weights=None,  \n",
    "    weight_clip_range=(0.1, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    use_gpu=True,\n",
    "    amp=False,\n",
    "    num_workers=0,\n",
    "    early_stopping_tolerance=4,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    save_path=\"NUL\"  # ‚ö†Ô∏è para no guardar en disco\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS GENERALES ({DATASET_NAME}):\")\n",
    "print(f\"   Test Accuracy: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {results['test_loss']:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -------- CURVAS DE ENTRENAMIENTO --------\n",
    "epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist[\"train_loss\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_loss\"], 'b-', label='Validation')\n",
    "plt.title('Loss evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist[\"train_acc\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_acc\"], 'b-', label='Validation')\n",
    "plt.title('Accuracy evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- EVALUACI√ìN DETALLADA --------\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)\n",
    "        x = np.transpose(x, (2,0,1))   # (C,F,T)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(SpectroDataset(X, y, splits[\"test\"]),\n",
    "                         batch_size=256, shuffle=False, num_workers=0,\n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval(); yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        pred = model(xb).argmax(1).cpu().numpy()\n",
    "        yp.append(pred); yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "y_pred, y_true = predict(model, test_loader)\n",
    "\n",
    "# --- m√©tricas por clase ---\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=range(5), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "cm_norm = cm / np.maximum(cm.sum(1, keepdims=True), 1)\n",
    "\n",
    "# --- tabla de m√©tricas ---\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"etapa\": labels,\n",
    "    \"precision\": np.round(prec,3),\n",
    "    \"recall\": np.round(rec,3),\n",
    "    \"f1_score\": np.round(f1,3),\n",
    "    \"soporte\": support\n",
    "})\n",
    "display(df_metrics.style.set_caption(f\"M√©tricas por etapa - {DATASET_NAME}\").format(precision=3))\n",
    "\n",
    "# --- m√©tricas globales ---\n",
    "acc_global = accuracy_score(y_true, y_pred)\n",
    "kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy global: {acc_global:.3f}\")\n",
    "print(f\"‚úÖ Cohen‚Äôs Œ∫: {kappa_global:.3f}\")\n",
    "\n",
    "# --- matriz de confusi√≥n ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de confusi√≥n (normalizada) ‚Äî {DATASET_NAME}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4118b4a",
   "metadata": {},
   "source": [
    "## Cuarto dataset: EEG1, EEG2 y EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ENTRENAMIENTO + CURVAS + EVALUACI√ìN COMPLETA \n",
    "# ===============================================================\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- CONFIGURACI√ìN --------\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Reemplaza X, y, splits seg√∫n dataset (x1,y1,splits1), (x2,y2,splits2), etc.\n",
    "X, y, splits = x4, y4, splits4\n",
    "DATASET_NAME = \"EEG1+EEG2+EMG\"\n",
    "\n",
    "# -------- 1ENTRENAMIENTO --------\n",
    "try:\n",
    "    model = SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "except TypeError:\n",
    "    model = SleepStageModel(num_classes=5)\n",
    "\n",
    "model, hist, results = train_sleep_model(\n",
    "    model, X, y, splits,\n",
    "    lr=5e-6,\n",
    "    batch_size=256, # Cambiar a 256 xd\n",
    "    epochs=50,\n",
    "    criterion_name='ce',\n",
    "    class_weights=None,  \n",
    "    weight_clip_range=(0.1, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    use_gpu=True,\n",
    "    amp=False,\n",
    "    num_workers=0,\n",
    "    early_stopping_tolerance=4,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    save_path=\"NUL\"  # ‚ö†Ô∏è para no guardar en disco\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS GENERALES ({DATASET_NAME}):\")\n",
    "print(f\"   Test Accuracy: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {results['test_loss']:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -------- CURVAS DE ENTRENAMIENTO --------\n",
    "epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist[\"train_loss\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_loss\"], 'b-', label='Validation')\n",
    "plt.title('Loss evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist[\"train_acc\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_acc\"], 'b-', label='Validation')\n",
    "plt.title('Accuracy evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- EVALUACI√ìN DETALLADA --------\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)\n",
    "        x = np.transpose(x, (2,0,1))   # (C,F,T)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(SpectroDataset(X, y, splits[\"test\"]),\n",
    "                         batch_size=256, shuffle=False, num_workers=0,\n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval(); yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        pred = model(xb).argmax(1).cpu().numpy()\n",
    "        yp.append(pred); yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "y_pred, y_true = predict(model, test_loader)\n",
    "\n",
    "# --- m√©tricas por clase ---\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=range(5), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "cm_norm = cm / np.maximum(cm.sum(1, keepdims=True), 1)\n",
    "\n",
    "# --- tabla de m√©tricas ---\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"etapa\": labels,\n",
    "    \"precision\": np.round(prec,3),\n",
    "    \"recall\": np.round(rec,3),\n",
    "    \"f1_score\": np.round(f1,3),\n",
    "    \"soporte\": support\n",
    "})\n",
    "display(df_metrics.style.set_caption(f\"M√©tricas por etapa - {DATASET_NAME}\").format(precision=3))\n",
    "\n",
    "# --- m√©tricas globales ---\n",
    "acc_global = accuracy_score(y_true, y_pred)\n",
    "kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy global: {acc_global:.3f}\")\n",
    "print(f\"‚úÖ Cohen‚Äôs Œ∫: {kappa_global:.3f}\")\n",
    "\n",
    "# --- matriz de confusi√≥n ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de confusi√≥n (normalizada) ‚Äî {DATASET_NAME}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffb336",
   "metadata": {},
   "source": [
    "## Quinto dataset: EOG y EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ENTRENAMIENTO + CURVAS + EVALUACI√ìN COMPLETA \n",
    "# ===============================================================\n",
    "\n",
    "import torch, numpy as np, matplotlib.pyplot as plt, seaborn as sns, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- CONFIGURACI√ìN --------\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if (use_gpu and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Reemplaza X, y, splits seg√∫n dataset (x1,y1,splits1), (x2,y2,splits2), etc.\n",
    "X, y, splits = x5, y5, splits5\n",
    "DATASET_NAME = \"EOG+EMG\"\n",
    "\n",
    "# -------- 1ENTRENAMIENTO --------\n",
    "try:\n",
    "    model = SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "except TypeError:\n",
    "    model = SleepStageModel(num_classes=5)\n",
    "\n",
    "model, hist, results = train_sleep_model(\n",
    "    model, X, y, splits,\n",
    "    lr=1e-4,\n",
    "    batch_size=256,\n",
    "    epochs=35,\n",
    "    criterion_name='ce',\n",
    "    class_weights=None,  \n",
    "    weight_clip_range=(0.1, 2.5),\n",
    "    grad_clip=1.0,\n",
    "    use_gpu=True,\n",
    "    amp=False,\n",
    "    num_workers=0,\n",
    "    early_stopping_tolerance=8,\n",
    "    early_stopping_metric=\"val_acc\",\n",
    "    save_path=\"NUL\"  # ‚ö†Ô∏è para no guardar en disco\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä RESULTADOS GENERALES ({DATASET_NAME}):\")\n",
    "print(f\"   Test Accuracy: {results['test_acc']:.4f} ({results['test_acc']*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {results['test_loss']:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -------- CURVAS DE ENTRENAMIENTO --------\n",
    "epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist[\"train_loss\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_loss\"], 'b-', label='Validation')\n",
    "plt.title('Loss evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist[\"train_acc\"], 'r-', label='Training')\n",
    "plt.plot(epochs, hist[\"val_acc\"], 'b-', label='Validation')\n",
    "plt.title('Accuracy evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- EVALUACI√ìN DETALLADA --------\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)\n",
    "        x = np.transpose(x, (2,0,1))   # (C,F,T)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(SpectroDataset(X, y, splits[\"test\"]),\n",
    "                         batch_size=256, shuffle=False, num_workers=0,\n",
    "                         pin_memory=(device.type=='cuda'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval(); yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        pred = model(xb).argmax(1).cpu().numpy()\n",
    "        yp.append(pred); yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "y_pred, y_true = predict(model, test_loader)\n",
    "\n",
    "# --- m√©tricas por clase ---\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=range(5), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(5))\n",
    "cm_norm = cm / np.maximum(cm.sum(1, keepdims=True), 1)\n",
    "\n",
    "# --- tabla de m√©tricas ---\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"etapa\": labels,\n",
    "    \"precision\": np.round(prec,3),\n",
    "    \"recall\": np.round(rec,3),\n",
    "    \"f1_score\": np.round(f1,3),\n",
    "    \"soporte\": support\n",
    "})\n",
    "display(df_metrics.style.set_caption(f\"M√©tricas por etapa - {DATASET_NAME}\").format(precision=3))\n",
    "\n",
    "# --- m√©tricas globales ---\n",
    "acc_global = accuracy_score(y_true, y_pred)\n",
    "kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "print(f\"‚úÖ Accuracy global: {acc_global:.3f}\")\n",
    "print(f\"‚úÖ Cohen‚Äôs Œ∫: {kappa_global:.3f}\")\n",
    "\n",
    "# --- matriz de confusi√≥n ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.title(f\"Matriz de confusi√≥n (normalizada) ‚Äî {DATASET_NAME}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1008e",
   "metadata": {},
   "source": [
    "# Ejecuci√≥n (3 veces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738434d1",
   "metadata": {},
   "source": [
    "## Primer dataset: EEG1,EEG2,EOG,EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ab3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = x1, y1, splits1\n",
    "DATASET_NAME = \"EEG1+EEG2+EOG+EMG\"\n",
    "\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo \n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "\n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run \n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb69be",
   "metadata": {},
   "source": [
    "## Segundo dataset: EEG1,EEG2 y EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = x2, y2, splits2\n",
    "DATASET_NAME = \"EEG1+EEG2+EOG\"\n",
    "\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo \n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run \n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d3f7a",
   "metadata": {},
   "source": [
    "## Tercer dataset: S√≥lo el EEG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = x3, y3, splits3\n",
    "DATASET_NAME = \"EEG1\"\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths (solo se usan si guardas algo) =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo \n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run \n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29c064",
   "metadata": {},
   "source": [
    "## Cuarto dataset: EEG1, EEG2 y EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfe5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = x4, y4, splits4\n",
    "DATASET_NAME = \"EEG1+EEG2+EMG\"\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths (solo se usan si guardas algo) =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo =======\n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "\n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run \n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1b831",
   "metadata": {},
   "source": [
    "## Quinto dataset: EOG y EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = x5, y5, splits5\n",
    "DATASET_NAME = \"EOG+EMG\"\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths (solo se usan si guardas algo) =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo \n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "\n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run \n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339035d",
   "metadata": {},
   "source": [
    "# Set 1A (Todo filtrado igual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET \"Set A\": STFT 2s/hop 2s, Hamming nperseg=256, 0.5‚Äì40 Hz\n",
    "# Lee ventanas ya guardadas (WINDOWS_DIR) y arma (N, 61, 15, 4)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.signal import stft, get_window, resample\n",
    "\n",
    "\n",
    "\n",
    "CHANNEL_PATTERNS = {\n",
    "    \"EEG1\": [\"EEG Fpz-Cz\", \"Fpz-Cz\"],\n",
    "    \"EEG2\": [\"EEG Pz-Oz\", \"Pz-Oz\"],\n",
    "    \"EOG\" : [\"EOG\", \"EOG horizontal\", \"EOG horizontal derivation\"],\n",
    "    \"EMG\" : [\"EMG\", \"EMG submental\", \"Submental EMG\"]\n",
    "}\n",
    "\n",
    "# Banda com√∫n 0.5‚Äì30 Hz para TODOS los canales (requisito del Set A)\n",
    "FMIN, FMAX = 0.5, 30.0\n",
    "\n",
    "# Salida 61√ó15 (STFT 2s / hop 2s en una ventana de 30s)\n",
    "N_FREQ_OUT, N_TIME_OUT = 61, 15\n",
    "WIN_SEC, SEG_SEC, HOP_SEC = 30.0, 2.0, 2.0\n",
    "\n",
    "# STFT: ventana Hamming fija de 256 puntos (NumPy 2.0-safe)\n",
    "NPERSEG_FIXED = 256\n",
    "WINDOW_TYPE = \"hamming\"\n",
    "\n",
    "LABEL2ID = {\"W\":0, \"N1\":1, \"N2\":2, \"N3\":3, \"REM\":4}\n",
    "ID2LABEL = {v:k for k,v in LABEL2ID.items()}\n",
    "\n",
    "def _load_npz(path: Path):\n",
    "    d = np.load(path, allow_pickle=False)\n",
    "    return {\n",
    "        \"X\": d[\"X\"],\n",
    "        \"y\": d[\"y\"].astype(np.uint8),\n",
    "        \"t\": d[\"t\"].astype(np.float32),\n",
    "        \"fs\": float(d[\"fs\"]),\n",
    "        \"canal\": str(d[\"canal\"])\n",
    "    }\n",
    "\n",
    "def _load_pkl(path: Path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    if isinstance(data.get(\"etiquetas\", []), list) and data[\"etiquetas\"]:\n",
    "        y = []\n",
    "        for s in data[\"etiquetas\"]:\n",
    "            y.append(int(s) if isinstance(s, (int, np.integer)) else LABEL2ID.get(str(s), 255))\n",
    "        y = np.array(y, dtype=np.uint8)\n",
    "    else:\n",
    "        y = np.array(data.get(\"etiquetas\", []), dtype=np.uint8)\n",
    "    return {\n",
    "        \"X\": np.asarray(data[\"ventanas\"], dtype=np.float32),\n",
    "        \"y\": y,\n",
    "        \"t\": np.asarray(data[\"tiempos_inicio\"], dtype=np.float32),\n",
    "        \"fs\": float(data.get(\"freq_muestreo\", 100.0)),\n",
    "        \"canal\": str(data.get(\"nombre_canal\", \"CANAL\"))\n",
    "    }\n",
    "\n",
    "def load_channel_file(paciente: str, canal_nombre: str, windows_dir: Path):\n",
    "    base = windows_dir / f\"{paciente}_{canal_nombre.replace(' ', '_')}\"\n",
    "    npz_path, pkl_path = base.with_suffix(\".npz\"), base.with_suffix(\".pkl\")\n",
    "    if npz_path.exists(): return _load_npz(npz_path), \".npz\"\n",
    "    if pkl_path.exists(): return _load_pkl(pkl_path), \".pkl\"\n",
    "    return None, None\n",
    "\n",
    "def stft_2s_2s(x, fs,\n",
    "               fmin=FMIN, fmax=FMAX,\n",
    "               n_freq_out=N_FREQ_OUT, n_time_out=N_TIME_OUT,\n",
    "               win_sec=WIN_SEC, seg_sec=SEG_SEC, hop_sec=HOP_SEC):\n",
    "    # NumPy 2.0-safe\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "\n",
    "    # fuerza 30 s\n",
    "    expected_len = int(round(win_sec * fs))\n",
    "    if len(x) != expected_len:\n",
    "        x = x[:expected_len] if len(x) > expected_len else np.pad(x, (0, expected_len - len(x)))\n",
    "\n",
    "    nperseg = NPERSEG_FIXED\n",
    "    hop_samps = int(round(hop_sec * fs))\n",
    "    noverlap = max(0, nperseg - hop_samps)\n",
    "\n",
    "    nfft = 1\n",
    "    while nfft < nperseg:\n",
    "        nfft <<= 1\n",
    "\n",
    "    f, t, Z = stft(\n",
    "        x, fs=fs,\n",
    "        window=get_window(WINDOW_TYPE, nperseg, fftbins=True),\n",
    "        nperseg=nperseg, noverlap=noverlap, nfft=nfft,\n",
    "        boundary=None, padded=False, detrend=False, return_onesided=True\n",
    "    )\n",
    "    band = (f >= fmin) & (f <= fmax)\n",
    "    Zb = Z[band, :]\n",
    "    P = (np.abs(Zb) ** 2).astype(np.float32)\n",
    "    S = np.log10(P + 1e-12)\n",
    "\n",
    "    # remuestreo a (61,15)\n",
    "    if S.shape[0] != n_freq_out:\n",
    "        S = resample(S, n_freq_out, axis=0)\n",
    "    if S.shape[1] != n_time_out:\n",
    "        S = resample(S, n_time_out, axis=1)\n",
    "    return S\n",
    "\n",
    "def pick_channel_name(df_patient: pd.DataFrame, aliases: list[str]) -> str | None:\n",
    "    names = list(df_patient[\"Canal\"].unique())\n",
    "    u_names = [n.upper() for n in names]\n",
    "    for alias in aliases:\n",
    "        alias_u = alias.upper()\n",
    "        for n, u in zip(names, u_names):\n",
    "            if u == alias_u: return n\n",
    "        for n, u in zip(names, u_names):\n",
    "            if alias_u in u: return n\n",
    "    return None\n",
    "\n",
    "def build_cnn_dataset_setA(\n",
    "    analysis_csv: Path,\n",
    "    windows_dir: Path,\n",
    "    dtype=\"float32\",\n",
    "    memmap_path: Path | None = None,\n",
    "    max_patients: int | None = None\n",
    "):\n",
    "    df = pd.read_csv(analysis_csv)\n",
    "    for col in [\"Paciente\", \"Canal\"]:\n",
    "        assert col in df.columns, f\"Falta columna {col} en {analysis_csv}\"\n",
    "\n",
    "    patients = list(df[\"Paciente\"].unique())\n",
    "    if max_patients: patients = patients[:max_patients]\n",
    "\n",
    "    all_specs, all_labels, counts = [], [], []\n",
    "\n",
    "    # memmap opcional (por defecto NO guarda en disco)\n",
    "    if memmap_path is not None:\n",
    "        total_N = 0\n",
    "        for p in patients:\n",
    "            dpf = df[df[\"Paciente\"] == p]\n",
    "            chosen = {k: pick_channel_name(dpf, v) for k, v in CHANNEL_PATTERNS.items()}\n",
    "            if any(v is None for v in chosen.values()): continue\n",
    "            loaded = {}\n",
    "            ok = True\n",
    "            for k, nm in chosen.items():\n",
    "                dfile, _ = load_channel_file(p, nm, windows_dir)\n",
    "                if dfile is None: ok = False; break\n",
    "                loaded[k] = dfile\n",
    "            if not ok: continue\n",
    "            times_sets = [set(np.round(loaded[k][\"t\"], 4)) for k in loaded]\n",
    "            common = set.intersection(*times_sets)\n",
    "            total_N += len(common)\n",
    "\n",
    "        memmap_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        X = np.memmap(memmap_path, dtype=(np.float16 if dtype==\"float16\" else np.float32),\n",
    "                      mode='w+', shape=(total_N, N_FREQ_OUT, N_TIME_OUT, 4))\n",
    "        y = np.memmap(memmap_path.with_suffix(\".labels.npy\"), dtype=np.uint8,\n",
    "                      mode='w+', shape=(total_N,))\n",
    "        widx = 0\n",
    "    else:\n",
    "        X = y = None\n",
    "\n",
    "    for p in patients:\n",
    "        dpf = df[df[\"Paciente\"] == p]\n",
    "        chosen = {k: pick_channel_name(dpf, v) for k, v in CHANNEL_PATTERNS.items()}\n",
    "        if any(v is None for v in chosen.values()): continue\n",
    "\n",
    "        loaded = {}\n",
    "        ok = True\n",
    "        for k, nm in chosen.items():\n",
    "            dfile, _ = load_channel_file(p, nm, windows_dir)\n",
    "            if dfile is None: ok = False; break\n",
    "            loaded[k] = dfile\n",
    "        if not ok: continue\n",
    "\n",
    "        # alinear por tiempos comunes\n",
    "        tr = {k: np.round(loaded[k][\"t\"], 4) for k in loaded}\n",
    "        common = set(tr[\"EEG1\"])\n",
    "        for k in [\"EEG2\", \"EOG\", \"EMG\"]: common &= set(tr[k])\n",
    "        if not common: continue\n",
    "        common_sorted = np.array(sorted(list(common)), dtype=np.float32)\n",
    "\n",
    "        idx_maps = {}\n",
    "        for k in loaded:\n",
    "            t2idx = {float(t): i for i, t in enumerate(tr[k])}\n",
    "            idx_maps[k] = [t2idx[float(t)] for t in common_sorted]\n",
    "\n",
    "        # STFT por canal (0.5‚Äì30 Hz)\n",
    "        ch_specs = []\n",
    "        labels_p = None\n",
    "        for k in [\"EEG1\",\"EEG2\",\"EOG\",\"EMG\"]:\n",
    "            d_k = loaded[k]\n",
    "            fs = d_k[\"fs\"]\n",
    "            Xraw = d_k[\"X\"][idx_maps[k]]\n",
    "            yk   = d_k[\"y\"][idx_maps[k]]\n",
    "            if labels_p is None: labels_p = yk.copy()\n",
    "\n",
    "            n = Xraw.shape[0]\n",
    "            S_k = np.empty((n, N_FREQ_OUT, N_TIME_OUT), dtype=np.float32)\n",
    "            for i in range(n):\n",
    "                S_k[i] = stft_2s_2s(Xraw[i], fs)\n",
    "            ch_specs.append(S_k)\n",
    "\n",
    "        specs_p = np.stack(ch_specs, axis=-1)  # (n, 61, 15, 4)\n",
    "        if dtype == \"float16\": specs_p = specs_p.astype(np.float16)\n",
    "\n",
    "        if memmap_path is not None:\n",
    "            nn = specs_p.shape[0]\n",
    "            X[widx:widx+nn] = specs_p\n",
    "            y[widx:widx+nn] = labels_p\n",
    "            widx += nn\n",
    "        else:\n",
    "            all_specs.append(specs_p)\n",
    "            all_labels.append(labels_p)\n",
    "\n",
    "        counts.append((p, int(specs_p.shape[0])))\n",
    "\n",
    "    if memmap_path is None:\n",
    "        if all_specs:\n",
    "            X = np.concatenate(all_specs, axis=0)\n",
    "            y = np.concatenate(all_labels, axis=0)\n",
    "        else:\n",
    "            X = np.empty((0, N_FREQ_OUT, N_TIME_OUT, 4), dtype=(np.float16 if dtype==\"float16\" else np.float32))\n",
    "            y = np.empty((0,), dtype=np.uint8)\n",
    "\n",
    "    meta = {\n",
    "        \"shape\": tuple(X.shape),\n",
    "        \"labels_unique\": sorted(list(map(int, np.unique(y)))) if y.size else [],\n",
    "        \"label_map\": ID2LABEL,\n",
    "        \"counts_per_patient\": counts,\n",
    "        \"channels_used\": CHANNEL_PATTERNS,\n",
    "        \"stft_config\": {\n",
    "            \"window_type\": WINDOW_TYPE,\n",
    "            \"nperseg\": NPERSEG_FIXED,\n",
    "            \"freq_range\": (FMIN, FMAX),\n",
    "            \"output_shape\": (N_FREQ_OUT, N_TIME_OUT),\n",
    "            \"hop_s\": HOP_SEC, \"seg_s\": SEG_SEC, \"win_s\": WIN_SEC\n",
    "        }\n",
    "    }\n",
    "    return X, y, meta\n",
    "\n",
    "# ---------- Ejecutar (no guarda a disco por defecto) ----------\n",
    "resumen_csv = ANALYSIS_DIR / \"resumen_global.csv\"\n",
    "SAVE_TO_DISK = False  # <- c√°mbialo a True si quieres memmap en OUTPUT_DIR\n",
    "memmap_path = (OUTPUT_DIR / \"SetA_X.dat\") if SAVE_TO_DISK else None\n",
    "\n",
    "X_A, y_A, meta_A = build_cnn_dataset_setA(\n",
    "    analysis_csv=resumen_csv,\n",
    "    windows_dir=WINDOWS_DIR,\n",
    "    dtype=\"float32\",\n",
    "    memmap_path=memmap_path,\n",
    "    max_patients=None\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Set A listo (0.5‚Äì40 Hz, 4 canales)\")\n",
    "print(\"   X_A:\", X_A.shape, \"| y_A:\", y_A.shape, \"| clases:\", sorted(np.unique(y_A)))\n",
    "print(\"   Pacientes:\", len(meta_A[\"counts_per_patient\"]))\n",
    "print(\"   Config STFT:\", meta_A[\"stft_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Patient IDs + Split 60/20/20 por paciente (sin leakage)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def make_patient_ids(meta):\n",
    "    ids = []\n",
    "    for patient, n in meta[\"counts_per_patient\"]:\n",
    "        ids.extend([patient] * int(n))\n",
    "    return np.array(ids)\n",
    "\n",
    "patient_ids_A = make_patient_ids(meta_A)\n",
    "assert len(patient_ids_A) == len(y_A) == X_A.shape[0], \"Desalineaci√≥n en Set A\"\n",
    "\n",
    "def split_by_patient(X, y, patient_ids, test_size=0.20, val_size=0.20, random_state=42):\n",
    "    N = len(y)\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_val_idx, test_idx = next(gss1.split(np.zeros(N), y, groups=patient_ids))\n",
    "\n",
    "    rel_val = val_size / (1.0 - test_size)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=rel_val, random_state=random_state+1)\n",
    "    pv = patient_ids[train_val_idx]\n",
    "    yv = y[train_val_idx]\n",
    "    sub_train_idx, val_idx_sub = next(gss2.split(np.zeros(len(train_val_idx)), yv, groups=pv))\n",
    "\n",
    "    train_idx = train_val_idx[sub_train_idx]\n",
    "    val_idx = train_val_idx[val_idx_sub]\n",
    "    return {\"train\": train_idx, \"val\": val_idx, \"test\": test_idx}\n",
    "\n",
    "splits_A = split_by_patient(X_A, y_A, patient_ids_A, test_size=0.20, val_size=0.20, random_state=42)\n",
    "\n",
    "def print_split_summary(y, patient_ids, splits, label_names={0:\"W\",1:\"N1\",2:\"N2\",3:\"N3\",4:\"REM\"}):\n",
    "    p_train = set(np.unique(patient_ids[splits[\"train\"]]))\n",
    "    p_val   = set(np.unique(patient_ids[splits[\"val\"]]))\n",
    "    p_test  = set(np.unique(patient_ids[splits[\"test\"]]))\n",
    "    print(\"====== PACIENTES ======\")\n",
    "    print(f\"Train: {len(p_train)} | Val: {len(p_val)} | Test: {len(p_test)}\")\n",
    "    print(\"Intersecciones (deben ser 0):\",\n",
    "          len(p_train & p_val), len(p_train & p_test), len(p_val & p_test))\n",
    "    print(\"\\n====== DISTRIBUCI√ìN (ventanas) ======\")\n",
    "    for name, idx in splits.items():\n",
    "        yy = y[idx]\n",
    "        uniq, cnt = np.unique(yy, return_counts=True)\n",
    "        total = len(yy)\n",
    "        nice = \", \".join([f\"{label_names[int(k)]}: {int(v)} ({v/total*100:.1f}%)\"\n",
    "                          for k, v in zip(uniq, cnt)])\n",
    "        print(f\"{name:>5} -> N={total} | {nice}\")\n",
    "\n",
    "print_split_summary(y_A, patient_ids_A, splits_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELDA √öNICA: MULTI-RUN + M√âTRICAS \n",
    "# ================================================\n",
    "import os, json, copy, math, random, pickle, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 0) SELECCI√ìN DEL DATASET ACTUAL =========\n",
    "X, y, splits = X_A, y_A, splits_A\n",
    "DATASET_NAME = \"EEG1+EEG2+EOG+EMG Set A (0.5-30 Hz)\"\n",
    "\n",
    "# ========= 1) FLAGS (por defecto NO guarda) =========\n",
    "SAVE_CHECKPOINTS   = False   # Guarda best_model.pt por run\n",
    "SAVE_HISTORIES     = False   # Guarda history.npz por run\n",
    "SAVE_PER_RUN_FILES = False   # Guarda CSV / PNG / NPY por run (m√©tricas y CM)\n",
    "SAVE_AGGREGATES    = False   # Guarda tablas y plots agregados\n",
    "\n",
    "# ========= 2) CONFIG GLOBAL =========\n",
    "N_RUNS = 3\n",
    "BASE_SEED = 42\n",
    "CONFIG = {\n",
    "    \"lr\":5e-6,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"criterion_name\": \"ce\",\n",
    "    \"class_weights\": None,\n",
    "    \"weight_clip_range\": (0.1, 2.5),\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_gpu\": True,\n",
    "    \"amp\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"early_stopping_tolerance\": 5,\n",
    "    \"early_stopping_metric\": \"val_acc\"\n",
    "}\n",
    "\n",
    "# ======= Paths  =======\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR) if 'OUTPUT_DIR' in globals() else (Path.cwd() / \"outputs\")\n",
    "RUNS_DIR   = OUTPUT_DIR / \"multiple_runs\" / DATASET_NAME.replace(\" \", \"_\")\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 3) Utils =========\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SpectroDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X, self.y = X, y\n",
    "        self.idx = np.asarray(indices)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = self.idx[i]\n",
    "        x = np.asarray(self.X[j], dtype=np.float32)  # (H,W,C)\n",
    "        x = np.transpose(x, (2,0,1))                 # -> (C,H,W)\n",
    "        yj = int(self.y[j])\n",
    "        return torch.from_numpy(x), torch.tensor(yj, dtype=torch.long)\n",
    "\n",
    "def _build_loaders(X, y, splits, batch_size=256, num_workers=0, pin=True):\n",
    "    train_ds = SpectroDataset(X, y, splits['train'])\n",
    "    val_ds   = SpectroDataset(X, y, splits['val'])\n",
    "    test_ds  = SpectroDataset(X, y, splits['test'])\n",
    "\n",
    "    # Weighted sampler (balanceo por clase en TRAIN)\n",
    "    y_train_subset = y[splits['train']]\n",
    "    class_counts = np.bincount(y_train_subset, minlength=int(np.max(y))+1)\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = class_weights[y_train_subset]\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=torch.DoubleTensor(sample_weights),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=pin, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======= Modelo \n",
    "def _new_model():\n",
    "    try:\n",
    "        return SleepStageModel(num_classes=5, in_ch=X.shape[-1])\n",
    "    except TypeError:\n",
    "        return SleepStageModel(num_classes=5)\n",
    "\n",
    "# ======= Entrenamiento \n",
    "\n",
    "# ======= M√©tricas detalladas por run =======\n",
    "@torch.no_grad()\n",
    "def evaluate_detailed(model, test_loader, device):\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_p.append(p); all_t.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(n_classes), average=None, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.divide(cm, np.maximum(row_sums, 1), where=(row_sums!=0))\n",
    "\n",
    "    # accuracy/kappa one-vs-rest\n",
    "    N = y_true.size\n",
    "    acc_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    kappa_per_class = np.zeros(n_classes, dtype=np.float64)\n",
    "    for k in range(n_classes):\n",
    "        TP = cm[k, k]\n",
    "        FN = cm[k, :].sum() - TP\n",
    "        FP = cm[:, k].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc_per_class[k] = (TP + TN) / max(1, cm.sum())\n",
    "\n",
    "        obs = acc_per_class[k]\n",
    "        p_yes_true = (TP + FN) / N\n",
    "        p_yes_pred = (TP + FP) / N\n",
    "        p_no_true  = (FP + TN) / N\n",
    "        p_no_pred  = (FN + TN) / N\n",
    "        exp = p_yes_true * p_yes_pred + p_no_true * p_no_pred\n",
    "        kappa_per_class[k] = (obs - exp) / (1 - exp + 1e-12)\n",
    "\n",
    "    df_per_class = pd.DataFrame({\n",
    "        \"etapa\": labels,\n",
    "        \"precision\": np.round(prec, 3),\n",
    "        \"recall\":    np.round(rec, 3),\n",
    "        \"f1_score\":  np.round(f1, 3),\n",
    "        \"accuracy\":  np.round(acc_per_class, 3),\n",
    "        \"kappa\":     np.round(kappa_per_class, 3),\n",
    "        \"soporte\":   support.astype(int)\n",
    "    })\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    kappa_global = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"df\": df_per_class,\n",
    "        \"cm\": cm,\n",
    "        \"cm_norm\": cm_norm,\n",
    "        \"acc\": overall_acc,\n",
    "        \"kappa\": kappa_global,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# ========= 4) LOOP de runs =========\n",
    "assert 'train_sleep_model' in globals(), \"Falta la funci√≥n train_sleep_model en el entorno.\"\n",
    "device = torch.device(\"cuda\" if (CONFIG[\"use_gpu\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "pin_mem = (device.type == \"cuda\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"üöÄ MULTI-RUN sobre dataset: {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Seeds: {[BASE_SEED+i for i in range(N_RUNS)]}\")\n",
    "print(f\"Guardar checkpoints: {SAVE_CHECKPOINTS} | Guardar histories: {SAVE_HISTORIES} | Guardar per-run: {SAVE_PER_RUN_FILES}\")\n",
    "print()\n",
    "\n",
    "# Loaders (fijos por dataset)\n",
    "train_loader, val_loader, test_loader = _build_loaders(\n",
    "    X, y, splits, batch_size=CONFIG[\"batch_size\"], num_workers=CONFIG[\"num_workers\"], pin=pin_mem\n",
    ")\n",
    "\n",
    "all_runs_data = []\n",
    "for run_id in range(1, N_RUNS+1):\n",
    "    seed = BASE_SEED + (run_id-1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = _new_model()\n",
    "    run_dir = RUNS_DIR / f\"run_{run_id:02d}\"\n",
    "    if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES):\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = (str(run_dir / \"best_model.pt\")) if SAVE_CHECKPOINTS else None\n",
    "\n",
    "    # Entrenar\n",
    "    model, hist, results = train_sleep_model(\n",
    "        model=model,\n",
    "        X=X, y=y, splits=splits,\n",
    "        save_path=(save_path if save_path else \"best_model_tmp.pt\"),\n",
    "        **CONFIG\n",
    "    )\n",
    "\n",
    "    # Curvas del √∫ltimo run (on-screen)\n",
    "    if run_id == N_RUNS:\n",
    "        epochs_arr = range(1, len(hist[\"train_loss\"])+1)\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        ax[0].plot(epochs_arr, hist[\"train_loss\"], 'r-', label='training')\n",
    "        ax[0].plot(epochs_arr, hist[\"val_loss\"], 'b-', label='validation')\n",
    "        ax[0].set_title('Loss evolution'); ax[0].set_xlabel('Epoch'); ax[0].set_ylabel('Loss'); ax[0].grid(True, alpha=.3); ax[0].legend()\n",
    "\n",
    "        ax[1].plot(epochs_arr, hist[\"train_acc\"], 'r-', label='training')\n",
    "        ax[1].plot(epochs_arr, hist[\"val_acc\"], 'b-', label='validation')\n",
    "        ax[1].set_title('Accuracy evolution'); ax[1].set_xlabel('Epoch'); ax[1].set_ylabel('Accuracy'); ax[1].grid(True, alpha=.3); ax[1].legend()\n",
    "        plt.suptitle(f\"Learning Curves ‚Äî {DATASET_NAME} (Run {run_id})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar opcional: history / config\n",
    "    if SAVE_HISTORIES:\n",
    "        np.savez(run_dir / \"history.npz\",\n",
    "                 train_loss=hist[\"train_loss\"], val_loss=hist[\"val_loss\"],\n",
    "                 train_acc=hist[\"train_acc\"], val_acc=hist[\"val_acc\"],\n",
    "                 lr=hist[\"lr\"])\n",
    "        with open(run_dir / \"config.json\",\"w\") as f:\n",
    "            cfg = copy.deepcopy(CONFIG); cfg.update(seed=seed, run_id=run_id, dataset=DATASET_NAME, ts=datetime.now().isoformat())\n",
    "            json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Evaluaci√≥n detallada por run\n",
    "    eval_res = evaluate_detailed(model, test_loader, device)\n",
    "\n",
    "    # Mostrar tabla por etapa en pantalla (sin guardar por defecto)\n",
    "    print(f\"\\nüìä RUN {run_id} ‚Äî M√©tricas por etapa\")\n",
    "    display(eval_res[\"df\"].style.set_caption(f\"Run {run_id} ‚Äî {DATASET_NAME}\"))\n",
    "\n",
    "    print(f\"   ‚û§ Acc={eval_res['acc']:.4f} | Kappa={eval_res['kappa']:.4f}\")\n",
    "\n",
    "    # Plots de CM (on-screen)\n",
    "    labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.heatmap(eval_res[\"cm\"], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title(f\"CM Cruda ‚Äî Run {run_id}\")\n",
    "    axes[0].set_xlabel(\"Predicho\"); axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "    sns.heatmap(eval_res[\"cm_norm\"], annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=axes[1])\n",
    "    axes[1].set_title(f\"CM Normalizada ‚Äî Run {run_id}\")\n",
    "    axes[1].set_xlabel(\"Predicho\"); axes[1].set_ylabel(\"Real\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Guardado por run (opcional)\n",
    "    if SAVE_PER_RUN_FILES:\n",
    "        eval_res[\"df\"].to_csv(run_dir / \"eval_test_per_class.csv\", index=False)\n",
    "        pd.DataFrame({\"y_true\": eval_res[\"y_true\"].astype(int),\n",
    "                      \"y_pred\": eval_res[\"y_pred\"].astype(int)}).to_csv(run_dir / \"eval_test_pred_vs_true.csv\", index=False)\n",
    "        np.save(run_dir / \"eval_test_cm.npy\", eval_res[\"cm\"])\n",
    "        np.save(run_dir / \"eval_test_cm_norm.npy\", eval_res[\"cm_norm\"])\n",
    "        with open(run_dir / \"eval_test_summary.txt\",\"w\") as f:\n",
    "            f.write(f\"accuracy_global={eval_res['acc']:.6f}\\n\")\n",
    "            f.write(f\"kappa_global={eval_res['kappa']:.6f}\\n\")\n",
    "\n",
    "    all_runs_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"history\": hist,\n",
    "        \"results\": results,\n",
    "        \"eval\": eval_res,\n",
    "        \"run_dir\": (run_dir if (SAVE_CHECKPOINTS or SAVE_HISTORIES or SAVE_PER_RUN_FILES or SAVE_AGGREGATES) else None)\n",
    "    })\n",
    "\n",
    "# ========= 5) Resumen simple =========\n",
    "test_accs = [rd[\"results\"][\"test_acc\"] for rd in all_runs_data]\n",
    "test_losses = [rd[\"results\"][\"test_loss\"] for rd in all_runs_data]\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"‚úÖ {N_RUNS} corridas completadas ‚Äî {DATASET_NAME}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Test Acc:  mean={np.mean(test_accs):.4f}  std={np.std(test_accs):.4f}  \"\n",
    "      f\"min={np.min(test_accs):.4f}  max={np.max(test_accs):.4f}\")\n",
    "print(f\"Test Loss: mean={np.mean(test_losses):.4f}  std={np.std(test_losses):.4f}  \"\n",
    "      f\"min={np.min(test_losses):.4f}  max={np.max(test_losses):.4f}\")\n",
    "\n",
    "# ========= 6) Agregaci√≥n (media ¬± std) y plots agregados =========\n",
    "labels = [\"W\",\"N1\",\"N2\",\"N3\",\"REM\"]\n",
    "metrics_cols = [\"precision\", \"recall\", \"f1_score\", \"accuracy\", \"kappa\"]\n",
    "\n",
    "# stack m√©tricas por clase\n",
    "per_class_list = [rd[\"eval\"][\"df\"][metrics_cols].to_numpy() for rd in all_runs_data]  # list of (5x5)\n",
    "per_class_arr  = np.stack(per_class_list, axis=0)  # (n_runs, 5, 5)\n",
    "\n",
    "means = per_class_arr.mean(axis=0)  # (5,5)\n",
    "stds  = per_class_arr.std(axis=0)   # (5,5)\n",
    "\n",
    "# Mostrar tabla agregada (en pantalla)\n",
    "df_agg = pd.DataFrame({\"etapa\": labels})\n",
    "for j, col in enumerate(metrics_cols):\n",
    "    df_agg[col] = [f\"{means[i,j]:.3f} ¬± {stds[i,j]:.3f}\" for i in range(len(labels))]\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS AGREGADAS POR ETAPA (media ¬± std):\")\n",
    "print(df_agg.to_string(index=False))\n",
    "\n",
    "# F1 barplot agregado (on-screen)\n",
    "f1_means = means[:, metrics_cols.index(\"f1_score\")]\n",
    "f1_stds  = stds[:,  metrics_cols.index(\"f1_score\")]\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, f1_means, yerr=f1_stds, capsize=4, alpha=.85)\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"Etapa\"); plt.ylabel(\"F1-Score\"); plt.title(f\"F1 por etapa (media¬±std) ‚Äî {DATASET_NAME}\")\n",
    "plt.grid(axis='y', alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# CM agregada (normalizada)\n",
    "cm_norm_mean = np.mean([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "cm_norm_std  = np.std ([rd[\"eval\"][\"cm_norm\"] for rd in all_runs_data], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm_norm_mean, annot=True, fmt=\".3f\", cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, ax=ax[0])\n",
    "ax[0].set_title(f\"CM Normalizada (media) ‚Äî {DATASET_NAME}\")\n",
    "sns.heatmap(cm_norm_std, annot=True, fmt=\".3f\", cmap='Reds',\n",
    "            xticklabels=labels, yticklabels=labels, vmin=0, vmax=0.2, ax=ax[1])\n",
    "ax[1].set_title(f\"CM Normalizada (std) ‚Äî {DATASET_NAME}\")\n",
    "for a in ax: a.set_xlabel(\"Predicho\"); a.set_ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Guardados agregados (opcional)\n",
    "if SAVE_AGGREGATES:\n",
    "    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_agg.to_csv(RUNS_DIR / \"metrics_aggregated_per_class.csv\", index=False)\n",
    "    np.save(RUNS_DIR / \"cm_norm_mean.npy\", cm_norm_mean)\n",
    "    np.save(RUNS_DIR / \"cm_norm_std.npy\", cm_norm_std)\n",
    "\n",
    "print(\"\\n Listo. \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
